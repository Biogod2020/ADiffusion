{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22d457e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39fa9c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_manifest(tiles_basedir, sentences_basedir, output_dir, val_split=0.05):\n",
    "    \"\"\"\n",
    "    Scans tile and sentence directories to create a CSV manifest for OpenClip training.\n",
    "\n",
    "    Args:\n",
    "        tiles_basedir (str): The root directory containing the sample tile folders (e.g., '.../TENX101_BN1_tiles').\n",
    "        sentences_basedir (str): The root directory containing the sample sentence folders.\n",
    "        output_dir (str): Directory where the manifest CSV files will be saved.\n",
    "        val_split (float): The fraction of data to use for the validation set.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    manifest_data = []\n",
    "    print(\"Scanning directories to build manifest...\")\n",
    "\n",
    "    # Assumes sentence directories match tile directories but with a different suffix\n",
    "    sentence_dirs = [d for d in os.listdir(sentences_basedir) if d.endswith('_sentences_hvg')]\n",
    "\n",
    "    for sent_dir_name in tqdm(sentence_dirs, desc=\"Processing Samples\"):\n",
    "        sample_id = sent_dir_name.replace('_sentences_hvg', '')\n",
    "        \n",
    "        # Construct corresponding tile directory name\n",
    "        tile_dir_name = f\"{sample_id}_tiles\" # Or another suffix if it's different\n",
    "        \n",
    "        sent_dir_path = os.path.join(sentences_basedir, sent_dir_name)\n",
    "        tile_dir_path = os.path.join(tiles_basedir, tile_dir_name)\n",
    "        \n",
    "        if not os.path.isdir(tile_dir_path):\n",
    "            print(f\"Warning: Tile directory not found for {sample_id}, skipping.\")\n",
    "            continue\n",
    "            \n",
    "        for sentence_file in os.listdir(sent_dir_path):\n",
    "            if not sentence_file.endswith('.txt'):\n",
    "                continue\n",
    "            \n",
    "            # Construct the corresponding image file name from the sentence file name\n",
    "            # e.g., 'TENX101_BN1_2544_13499.txt' -> 'TENX101_BN1_2544_13499.png'\n",
    "            image_file = sentence_file.replace('.txt', '.png')\n",
    "            image_path = os.path.join(tile_dir_path, image_file)\n",
    "            \n",
    "            # Check if both files actually exist before adding to the manifest\n",
    "            if os.path.exists(image_path):\n",
    "                # Read the gene sentence from the file\n",
    "                with open(os.path.join(sent_dir_path, sentence_file), 'r') as f:\n",
    "                    gene_sentence = f.read().strip()\n",
    "                \n",
    "                manifest_data.append({\n",
    "                    'image_path': image_path,\n",
    "                    'gene_sentence': gene_sentence\n",
    "                })\n",
    "\n",
    "    if not manifest_data:\n",
    "        raise ValueError(\"No matching image-sentence pairs were found. Check your directory paths and file naming conventions.\")\n",
    "\n",
    "    # Convert to a DataFrame\n",
    "    df = pd.DataFrame(manifest_data)\n",
    "    print(f\"Successfully found {len(df)} matching image-sentence pairs.\")\n",
    "\n",
    "    # Split into training and validation sets\n",
    "    if val_split > 0:\n",
    "        train_df, val_df = train_test_split(df, test_size=val_split, random_state=42)\n",
    "        val_path = os.path.join(output_dir, 'validation_manifest.csv')\n",
    "        val_df.to_csv(val_path, index=False, sep=',')\n",
    "        print(f\"Validation manifest saved to: {val_path} ({len(val_df)} samples)\")\n",
    "    else:\n",
    "        train_df = df\n",
    "\n",
    "    train_path = os.path.join(output_dir, 'train_manifest.csv')\n",
    "    train_df.to_csv(train_path, index=False, sep=',')\n",
    "    print(f\"Training manifest saved to: {train_path} ({len(train_df)} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d43ccd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_basedir = '/cwStorage/nodecw_group/jijh/hest_sentences'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b027c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_basedir = '/cwStorage/nodecw_group/jijh/hest_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e64ae828",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/cwStorage/nodecw_group/jijh/openclip_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28d81690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning directories to build manifest...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce42ec8bebe34b17b5d675a18ec9da41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Samples:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully found 307724 matching image-sentence pairs.\n",
      "Validation manifest saved to: /cwStorage/nodecw_group/jijh/openclip_train/validation_manifest.csv (15387 samples)\n",
      "Training manifest saved to: /cwStorage/nodecw_group/jijh/openclip_train/train_manifest.csv (292337 samples)\n"
     ]
    }
   ],
   "source": [
    "create_manifest(tiles_basedir=tile_basedir,\n",
    "                sentences_basedir=sentences_basedir,\n",
    "                output_dir=output_dir,\n",
    "                val_split=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5279c1",
   "metadata": {},
   "source": [
    "# Clip Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f6bdf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#           Cell: OpenCLIP Fine-tuning Launcher (Robust Version)\n",
    "# ==============================================================================\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import shlex\n",
    "import fcntl\n",
    "import select\n",
    "import datetime\n",
    "\n",
    "# --- 1. Training Configuration ---\n",
    "class OpenClipFinetuneConfig:\n",
    "    # --- Hardware & Distributed Training ---\n",
    "    GPU_IDS = [0, 1, 2]\n",
    "    NUM_GPUS = len(GPU_IDS)\n",
    "    DDP_MASTER_PORT = 29501\n",
    "\n",
    "    # --- Paths (使用绝对路径) ---\n",
    "    PROJECT_ROOT = \"/cwStorage/nodecw_group/jijh/openclip_train\"\n",
    "    OPEN_CLIP_SRC_DIR = os.path.join(PROJECT_ROOT, \"open_clip/src\")\n",
    "    TRAIN_MANIFEST = os.path.join(PROJECT_ROOT, \"train_manifest.csv\")\n",
    "    VAL_MANIFEST = os.path.join(PROJECT_ROOT, \"validation_manifest.csv\")\n",
    "    \n",
    "    TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    LOGS_DIR = os.path.join(PROJECT_ROOT, \"train_log\", f\"finetune_{TIMESTAMP}\")\n",
    "    \n",
    "    PRETRAINED_MODEL_PATH = os.path.join(PROJECT_ROOT, \"open_clip/CLIP-ViT-B-32/open_clip_model.safetensors\")\n",
    "\n",
    "    # --- Model & Data Hyperparameters ---\n",
    "    MODEL_NAME = \"ViT-B-32\"\n",
    "    DATASET_TYPE = \"csv\"\n",
    "    CSV_SEPARATOR = \",\"\n",
    "    CSV_IMG_KEY = \"image_path\"\n",
    "    CSV_CAPTION_KEY = \"gene_sentence\"\n",
    "\n",
    "    # --- Training Hyperparameters ---\n",
    "    EPOCHS = 10\n",
    "    BATCH_SIZE_PER_GPU = 128\n",
    "    LEARNING_RATE = 5e-6\n",
    "    WEIGHT_DECAY = 0.1\n",
    "    WARMUP_STEPS = 500\n",
    "    PRECISION = \"amp_bfloat16\" # For H100\n",
    "    NUM_WORKERS = 8\n",
    "\n",
    "    # --- Logging & Saving ---\n",
    "    REPORT_TO = \"tensorboard\"\n",
    "    SAVE_FREQUENCY = 1\n",
    "    LOG_EVERY_N_STEPS = 100\n",
    "\n",
    "    @classmethod\n",
    "    def build_command(cls):\n",
    "        \"\"\"\n",
    "        构建一个更简洁、更具兼容性的torchrun命令。\n",
    "        移除了所有可能导致'unrecognized arguments'错误的非核心参数。\n",
    "        \"\"\"\n",
    "        python_executable = sys.executable\n",
    "        \n",
    "        cmd = [\n",
    "            python_executable, \"-m\", \"torch.distributed.run\",\n",
    "            f\"--nproc_per_node={cls.NUM_GPUS}\",\n",
    "            f\"--master_port={cls.DDP_MASTER_PORT}\",\n",
    "            # 确保主脚本路径正确\n",
    "            os.path.join(cls.OPEN_CLIP_SRC_DIR, \"open_clip_train/main.py\"),\n",
    "            \n",
    "            # 核心参数\n",
    "            \"--model\", cls.MODEL_NAME,\n",
    "            \"--pretrained\", cls.PRETRAINED_MODEL_PATH,\n",
    "            \"--train-data\", cls.TRAIN_MANIFEST,\n",
    "            \"--val-data\", cls.VAL_MANIFEST,\n",
    "            \"--dataset-type\", cls.DATASET_TYPE,\n",
    "            \"--csv-separator\", cls.CSV_SEPARATOR,\n",
    "            \"--csv-img-key\", cls.CSV_IMG_KEY,\n",
    "            \"--csv-caption-key\", cls.CSV_CAPTION_KEY,\n",
    "            \"--logs\", cls.LOGS_DIR,\n",
    "            \n",
    "            # 训练超参数\n",
    "            \"--epochs\", str(cls.EPOCHS),\n",
    "            \"--batch-size\", str(cls.BATCH_SIZE_PER_GPU),\n",
    "            \"--lr\", str(cls.LEARNING_RATE),\n",
    "            \"--wd\", str(cls.WEIGHT_DECAY),\n",
    "            \"--warmup\", str(cls.WARMUP_STEPS),\n",
    "            \"--precision\", cls.PRECISION,\n",
    "            \n",
    "            # 系统和日志参数\n",
    "            \"--workers\", str(cls.NUM_WORKERS),\n",
    "            \"--report-to\", cls.REPORT_TO,\n",
    "            \"--save-frequency\", str(cls.SAVE_FREQUENCY),\n",
    "            \"--log-every-n-steps\", str(cls.LOG_EVERY_N_STEPS),\n",
    "        ]\n",
    "        return cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38992b7c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a032dd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 配置加载成功。日志和模型将保存到: /cwStorage/nodecw_group/jijh/openclip_train/train_log/finetune_20250619-212110\n"
     ]
    }
   ],
   "source": [
    "# --- 2. 实例化配置并准备环境 ---\n",
    "config = OpenClipFinetuneConfig()\n",
    "os.makedirs(config.LOGS_DIR, exist_ok=True)\n",
    "print(f\"✅ 配置加载成功。日志和模型将保存到: {config.LOGS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df7722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 所有关键文件和目录均已找到。\n",
      "\n",
      "================================================================================\n",
      "      即将执行以下精简命令:\n",
      "================================================================================\n",
      "/public/home/jijh/micromamba/envs/gigapath/bin/python -m torch.distributed.run --nproc_per_node=3 --master_port=29501 /cwStorage/nodecw_group/jijh/openclip_train/open_clip/src/open_clip_train/main.py --model ViT-B-32 --pretrained /cwStorage/nodecw_group/jijh/openclip_train/open_clip/CLIP-ViT-B-32/open_clip_model.safetensors --train-data /cwStorage/nodecw_group/jijh/openclip_train/train_manifest.csv --val-data /cwStorage/nodecw_group/jijh/openclip_train/validation_manifest.csv --dataset-type csv --csv-separator , --csv-img-key image_path --csv-caption-key gene_sentence --logs /cwStorage/nodecw_group/jijh/openclip_train/train_log/finetune_20250619-212110 --epochs 10 --batch-size 128 --lr 5e-06 --wd 0.1 --warmup 500 --precision amp_bfloat16 --workers 8 --report-to tensorboard --save-frequency 1 --log-every-n-steps 100\n",
      "================================================================================ \n",
      "\n",
      "--- [ 训练开始，实时日志如下 ] ---\n",
      "\n",
      "\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n"
     ]
    }
   ],
   "source": [
    "# --- 3. 检查关键文件和目录是否存在 ---\n",
    "paths_to_check = {\n",
    "    \"训练脚本\": os.path.join(config.OPEN_CLIP_SRC_DIR, \"open_clip_train/main.py\"),\n",
    "    \"训练清单文件\": config.TRAIN_MANIFEST,\n",
    "    \"验证清单文件\": config.VAL_MANIFEST,\n",
    "    \"预训练模型\": config.PRETRAINED_MODEL_PATH,\n",
    "}\n",
    "all_paths_ok = True\n",
    "for name, path in paths_to_check.items():\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"❌ 错误: {name} 未找到，路径: {path}\")\n",
    "        all_paths_ok = False\n",
    "\n",
    "if all_paths_ok:\n",
    "    print(\"✅ 所有关键文件和目录均已找到。\")\n",
    "    \n",
    "    # --- 4. 构建并执行命令 ---\n",
    "    command_list = config.build_command()\n",
    "    \n",
    "    execution_env = os.environ.copy()\n",
    "    execution_env[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, config.GPU_IDS))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"      即将执行以下精简命令:\")\n",
    "    print(\"=\"*80)\n",
    "    print(shlex.join(command_list))\n",
    "    print(\"=\"*80, \"\\n\")\n",
    "    \n",
    "    # --- 实时流式输出的执行逻辑 ---\n",
    "    try:\n",
    "        # 注意: 这里不再需要 `cwd` 参数，因为主脚本路径已经是绝对路径\n",
    "        process = subprocess.Popen(\n",
    "            command_list,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            encoding='utf-8',\n",
    "            errors='replace',\n",
    "            env=execution_env,\n",
    "            bufsize=1\n",
    "        )\n",
    "\n",
    "        fd = process.stdout.fileno()\n",
    "        fl = fcntl.fcntl(fd, fcntl.F_GETFL)\n",
    "        fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)\n",
    "\n",
    "        print(\"--- [ 训练开始，实时日志如下 ] ---\\n\")\n",
    "        \n",
    "        while True:\n",
    "            if process.poll() is not None:\n",
    "                break\n",
    "            \n",
    "            readable, _, _ = select.select([process.stdout], [], [], 0.1)\n",
    "            \n",
    "            if process.stdout in readable:\n",
    "                try:\n",
    "                    line = process.stdout.read()\n",
    "                    if line:\n",
    "                        sys.stdout.write(line)\n",
    "                        sys.stdout.flush()\n",
    "                except (TypeError, ValueError):\n",
    "                    break\n",
    "        \n",
    "        remaining_output, _ = process.communicate()\n",
    "        if remaining_output:\n",
    "            print(remaining_output.strip())\n",
    "\n",
    "        exit_code = process.returncode\n",
    "        print(f\"\\n--- [ 训练结束，退出码: {exit_code} ] ---\")\n",
    "        if exit_code == 0:\n",
    "            print(\"✅ 微调任务成功完成！\")\n",
    "        else:\n",
    "            print(\"❌ 微调任务失败。请检查上面的日志输出以获取详细错误信息。\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 执行过程中发生意外错误: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n请修正上述路径错误后再运行此单元格。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13180d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gigapath)",
   "language": "python",
   "name": "gigapath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
