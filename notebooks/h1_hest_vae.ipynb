{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 从 diffusers 导入 AutoencoderKL\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ====================================================\n",
    "# 1. 数据集部分：加载指定目录下所有子文件夹中的图像\n",
    "# ====================================================\n",
    "class TilesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    遍历指定根目录下所有子文件夹中的图像。\n",
    "    每个文件夹内图像尺寸一致，但不同文件夹间尺寸可能不同。\n",
    "    预处理时将图像调整为固定目标分辨率（例如512×512，确保宽高为8的倍数）。\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, target_size=(512, 512), image_exts=['.png', '.jpg', '.jpeg']):\n",
    "        self.root_dir = root_dir\n",
    "        self.target_size = target_size\n",
    "        self.image_paths = []\n",
    "        subfolders = [os.path.join(root_dir, d) for d in os.listdir(root_dir)\n",
    "                      if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        for folder in subfolders:\n",
    "            for ext in image_exts:\n",
    "                self.image_paths.extend(glob.glob(os.path.join(folder, f'*{ext}')))\n",
    "        print(f\"共找到 {len(self.image_paths)} 张图像\")\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(target_size, interpolation=Image.LANCZOS),\n",
    "            transforms.ToTensor(),  # [0,1]\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                 std=[0.5, 0.5, 0.5])  # [-1,1]\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# ====================================================\n",
    "# 2. 构建 AutoencoderKL 模型（从头训练）\n",
    "# ====================================================\n",
    "def create_vae_model(sample_size=512, latent_channels=4):\n",
    "    \"\"\"\n",
    "    构建从头训练的 VAE 模型：\n",
    "    使用 3 个下采样块，对应 block_out_channels=(128, 256, 512)；\n",
    "    同时明确指定下采样和上采样块类型，确保各层通道匹配。\n",
    "    \"\"\"\n",
    "    vae = AutoencoderKL(\n",
    "        in_channels=3,\n",
    "        out_channels=3,\n",
    "        sample_size=sample_size,      # 输入图像分辨率 512\n",
    "        latent_channels=latent_channels,  # latent 通道数\n",
    "        block_out_channels=(128, 256, 512),\n",
    "        down_block_types=[\n",
    "            \"DownEncoderBlock2D\",\n",
    "            \"DownEncoderBlock2D\",\n",
    "            \"DownEncoderBlock2D\"\n",
    "        ],\n",
    "        up_block_types=[\n",
    "            \"UpDecoderBlock2D\",\n",
    "            \"UpDecoderBlock2D\",\n",
    "            \"UpDecoderBlock2D\"\n",
    "        ],\n",
    "        layers_per_block=2,\n",
    "        norm_num_groups=32,\n",
    "        act_fn=\"silu\",\n",
    "        scaling_factor=0.18215,\n",
    "        force_upcast=True,\n",
    "        use_quant_conv=True,\n",
    "        use_post_quant_conv=True,\n",
    "        mid_block_add_attention=True\n",
    "    )\n",
    "    return vae\n",
    "\n",
    "# ====================================================\n",
    "# 3. 定义 VAE 损失函数\n",
    "# ====================================================\n",
    "def compute_vae_loss(vae, images):\n",
    "    \"\"\"\n",
    "    计算 VAE 损失：\n",
    "      1. 使用 vae.encode(images) 得到 latent 分布（包含 mean 与 logvar）\n",
    "      2. 采样 latent，并乘以 scaling_factor（例如 0.18215）\n",
    "      3. 使用 vae.decode(latents) 得到重构图像\n",
    "      4. 计算重构损失（MSE）和 KL 散度\n",
    "    返回总损失、重构损失和 KL 损失\n",
    "    \"\"\"\n",
    "    encoded = vae.encode(images)\n",
    "    latent_dist = encoded.latent_dist\n",
    "    scale = vae.config.scaling_factor  # 例如 0.18215\n",
    "    latents = latent_dist.sample() * scale\n",
    "    decoded = vae.decode(latents)\n",
    "    recon_images = decoded.sample\n",
    "\n",
    "    recon_loss = nn.functional.mse_loss(recon_images, images, reduction='mean')\n",
    "    mu = latent_dist.mean\n",
    "    logvar = latent_dist.logvar\n",
    "    kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    total_loss = recon_loss + kl_loss\n",
    "    return total_loss, recon_loss, kl_loss\n",
    "\n",
    "# ====================================================\n",
    "# 4. 训练过程（使用 tqdm 实时监控训练进度）\n",
    "# ====================================================\n",
    "def train_vae(vae, dataloader, optimizer, device, num_epochs=50):\n",
    "    vae.train()\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_recon = 0.0\n",
    "        epoch_kl = 0.0\n",
    "        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch}/{num_epochs}\")\n",
    "        for batch_idx, images in progress_bar:\n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss, recon_loss, kl_loss = compute_vae_loss(vae, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_recon += recon_loss.item()\n",
    "            epoch_kl += kl_loss.item()\n",
    "\n",
    "            progress_bar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Recon\": f\"{recon_loss.item():.4f}\",\n",
    "                \"KL\": f\"{kl_loss.item():.4f}\"\n",
    "            })\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        avg_recon = epoch_recon / len(dataloader)\n",
    "        avg_kl = epoch_kl / len(dataloader)\n",
    "        print(f\"Epoch {epoch} 完成, 平均 Loss: {avg_loss:.4f}, Recon: {avg_recon:.4f}, KL: {avg_kl:.4f}\")\n",
    "        torch.save(vae.state_dict(), f\"vae_epoch_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 参数设置\n",
    "root_dir = \"/cwStorage/nodecw_group/jijh/hest_output\"\n",
    "target_size = (512, 512)  # 固定目标分辨率，确保宽高为 8 的倍数\n",
    "batch_size = 4            # 可根据实际 GPU 显存调整\n",
    "num_epochs = 50\n",
    "learning_rate = 1e-4\n",
    "latent_channels = 4       # Stable Diffusion 默认 latent_channels 通常为 4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 构建数据集与 DataLoader\n",
    "dataset = TilesDataset(root_dir=root_dir, target_size=target_size)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 AutoencoderKL 模型并移动到设备上\n",
    "vae = create_vae_model(sample_size=target_size[0], latent_channels=latent_channels).to(device)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "\n",
    "# 开始训练（使用 tqdm 监控训练进度）\n",
    "train_vae(vae, dataloader, optimizer, device, num_epochs=num_epochs)\n",
    "\n",
    "print(\"训练结束，模型已保存。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()  # 释放 GPU 缓存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# 使用 tqdm.notebook 在 Jupyter 中显示进度条\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 导入 AutoencoderKL\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "# -----------------------------\n",
    "# 1. 定义数据集\n",
    "# -----------------------------\n",
    "class TilesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    遍历指定根目录下所有子文件夹中的图像，\n",
    "    并对图像做预处理：调整为固定目标分辨率、归一化至 [-1,1]。\n",
    "    \n",
    "    参数:\n",
    "      - root_dir: 数据根目录\n",
    "      - target_size: 目标尺寸（例如 (512,512)）\n",
    "      - image_exts: 图像扩展名列表\n",
    "      - max_samples: 如果不为 None，则随机选取 max_samples 个样本（用于快速微调）\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, target_size=(512, 512), image_exts=['.png', '.jpg', '.jpeg'], max_samples=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.target_size = target_size\n",
    "        self.image_paths = []\n",
    "        subfolders = [os.path.join(root_dir, d) for d in os.listdir(root_dir)\n",
    "                      if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        for folder in subfolders:\n",
    "            for ext in image_exts:\n",
    "                self.image_paths.extend(glob.glob(os.path.join(folder, f'*{ext}')))\n",
    "        # 若设置 max_samples，则随机选取指定数量的样本\n",
    "        if max_samples is not None and len(self.image_paths) > max_samples:\n",
    "            self.image_paths = random.sample(self.image_paths, max_samples)\n",
    "        print(f\"共找到 {len(self.image_paths)} 张图像\")\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(target_size, interpolation=Image.LANCZOS),\n",
    "            transforms.ToTensor(),  # [0,1]\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                 std=[0.5, 0.5, 0.5])  # 转换到 [-1,1]\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# -----------------------------\n",
    "# 2. 定义模型获取函数\n",
    "# -----------------------------\n",
    "def get_vae_model(sample_size=512, latent_channels=4, use_pretrained=True,\n",
    "                  pretrained_model_name_or_path=\"stabilityai/stable-diffusion-2-1-base\"):\n",
    "    \"\"\"\n",
    "    获取 VAE 模型。\n",
    "      - use_pretrained=True: 加载官方 Stable Diffusion 模型中的 VAE（subfolder=\"vae\"）\n",
    "      - use_pretrained=False: 从头构造新的 VAE 模型，采用 3 个下采样模块\n",
    "    \"\"\"\n",
    "    if use_pretrained:\n",
    "        # 加载预训练 VAE 模型（注意：确保安装的 diffusers 版本支持该调用）\n",
    "        vae = AutoencoderKL.from_pretrained(pretrained_model_name_or_path, subfolder=\"vae\")\n",
    "        print(\"Loaded pretrained VAE model.\")\n",
    "    else:\n",
    "        # 从头构造 VAE 模型，此处指定 3 个下采样/上采样模块，通道分别为 128,256,512\n",
    "        vae = AutoencoderKL(\n",
    "            in_channels=3,\n",
    "            out_channels=3,\n",
    "            sample_size=sample_size,\n",
    "            latent_channels=latent_channels,\n",
    "            block_out_channels=(128, 256, 512),\n",
    "            down_block_types=[\n",
    "                \"DownEncoderBlock2D\",\n",
    "                \"DownEncoderBlock2D\",\n",
    "                \"DownEncoderBlock2D\"\n",
    "            ],\n",
    "            up_block_types=[\n",
    "                \"UpDecoderBlock2D\",\n",
    "                \"UpDecoderBlock2D\",\n",
    "                \"UpDecoderBlock2D\"\n",
    "            ],\n",
    "            layers_per_block=2,\n",
    "            norm_num_groups=32,\n",
    "            act_fn=\"silu\",\n",
    "            scaling_factor=0.18215,\n",
    "            force_upcast=True,\n",
    "            use_quant_conv=True,\n",
    "            use_post_quant_conv=True,\n",
    "            mid_block_add_attention=True\n",
    "        )\n",
    "        print(\"Created new VAE model from scratch.\")\n",
    "    return vae\n",
    "\n",
    "# -----------------------------\n",
    "# 3. 定义 VAE 损失函数\n",
    "# -----------------------------\n",
    "def compute_vae_loss(vae, images):\n",
    "    \"\"\"\n",
    "    计算 VAE 损失：包含重构损失（MSE）和 KL 散度\n",
    "    \"\"\"\n",
    "    encoded = vae.encode(images)\n",
    "    latent_dist = encoded.latent_dist\n",
    "    scale = vae.config.scaling_factor  # 例如 0.18215\n",
    "    latents = latent_dist.sample() * scale\n",
    "    decoded = vae.decode(latents)\n",
    "    recon_images = decoded.sample\n",
    "\n",
    "    recon_loss = nn.functional.mse_loss(recon_images, images, reduction='mean')\n",
    "    mu = latent_dist.mean\n",
    "    logvar = latent_dist.logvar\n",
    "    kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    total_loss = recon_loss + kl_loss\n",
    "    return total_loss, recon_loss, kl_loss\n",
    "\n",
    "# -----------------------------\n",
    "# 4. 定义图像日志函数\n",
    "# -----------------------------\n",
    "def log_vae_images(vae, dataloader, device, epoch, save_images=False, output_dir='vae_logs'):\n",
    "    \"\"\"\n",
    "    从 dataloader 中取一个 batch，经过 VAE 重构后将原图与重构图拼接显示在 Notebook 中，\n",
    "    并可选择是否保存（默认不保存）。\n",
    "    \"\"\"\n",
    "    # 获取一个 batch\n",
    "    images = next(iter(dataloader))\n",
    "    images = images.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoded = vae.encode(images)\n",
    "        latent_dist = encoded.latent_dist\n",
    "        scale = vae.config.scaling_factor\n",
    "        latents = latent_dist.sample() * scale\n",
    "        decoded = vae.decode(latents)\n",
    "        recon_images = decoded.sample\n",
    "    \n",
    "    # 选择前 N 张图（上半部分为原图，下半部分为重构图）\n",
    "    num_show = min(8, images.size(0))\n",
    "    comparison = torch.cat([images[:num_show], recon_images[:num_show]], dim=0)\n",
    "    \n",
    "    # 生成图像网格\n",
    "    grid = make_grid(comparison, nrow=num_show, normalize=True, range=(-1, 1))\n",
    "    grid_np = grid.cpu().numpy().transpose(1, 2, 0)\n",
    "    \n",
    "    # 清除上次输出并显示当前图像\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(grid_np)\n",
    "    plt.title(f\"Epoch {epoch}: Original (top) vs Reconstructed (bottom)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # 如果选择保存，则写入文件\n",
    "    if save_images:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        save_path = os.path.join(output_dir, f'orig_recon_epoch_{epoch}.png')\n",
    "        save_image(comparison, save_path, nrow=num_show, normalize=True, range=(-1, 1))\n",
    "        print(f\"Saved comparison image to {save_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. 定义训练函数（使用混合精度加速）\n",
    "# -----------------------------\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_vae(vae, dataloader, optimizer, device, num_epochs=50, save_images=False):\n",
    "    \"\"\"\n",
    "    训练 VAE 模型，每个 epoch 后在 Notebook 中显示对比图像。\n",
    "    \"\"\"\n",
    "    vae.train()\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_recon = 0.0\n",
    "        epoch_kl = 0.0\n",
    "        \n",
    "        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch}/{num_epochs}\")\n",
    "        for batch_idx, images in progress_bar:\n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                loss, recon_loss, kl_loss = compute_vae_loss(vae, images)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_recon += recon_loss.item()\n",
    "            epoch_kl += kl_loss.item()\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Recon\": f\"{recon_loss.item():.4f}\",\n",
    "                \"KL\": f\"{kl_loss.item():.4f}\"\n",
    "            })\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        avg_recon = epoch_recon / len(dataloader)\n",
    "        avg_kl = epoch_kl / len(dataloader)\n",
    "        print(f\"Epoch {epoch} completed: Avg Loss: {avg_loss:.4f}, Recon: {avg_recon:.4f}, KL: {avg_kl:.4f}\")\n",
    "        \n",
    "        # 每个 epoch 结束后显示对比图像\n",
    "        log_vae_images(vae, dataloader, device, epoch, save_images=save_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# 使用 tqdm.notebook 在 Jupyter 中显示进度条\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# 1. 定义数据集\n",
    "# -----------------------------\n",
    "class TilesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    遍历指定根目录下所有子文件夹中的图像，\n",
    "    并对图像做预处理：调整为固定目标分辨率、归一化至 [-1,1]。\n",
    "    \n",
    "    参数:\n",
    "      - root_dir: 数据根目录\n",
    "      - target_size: 目标尺寸（例如 (512,512)）\n",
    "      - image_exts: 图像扩展名列表\n",
    "      - max_samples: 如果不为 None，则随机选取 max_samples 个样本（用于快速微调）\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, target_size=(512, 512), image_exts=['.png', '.jpg', '.jpeg'], max_samples=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.target_size = target_size\n",
    "        self.image_paths = []\n",
    "        subfolders = [os.path.join(root_dir, d) for d in os.listdir(root_dir)\n",
    "                      if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        for folder in subfolders:\n",
    "            for ext in image_exts:\n",
    "                self.image_paths.extend(glob.glob(os.path.join(folder, f'*{ext}')))\n",
    "        # 若设置 max_samples，则随机选取指定数量的样本\n",
    "        if max_samples is not None and len(self.image_paths) > max_samples:\n",
    "            self.image_paths = random.sample(self.image_paths, max_samples)\n",
    "        print(f\"共找到 {len(self.image_paths)} 张图像\")\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(target_size, interpolation=Image.LANCZOS),\n",
    "            transforms.ToTensor(),  # [0,1]\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                 std=[0.5, 0.5, 0.5])  # 转换到 [-1,1]\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# -----------------------------\n",
    "# 2. 定义模型获取函数\n",
    "# -----------------------------\n",
    "# 默认使用 Tiny AutoEncoder (TAESD) 作为预训练模型\n",
    "from diffusers import AutoencoderTiny\n",
    "\n",
    "def get_vae_model(sample_size=512, latent_channels=4, use_pretrained=True, sd_version='v2.1'):\n",
    "    \"\"\"\n",
    "    获取 VAE 模型。\n",
    "      - use_pretrained=True（默认）：加载预训练的 Tiny AutoEncoder 模型（保持参数为 FP32）。\n",
    "          sd_version: 'v2.1'（默认）加载适用于 Stable Diffusion v2.1 的 Tiny AutoEncoder，\n",
    "                      若设置为 'sdxl' 则加载适用于 SDXL 的版本。\n",
    "      - use_pretrained=False：从头构造新的 Tiny AutoEncoder 模型（采用 TAESD 默认配置）。\n",
    "    \"\"\"\n",
    "    if use_pretrained:\n",
    "        if sd_version == 'v2.1':\n",
    "            vae = AutoencoderTiny.from_pretrained(\"madebyollin/taesd\", torch_dtype=torch.float32)\n",
    "        elif sd_version == 'sdxl':\n",
    "            vae = AutoencoderTiny.from_pretrained(\"madebyollin/taesdxl\", torch_dtype=torch.float32)\n",
    "        else:\n",
    "            raise ValueError(\"sd_version 需为 'v2.1' 或 'sdxl'\")\n",
    "        print(\"Loaded pretrained Tiny AutoEncoder model.\")\n",
    "    else:\n",
    "        # 从头构造 Tiny AutoEncoder 模型，参数取自 TAESD 默认设置\n",
    "        vae = AutoencoderTiny(\n",
    "            in_channels=3,\n",
    "            out_channels=3,\n",
    "            encoder_block_out_channels=(64, 64, 64, 64),\n",
    "            decoder_block_out_channels=(64, 64, 64, 64),\n",
    "            act_fn=\"relu\",\n",
    "            latent_channels=latent_channels,\n",
    "            upsampling_scaling_factor=2,\n",
    "            num_encoder_blocks=(1, 3, 3, 3),\n",
    "            num_decoder_blocks=(3, 3, 3, 1),\n",
    "            latent_magnitude=3.0,\n",
    "            latent_shift=0.5,\n",
    "            force_upcast=False,\n",
    "            scaling_factor=1.0,\n",
    "            shift_factor=0.0\n",
    "        )\n",
    "        print(\"Created new Tiny AutoEncoder model from scratch.\")\n",
    "    return vae\n",
    "\n",
    "# -----------------------------\n",
    "# 3. 定义 VAE 损失函数\n",
    "# -----------------------------\n",
    "def compute_vae_loss(vae, images):\n",
    "    \"\"\"\n",
    "    计算 VAE 损失：包含重构损失（MSE）和 KL 散度\n",
    "    \"\"\"\n",
    "    encoded = vae.encode(images)\n",
    "    latent_dist = encoded.latents if hasattr(encoded, \"latents\") else encoded.latent_dist\n",
    "    if hasattr(encoded, \"latent_dist\"):\n",
    "        scale = vae.config.scaling_factor if hasattr(vae.config, \"scaling_factor\") else 1.0\n",
    "        latents = latent_dist.sample() * scale\n",
    "    else:\n",
    "        latents = encoded.latents\n",
    "    decoded = vae.decode(latents)\n",
    "    recon_images = decoded.sample if hasattr(decoded, \"sample\") else decoded\n",
    "\n",
    "    recon_loss = nn.functional.mse_loss(recon_images, images, reduction='mean')\n",
    "    \n",
    "    if hasattr(encoded, \"latent_dist\"):\n",
    "        mu = latent_dist.mean\n",
    "        logvar = latent_dist.logvar\n",
    "        kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    else:\n",
    "        kl_loss = torch.tensor(0.0, device=images.device)\n",
    "    \n",
    "    total_loss = recon_loss + kl_loss\n",
    "    return total_loss, recon_loss, kl_loss\n",
    "\n",
    "# -----------------------------\n",
    "# 4. 定义图像日志函数\n",
    "# -----------------------------\n",
    "def log_vae_images(vae, dataloader, device, epoch, save_images=False, output_dir='vae_logs'):\n",
    "    \"\"\"\n",
    "    从 dataloader 中取一个 batch，经过 VAE 重构后将原图与重构图拼接显示在 Notebook 中，\n",
    "    并可选择是否保存（默认不保存）。\n",
    "    \"\"\"\n",
    "    images = next(iter(dataloader))\n",
    "    images = images.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoded = vae.encode(images)\n",
    "        latent_dist = encoded.latents if hasattr(encoded, \"latents\") else encoded.latent_dist\n",
    "        if hasattr(encoded, \"latent_dist\"):\n",
    "            scale = vae.config.scaling_factor if hasattr(vae.config, \"scaling_factor\") else 1.0\n",
    "            latents = latent_dist.sample() * scale\n",
    "        else:\n",
    "            latents = encoded.latents\n",
    "        decoded = vae.decode(latents)\n",
    "        recon_images = decoded.sample if hasattr(decoded, \"sample\") else decoded\n",
    "    \n",
    "    num_show = min(8, images.size(0))\n",
    "    comparison = torch.cat([images[:num_show], recon_images[:num_show]], dim=0)\n",
    "    \n",
    "    grid = make_grid(comparison, nrow=num_show, normalize=True, value_range=(-1, 1))\n",
    "    grid_np = grid.cpu().numpy().transpose(1, 2, 0)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(grid_np)\n",
    "    plt.title(f\"Epoch {epoch}: Original (top) vs Reconstructed (bottom)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    if save_images:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        save_path = os.path.join(output_dir, f'orig_recon_epoch_{epoch}.png')\n",
    "        save_image(comparison, save_path, nrow=num_show, normalize=True, value_range=(-1, 1))\n",
    "        print(f\"Saved comparison image to {save_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. 定义训练函数（使用混合精度加速）\n",
    "# -----------------------------\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_vae(vae, dataloader, optimizer, device, num_epochs=50, save_images=False):\n",
    "    \"\"\"\n",
    "    训练 VAE 模型，每个 epoch 后在 Notebook 中显示对比图像。\n",
    "    \"\"\"\n",
    "    vae.train()\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_recon = 0.0\n",
    "        epoch_kl = 0.0\n",
    "        \n",
    "        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch}/{num_epochs}\")\n",
    "        for batch_idx, images in progress_bar:\n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                loss, recon_loss, kl_loss = compute_vae_loss(vae, images)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_recon += recon_loss.item()\n",
    "            epoch_kl += kl_loss.item()\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Recon\": f\"{recon_loss.item():.4f}\",\n",
    "                \"KL\": f\"{kl_loss.item():.4f}\"\n",
    "            })\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        avg_recon = epoch_recon / len(dataloader)\n",
    "        avg_kl = epoch_kl / len(dataloader)\n",
    "        print(f\"Epoch {epoch} completed: Avg Loss: {avg_loss:.4f}, Recon: {avg_recon:.4f}, KL: {avg_kl:.4f}\")\n",
    "        \n",
    "        # 每个 epoch 结束后显示对比图像\n",
    "        log_vae_images(vae, dataloader, device, epoch, save_images=save_images)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. 定义测试函数：利用训练好的 VAE 从噪声生成图像，并用 grid 对比显示原始噪声与生成的图像\n",
    "# -----------------------------\n",
    "def test_vae_from_noise(vae, device):\n",
    "    \"\"\"\n",
    "    利用训练好的 VAE，从随机噪声生成一张图片，\n",
    "    同时构造一个对比 grid：左边显示经过通道平均归一化的噪声图，\n",
    "    右边显示 VAE 解码生成的图像。\n",
    "    \"\"\"\n",
    "    vae.eval()\n",
    "    # 对于 512x512 图像，latent 尺寸通常为 (1, 4, 64, 64)\n",
    "    latent_shape = (1, 4, 64, 64)\n",
    "    latent_noise = torch.randn(latent_shape, device=device)\n",
    "    \n",
    "    # 获取模型配置中的 scaling_factor 和 shift_factor（若不存在，则默认 1.0 和 0.0）\n",
    "    scaling_factor = getattr(vae.config, \"scaling_factor\", 1.0)\n",
    "    shift_factor = getattr(vae.config, \"shift_factor\", 0.0)\n",
    "    # 按训练时的处理方式调整 latent\n",
    "    latent_adjusted = (latent_noise / scaling_factor) + shift_factor\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        decoded = vae.decode(latent_adjusted, return_dict=False)[0]\n",
    "        # VAE 输出通常在 [-1,1]，这里映射到 [0,1]\n",
    "        generated_img = (decoded.clamp(-1,1) + 1) / 2  # shape: (1, 3, H, W)\n",
    "    \n",
    "    # 为了显示原始噪声，取 latent_noise 在通道维度求平均，得到 (1,1,H,W)\n",
    "    latent_vis = latent_noise.mean(dim=1, keepdim=True)\n",
    "    latent_vis = (latent_vis - latent_vis.min()) / (latent_vis.max() - latent_vis.min() + 1e-5)\n",
    "    latent_vis = latent_vis.repeat(1, 3, 1, 1)\n",
    "    \n",
    "    # 如果 latent_vis 的尺寸与生成图像不一致，进行上采样\n",
    "    if latent_vis.shape[-2:] != generated_img.shape[-2:]:\n",
    "        latent_vis = F.interpolate(latent_vis, size=generated_img.shape[-2:], mode='bilinear', align_corners=False)\n",
    "    \n",
    "    # 合并两个图像到一个 batch 中：上方显示噪声，下方显示生成图像\n",
    "    comparison = torch.cat([latent_vis, generated_img], dim=0)\n",
    "    grid = make_grid(comparison, nrow=2, normalize=True, value_range=(0, 1))\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Left, nosie  |  Right, generated image\")\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 7. 定义主函数（Notebook 中调用）\n",
    "# -----------------------------\n",
    "def main(root_dir,\n",
    "         target_size=(512, 512),\n",
    "         batch_size=8,\n",
    "         num_epochs=50,\n",
    "         learning_rate=1e-4,\n",
    "         latent_channels=4,\n",
    "         use_pretrained=True,\n",
    "         sd_version='v2.1',   # 若要使用 SDXL 版本则传入 'sdxl'\n",
    "         num_samples=1000,\n",
    "         save_images=False):\n",
    "    \"\"\"\n",
    "    主函数：\n",
    "      - root_dir: 图像数据所在目录\n",
    "      - target_size: 图像预处理目标尺寸\n",
    "      - batch_size: 批次大小（可根据 GPU 显存调整）\n",
    "      - num_epochs: 训练轮数\n",
    "      - learning_rate: 学习率\n",
    "      - latent_channels: VAE 的 latent 通道数\n",
    "      - use_pretrained: 是否加载预训练的 Tiny AutoEncoder 模型（默认 True）\n",
    "      - sd_version: 'v2.1'（默认）或 'sdxl'\n",
    "      - num_samples: 使用的数据样本数（随机抽取，用于微调）\n",
    "      - save_images: 是否保存每个 epoch 输出的对比图像（默认 False，仅显示）\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # 构造数据集，仅选取 num_samples 个样本（用于快速微调）\n",
    "    dataset = TilesDataset(root_dir=root_dir, target_size=target_size, max_samples=num_samples)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n",
    "                            num_workers=4, pin_memory=True)\n",
    "    \n",
    "    # 获取 VAE 模型（预训练 Tiny AutoEncoder 或从头构造）\n",
    "    vae = get_vae_model(sample_size=target_size[0],\n",
    "                        latent_channels=latent_channels,\n",
    "                        use_pretrained=use_pretrained,\n",
    "                        sd_version=sd_version)\n",
    "    vae.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # 开始训练\n",
    "    train_vae(vae, dataloader, optimizer, device, num_epochs=num_epochs, save_images=save_images)\n",
    "    \n",
    "    print(\"训练结束！\")\n",
    "    # 返回训练好的模型，便于后续测试使用\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_URL\"] = \"https://hf-mirror.com/\"  # 请将此处替换为实际可用的镜像地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 9. 运行微调，并进行测试\n",
    "# -----------------------------\n",
    "trained_vae = main(\n",
    "    root_dir=\"/cwStorage/nodecw_group/jijh/hest_output\",\n",
    "    target_size=(512, 512),\n",
    "    batch_size=32,\n",
    "    num_epochs=10,\n",
    "    learning_rate=1e-4,\n",
    "    latent_channels=4,\n",
    "    use_pretrained=True,    # 使用预训练的 Tiny AutoEncoder 模型（默认）\n",
    "    sd_version='v2.1',      # 或者使用 'sdxl' 版本\n",
    "    num_samples=1000,       # 随机抽取 1000 个样本用于微调\n",
    "    save_images=False       # 默认仅在 Notebook 中显示图像，不保存；如需保存设为 True\n",
    ")\n",
    "\n",
    "# 使用训练好的 VAE 模型，从随机噪声生成一张图片，并在 grid 中对比显示噪声与生成图像\n",
    "test_vae_from_noise(trained_vae, torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用训练好的 VAE 模型，从随机噪声生成一张图片，并在 grid 中对比显示噪声与生成图像\n",
    "test_vae_from_noise(trained_vae, torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained_vae to /cwStorage/nodecw_group/jijh/model_path that could be directly loaded without defining the model\n",
    "torch.save(trained_vae.state_dict(), '/cwStorage/nodecw_group/jijh/model_path/vae_epoch_10.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用训练好的 VAE 模型，从数据集中随机选取一张图像，编码后解码生成图像\n",
    "\n",
    "# 从数据集中随机选取一张图像\n",
    "image = dataset[random.randint(0, len(dataset) - 1)]\n",
    "image = image.unsqueeze(0).to(device)\n",
    "\n",
    "# 编码并解码生成图像\n",
    "with torch.no_grad():\n",
    "    encoded = trained_vae.encode(image)\n",
    "    decoded = trained_vae.decode(encoded.latents)\n",
    "    generated_img = decoded.sample\n",
    "\n",
    "# 显示原始图像与生成图像\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image[0].cpu().permute(1, 2, 0).numpy())\n",
    "plt.axis('off')\n",
    "plt.title(\"Original Image\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(generated_img[0].cpu().permute(1, 2, 0).numpy())\n",
    "plt.axis('off')\n",
    "plt.title(\"Generated Image\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Diffusion 模型的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time  # 新增time模块，用于计时\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Dataset Definition\n",
    "# -----------------------------\n",
    "class TilesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Traverse all images in subdirectories under the given root directory and preprocess them:\n",
    "    resize to target_size and normalize to [-1, 1].\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, target_size=(512, 512), image_exts=['.png', '.jpg', '.jpeg'], max_samples=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.target_size = target_size\n",
    "        self.image_paths = []\n",
    "        subfolders = [os.path.join(root_dir, d) for d in os.listdir(root_dir)\n",
    "                      if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        for folder in subfolders:\n",
    "            for ext in image_exts:\n",
    "                self.image_paths.extend(glob.glob(os.path.join(folder, f'*{ext}')))\n",
    "        if max_samples is not None and len(self.image_paths) > max_samples:\n",
    "            self.image_paths = random.sample(self.image_paths, max_samples)\n",
    "        print(f\"Found {len(self.image_paths)} images.\")\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(target_size, interpolation=Image.LANCZOS),\n",
    "            transforms.ToTensor(),  # [0,1]\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                 std=[0.5, 0.5, 0.5])  # Map to [-1,1]\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Diffusion Model Definition (UNet & Scheduler)\n",
    "# -----------------------------\n",
    "def get_diffusion_model():\n",
    "    \"\"\"\n",
    "    Returns the UNet model and noise scheduler for latent diffusion.\n",
    "    \"\"\"\n",
    "    from diffusers import UNet2DModel, DDPMScheduler\n",
    "    unet = UNet2DModel(\n",
    "        sample_size=64,         # Corresponds to the VAE latent space size (e.g., 64x64)\n",
    "        in_channels=4,          # Number of channels in the VAE latent space\n",
    "        out_channels=4,\n",
    "        layers_per_block=2,\n",
    "        block_out_channels=(64, 128, 256, 512),\n",
    "        down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\"),\n",
    "        up_block_types=(\"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\")\n",
    "    )\n",
    "    scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "    print(\"Diffusion model and scheduler created.\")\n",
    "    return unet, scheduler\n",
    "\n",
    "def sample_latent_diffusion(unet, vae, scheduler, device, num_inference_steps=50, sample_batch_size=4):\n",
    "    \"\"\"\n",
    "    Starting from random noise, progressively denoise using the UNet, then decode using the fixed VAE.\n",
    "    Returns 4 generated images arranged in one row.\n",
    "    \"\"\"\n",
    "    unet.eval()\n",
    "    vae.eval()\n",
    "    \n",
    "    # Assume the VAE latent space size is (batch, 4, 64, 64)\n",
    "    latent_shape = (sample_batch_size, 4, 64, 64)\n",
    "    latents = torch.randn(latent_shape, device=device)\n",
    "    \n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    for t in scheduler.timesteps:\n",
    "        with torch.no_grad():\n",
    "            noise_pred = unet(latents, t).sample\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "    with torch.no_grad():\n",
    "        decoded = vae.decode(latents)\n",
    "        images = decoded.sample if hasattr(decoded, \"sample\") else decoded\n",
    "    images = (images.clamp(-1, 1) + 1) / 2  # Map to [0,1]\n",
    "    return images\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Checkpoint Save and Load Functions\n",
    "# -----------------------------\n",
    "def save_checkpoint(epoch, unet, optimizer, checkpoint_path):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'unet_state_dict': unet.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch} to {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_path, unet, optimizer):\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        unet.load_state_dict(checkpoint['unet_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Loaded checkpoint from {checkpoint_path}, resuming at epoch {start_epoch}\")\n",
    "        return start_epoch\n",
    "    else:\n",
    "        print(\"No checkpoint found, training from scratch.\")\n",
    "        return 0\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Diffusion Model Training Function\n",
    "# -----------------------------\n",
    "def train_diffusion_model(vae, unet, scheduler, dataloader, optimizer, device,\n",
    "                          num_epochs=10, checkpoint_path=None, start_epoch=0):\n",
    "    \"\"\"\n",
    "    Train the diffusion model (UNet) in the latent space of the fixed VAE.\n",
    "    在每个 epoch 结束时，输出当前 epoch 的平均 loss（以及与上个 epoch 的变化量），当前 epoch 耗时以及预计剩余训练时间。\n",
    "    同时，在每个 epoch 结束时显示生成的样本（4 张图片排列成一行）。\n",
    "    Supports resuming training from checkpoint using checkpoint_path.\n",
    "    \"\"\"\n",
    "    unet.train()\n",
    "    previous_avg_loss = None  # 用于记录上一个 epoch 的平均 loss\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_start_time = time.time()  # 记录本 epoch 开始时间\n",
    "        \n",
    "        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader),\n",
    "                            desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch_idx, images in progress_bar:\n",
    "            images = images.to(device)\n",
    "            # Encode images to latent space using the fixed VAE (VAE parameters are not updated)\n",
    "            with torch.no_grad():\n",
    "                encoded = vae.encode(images)\n",
    "                if hasattr(encoded, \"latent_dist\"):\n",
    "                    scale = vae.config.scaling_factor if hasattr(vae.config, \"scaling_factor\") else 1.0\n",
    "                    latent_dist = encoded.latent_dist\n",
    "                    latents = latent_dist.sample() * scale\n",
    "                else:\n",
    "                    latents = encoded.latents\n",
    "\n",
    "            # Generate random noise and corresponding timesteps\n",
    "            noise = torch.randn_like(latents)\n",
    "            b = latents.shape[0]\n",
    "            timesteps = torch.randint(0, scheduler.num_train_timesteps, (b,), device=device).long()\n",
    "            noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
    "            \n",
    "            # UNet predicts the noise residual\n",
    "            noise_pred = unet(noisy_latents, timesteps).sample\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        \n",
    "        # 计算当前 epoch 的平均 loss\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        # 计算本 epoch耗时\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        # 估计剩余训练时间\n",
    "        remaining_epochs = num_epochs - epoch - 1\n",
    "        estimated_remaining = epoch_duration * remaining_epochs\n",
    "        \n",
    "        # 如果有上一个 epoch 的 loss，则计算 loss 的变化\n",
    "        loss_change_str = \"\"\n",
    "        if previous_avg_loss is not None:\n",
    "            loss_change = avg_loss - previous_avg_loss\n",
    "            loss_change_str = f\", Loss change: {loss_change:.4f}\"\n",
    "        previous_avg_loss = avg_loss\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1} completed, Average Loss: {avg_loss:.4f}{loss_change_str}, \"\n",
    "              f\"Time taken: {str(datetime.timedelta(seconds=int(epoch_duration)))}; \"\n",
    "              f\"Estimated remaining time: {str(datetime.timedelta(seconds=int(estimated_remaining)))}\")\n",
    "        \n",
    "        # Display generated samples (4 images in one row) at the end of each epoch\n",
    "        gen_images = sample_latent_diffusion(unet, vae, scheduler, device, num_inference_steps=50, sample_batch_size=4)\n",
    "        grid = make_grid(gen_images, nrow=4, normalize=True, value_range=(0, 1))\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Epoch {epoch+1} Sample Output\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Save checkpoint if checkpoint_path is provided\n",
    "        if checkpoint_path is not None:\n",
    "            save_checkpoint(epoch, unet, optimizer, checkpoint_path)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Main Function (Model Definition, Data Loading, Resuming Training, and Final Output)\n",
    "# -----------------------------\n",
    "def main_diffusion_training(\n",
    "    root_dir=\"/cwStorage/nodecw_group/jijh/hest_output\",\n",
    "    max_samples=12000,\n",
    "    batch_size=54,\n",
    "    num_epochs=15,\n",
    "    learning_rate=1e-4,\n",
    "    checkpoint_path=None\n",
    "):\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # # Dataset configuration\n",
    "    # root_dir = \"/cwStorage/nodecw_group/jijh/hest_output\"  # Replace with your dataset path\n",
    "    # max_samples = 12000\n",
    "    # batch_size = 54\n",
    "    # num_epochs = 15\n",
    "    # learning_rate = 1e-4\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = TilesDataset(root_dir=root_dir, target_size=(512, 512), max_samples=max_samples)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n",
    "                            num_workers=4, pin_memory=True)\n",
    "    \n",
    "    # Assume that the trained VAE is already loaded and fixed (do not update)\n",
    "    global trained_vae  # This should be defined in your environment\n",
    "    trained_vae.to(device)\n",
    "    trained_vae.eval()\n",
    "    print(\"Fixed VAE loaded and set to evaluation mode.\")\n",
    "    \n",
    "    # Define diffusion model and scheduler\n",
    "    unet, scheduler = get_diffusion_model()\n",
    "    unet.to(device)\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(unet.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Construct checkpoint file name with parameter information\n",
    "    checkpoint_path = f\"/cwStorage/nodecw_group/jijh/model_path/diffusion_checkpoint_bs{batch_size}_ep{num_epochs}_lr{learning_rate}_ms{max_samples}.pt\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        start_epoch = load_checkpoint(checkpoint_path, unet, optimizer)\n",
    "    else:\n",
    "        print(\"No existing checkpoint found. Starting training from scratch.\")\n",
    "        start_epoch = 0\n",
    "    \n",
    "    print(\"Starting diffusion model training...\")\n",
    "    train_diffusion_model(vae=trained_vae, unet=unet, scheduler=scheduler,\n",
    "                          dataloader=dataloader, optimizer=optimizer, device=device,\n",
    "                          num_epochs=num_epochs, checkpoint_path=checkpoint_path, start_epoch=start_epoch)\n",
    "    \n",
    "    # Final output: generate a set of sample images to display final results\n",
    "    print(\"Training completed. Generating final sample outputs...\")\n",
    "    final_images = sample_latent_diffusion(unet, trained_vae, scheduler, device, num_inference_steps=50, sample_batch_size=4)\n",
    "    grid = make_grid(final_images, nrow=4, normalize=True, value_range=(0, 1))\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Final Generated Samples\")\n",
    "    plt.show()\n",
    "    \n",
    "    return unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6. Run Training Process\n",
    "# -----------------------------\n",
    "\n",
    "# Ensure that trained_vae is loaded before running this script.\n",
    "trained_unet = main_diffusion_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_unet = main_diffusion_training(\n",
    "    root_dir=\"/cwStorage/nodecw_group/jijh/hest_output\",\n",
    "    max_samples=30000,\n",
    "    batch_size=64,\n",
    "    num_epochs=15,\n",
    "    learning_rate=1e-4,\n",
    "    checkpoint_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "final_images = sample_latent_diffusion(another_unet, trained_vae, scheduler, device, num_inference_steps=100, sample_batch_size=16)\n",
    "grid = make_grid(final_images, nrow=4, normalize=True, value_range=(0, 1))\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"30k training samples, 15 epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "final_images = sample_latent_diffusion(trained_unet, trained_vae, scheduler, device, num_inference_steps=100, sample_batch_size=16)\n",
    "grid = make_grid(final_images, nrow=4, normalize=True, value_range=(0, 1))\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"10k training samples, 15 epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_sample_unet = main_diffusion_training(\n",
    "    root_dir=\"/cwStorage/nodecw_group/jijh/hest_output\",\n",
    "    max_samples=60000,\n",
    "    batch_size=64,\n",
    "    num_epochs=15,\n",
    "    learning_rate=1e-4,\n",
    "    checkpoint_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "final_images = sample_latent_diffusion(more_sample_unet, trained_vae, scheduler, device, num_inference_steps=100, sample_batch_size=16)\n",
    "grid = make_grid(final_images, nrow=4, normalize=True, value_range=(0, 1))\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"60k training samples, 15 epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sample_unet = main_diffusion_training(\n",
    "    root_dir=\"/cwStorage/nodecw_group/jijh/hest_output\",\n",
    "    max_samples=None,\n",
    "    batch_size=64,\n",
    "    num_epochs=15,\n",
    "    learning_rate=1e-4,\n",
    "    checkpoint_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "final_images = sample_latent_diffusion(all_sample_unet, trained_vae, scheduler, device, num_inference_steps=100, sample_batch_size=16)\n",
    "grid = make_grid(final_images, nrow=4, normalize=True, value_range=(0, 1))\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"310k training samples, 15 epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sample_unet = main_diffusion_training(\n",
    "    root_dir=\"/cwStorage/nodecw_group/jijh/hest_output\",\n",
    "    max_samples=None,\n",
    "    batch_size=64,\n",
    "    num_epochs=30,\n",
    "    learning_rate=1e-4,\n",
    "    checkpoint_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "final_images = sample_latent_diffusion(all_sample_unet, trained_vae, scheduler, device, num_inference_steps=100, sample_batch_size=16)\n",
    "grid = make_grid(final_images, nrow=4, normalize=True, value_range=(0, 1))\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"310k training samples, 30 epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python GPU",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
