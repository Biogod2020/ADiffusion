{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import skimage.io as io\n",
    "import random\n",
    "from torch_geometric.data import Data\n",
    "os.chdir('/public/home/jijh/diffusion_project/ADiffusion')\n",
    "import importlib\n",
    "import src.preprocessing.data_process\n",
    "importlib.reload(src.preprocessing.data_process)\n",
    "from src.preprocessing.data_process import extract_patches, create_graph_data_dict, construct_affinity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    \"\"\"Initialize random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "# Set random seed for reproducibility\n",
    "random_seed = 0\n",
    "seed_everything(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "file_dir = \"/public/home/jijh/st_project/cellbin_analysis/spatial_variation/wx_data/\"  # Directory containing the data files\n",
    "files = os.listdir(file_dir)  # List all files in the directory\n",
    "files = [i for i in files if i.endswith(\".h5ad\") and \"month\" in i]  # Filter files to include only those ending with \".h5ad\" and containing \"month\"\n",
    "file_paths = [os.path.join(file_dir, i) for i in files]  # Create full file paths for the filtered files\n",
    "adatas = {}  # Initialize an empty dictionary to store AnnData objects\n",
    "\n",
    "# Read each file and store the AnnData object in the dictionary\n",
    "for i in range(len(file_paths)):\n",
    "    adatas[files[i].split(\".\")[0]] = sc.read(file_paths[i])\n",
    "\n",
    "# Preprocess each AnnData object\n",
    "for key in tqdm(adatas.keys(), desc=\"Preprocessing datasets\"):\n",
    "    sc.pp.normalize_total(adatas[key], target_sum=1e4)  # Normalize counts per cell\n",
    "    sc.pp.log1p(adatas[key])  # Logarithmize the data\n",
    "    adatas[key].layers[\"raw\"] = adatas[key].X.copy()  # Store the raw data in the \"raw\" layer\n",
    "    sc.pp.scale(adatas[key], max_value=10)  # Scale the data to have a maximum value of 10\n",
    "    sc.tl.pca(adatas[key], svd_solver=\"arpack\")  # Perform PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract spatial coordinates for each cell\n",
    "cell_coords = {}\n",
    "for key in adatas.keys():\n",
    "    cell_coords[key] = adatas[key].obsm[\"spatial\"].copy()\n",
    "neighbors = {}\n",
    "for key in cell_coords.keys():\n",
    "    neighbors[key] = construct_affinity_matrix(cell_coords[key], mode='number', cutoff=10)\n",
    "\n",
    "# Load the plaque dataset\n",
    "img_dir = \"/public/home/jijh/st_project/cellbin_analysis/spatial_variation/wx_data/protein_seg_result/\"\n",
    "img_files = os.listdir(img_dir)\n",
    "img_files = [i for i in img_files if i.endswith(\".tiff\") and \"plaque\" in i]\n",
    "# Read the images\n",
    "\n",
    "imgs = {}\n",
    "for i in range(len(img_files)):\n",
    "    imgs[img_files[i].split(\".\")[0]] = io.imread(os.path.join(img_dir, img_files[i]))\n",
    "imgs.keys()\n",
    "# Rename the imgs to match the adata keys\n",
    "for key in list(imgs.keys()):\n",
    "    parts = key.split(\"_\")\n",
    "    if len(parts) > 1:\n",
    "        new_key = parts[1] + \"_\" + parts[2]\n",
    "        imgs[new_key] = imgs.pop(key)\n",
    "\n",
    "\n",
    "# Extract patches from the images\n",
    "patches = {}\n",
    "for key in imgs.keys():\n",
    "    patches[key] = extract_patches(imgs[key], cell_coords[key], patch_size=128)\n",
    "\n",
    "# Convert the patches to binary and calculate the area of positive pixels for each patch\n",
    "binary_patches = {}\n",
    "for key in patches.keys():\n",
    "    binary_patches[key] = [patch > 0 for patch in patches[key]]\n",
    "# Calculate the area of positive pixels for each patch\n",
    "areas = {}\n",
    "for key in binary_patches.keys():\n",
    "    areas[key] = [np.sum(patch) for patch in binary_patches[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph dictionary\n",
    "graph_data_dict = create_graph_data_dict(adatas, areas, neighbors, cell_coords, embeddings=[\"X\"])\n",
    "\n",
    "\n",
    "# Efficiently convert patches to tensors in batch\n",
    "for key, graph in tqdm(graph_data_dict.items(), desc=\"Adding patches to graph data\"):\n",
    "    # Ensure patches[key] is a list of NumPy arrays\n",
    "    patches_tensor = torch.tensor(np.array(patches[key]), dtype=torch.float)  # Convert patches to a single tensor efficiently\n",
    "    graph.patches = patches_tensor\n",
    "# Normalize the edge_attr\n",
    "for key, graph in tqdm(graph_data_dict.items(), desc=\"Normalizing edge_attr\"):\n",
    "    graph.edge_attr = graph.edge_attr / graph.edge_attr.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Reload the saved dictionaries\n",
    "save_dir = \"/public/home/jijh/diffusion_project/data_storage\"\n",
    "graph_data_dict = torch.load(os.path.join(save_dir,\"graph_data_dict.pth\"))\n",
    "positive_nodes_dict = torch.load(os.path.join(save_dir,\"positive_nodes_dict.pth\"))\n",
    "\n",
    "graph_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transform functions from torchvision\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "# Create a dataset of only positive patches\n",
    "class PositivePatchDataset(Dataset):\n",
    "    def __init__(self, patches, device):\n",
    "        self.patches = patches.to(device)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.patches.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patch = self.patches[idx]  # [1,128,128]\n",
    "        # Convert to PIL and back to tensor for augmentations\n",
    "        patch_np = (patch.squeeze(0).cpu().numpy() * 255).astype(np.uint8)\n",
    "        # Now apply transforms\n",
    "        patch_pil = transforms.ToPILImage()(patch_np)\n",
    "        patch_pil = self.transform(patch_pil)\n",
    "        patch_tensor = transforms.ToTensor()(patch_pil).to(self.patches.device)  # [1,128,128], float in [0,1]\n",
    "        return patch_tensor\n",
    "    \n",
    "# Filter for positive nodes\n",
    "positive_pathces = []\n",
    "\n",
    "for key, data in graph_data_dict.items():\n",
    "    positive_nodes = positive_nodes_dict[key].cpu()\n",
    "    positive_patches_sample = data.patches[positive_nodes].permute(0, 3, 1, 2)  # [num_positive, 1, 128, 128]\n",
    "    positive_pathces.append(positive_patches_sample)\n",
    "\n",
    "# Concatenate all positive patches\n",
    "positive_patches_sample = torch.cat(positive_pathces, dim=0)\n",
    "\n",
    "# Create a DataLoader for the positive patches\n",
    "positive_dataset = PositivePatchDataset(positive_patches_sample, device)\n",
    "plaque_loader = DataLoader(positive_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL\n",
    "\n",
    "# Define the autoencoder model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vae = AutoencoderKL(in_channels=1, out_channels=1, down_block_types=(\"DownEncoderBlock2D\", \"DownEncoderBlock2D\", \"DownEncoderBlock2D\", \"DownEncoderBlock2D\"), \n",
    "                    up_block_types=(\"UpDecoderBlock2D\", \"UpDecoderBlock2D\", \"UpDecoderBlock2D\", \"UpDecoderBlock2D\"), block_out_channels=(64, 128, 256, 512), latent_channels = 16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Define the PositivePatchDataset\n",
    "class PositivePatchDataset(Dataset):\n",
    "    def __init__(self, patches):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patches (torch.Tensor): Tensor of patches with shape [N, 1, 128, 128]\n",
    "        \"\"\"\n",
    "        self.patches = patches  # Keep patches on CPU\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ToTensor(),  # Ensure tensor is in [0,1]\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.patches.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patch = self.patches[idx]  # [1,128,128]\n",
    "        patch_np = (patch.squeeze(0).cpu().numpy() * 255).astype(np.uint8)  # Convert to [0,255] uint8\n",
    "        patch_pil = Image.fromarray(patch_np, mode='L')  # Convert to PIL Image in grayscale\n",
    "        patch_transformed = self.transform(patch_pil)  # Apply augmentations\n",
    "        return patch_transformed  # [1,128,128], float in [0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming graph_data_dict and positive_nodes_dict are already defined\n",
    "# and device is set (e.g., device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Collect positive patches\n",
    "positive_patches = []\n",
    "\n",
    "for key, data in graph_data_dict.items():\n",
    "    positive_nodes = positive_nodes_dict[key].cpu()  # Ensure on CPU\n",
    "    positive_patches_sample = data.patches[positive_nodes].permute(0, 3, 1, 2)  # [num_positive, 1, 128, 128]\n",
    "    positive_patches.append(positive_patches_sample)\n",
    "\n",
    "# Concatenate all positive patches\n",
    "positive_patches_sample = torch.cat(positive_patches, dim=0)  # [total_positive, 1, 128, 128]\n",
    "\n",
    "# Create the dataset\n",
    "positive_dataset = PositivePatchDataset(positive_patches_sample)\n",
    "\n",
    "# Split into training and validation sets (e.g., 80% train, 20% val)\n",
    "train_size = int(0.8 * len(positive_dataset))\n",
    "val_size = len(positive_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(positive_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "num_workers = 0  # Set to 0 for notebook environments\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True if torch.cuda.is_available() else False  # Optimize data transfer to GPU\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "# Combine into a dictionary for easy access\n",
    "plaque_loader = {'train': train_loader, 'val': val_loader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the values range of the patches from the positive_dataset\n",
    "patch_values = []\n",
    "for patch in positive_dataset:\n",
    "    patch_values.append(patch.numpy().flatten())\n",
    "\n",
    "patch_values = np.array(patch_values)\n",
    "patch_min = patch_values.min()\n",
    "patch_max = patch_values.max()\n",
    "\n",
    "patch_min, patch_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm  # Use tqdm for notebook\n",
    "import os\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "reconstruction_loss_fn = nn.BCEWithLogitsLoss(reduction='mean')  # Binary Cross-Entropy with Logits Loss\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-4)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # Optional\n",
    "\n",
    "# Training Parameters\n",
    "num_epochs = 20\n",
    "log_interval = 100      # How often to log training status\n",
    "save_interval = 5       # How often to save the model\n",
    "visualize_interval = max(1, num_epochs // 5)  # Visualize every 1/5 of total epochs\n",
    "warmup_epochs = 10       # Number of epochs to warm-up the KL term\n",
    "\n",
    "# Create a directory to save models\n",
    "os.makedirs('vae_checkpoints', exist_ok=True)\n",
    "\n",
    "# Lists to store loss values for plotting\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Function to visualize reconstructions\n",
    "def visualize_reconstructions(model, dataloader, device, epoch, num_images=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data = next(iter(dataloader))  # Retrieve only data\n",
    "        data = data.to(device)\n",
    "        latent_dist = model.encode(data).latent_dist  # Obtain latent distribution\n",
    "        recon_data = model.decode(latent_dist.sample()).sample  # Decode sampled latent vectors\n",
    "\n",
    "        # Move tensors to CPU and convert to numpy\n",
    "        data = data.cpu().numpy()\n",
    "        recon_data = recon_data.cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for i in range(num_images):\n",
    "        # Original Image\n",
    "        plt.subplot(2, num_images, i + 1)\n",
    "        plt.imshow(data[i].squeeze(), cmap='gray')\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Reconstructed Image\n",
    "        plt.subplot(2, num_images, i + 1 + num_images)\n",
    "        plt.imshow(recon_data[i].squeeze(), cmap='gray')\n",
    "        plt.title(\"Reconstructed\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Reconstruction at Epoch {epoch}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation Loop\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Training Phase\n",
    "    vae.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    \n",
    "    # Initialize tqdm progress bar for training\n",
    "    train_bar = tqdm(plaque_loader['train'], desc=f\"Epoch {epoch}/{num_epochs} [Train]\", leave=False)\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_bar):\n",
    "        data = data.to(device)  # Move data to device\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: Encode and Decode\n",
    "        latent_dist = vae.encode(data).latent_dist  # Obtain latent distribution\n",
    "        recon_data = vae.decode(latent_dist.sample()).sample  # Decode sampled latent vectors\n",
    "        \n",
    "        # Extract mu and logvar\n",
    "        mu = latent_dist.mean\n",
    "        logvar = latent_dist.logvar\n",
    "        \n",
    "        # Compute Loss\n",
    "        recon_loss = reconstruction_loss_fn(recon_data, data)\n",
    "        kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        # Calculate annealing factor\n",
    "        annealing_factor = min(1.0, epoch / warmup_epochs)\n",
    "        loss = recon_loss + kl_loss * annealing_factor\n",
    "        \n",
    "        # Backward and Optimize\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(vae.parameters(), max_norm=10.0)  # Optional: Gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update training loss\n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "        # Update tqdm progress bar with loss\n",
    "        train_bar.set_postfix({'Loss': loss.item(), 'Recon': recon_loss.item(), 'KL': kl_loss.item()})\n",
    "    \n",
    "    # Calculate average training loss for the epoch\n",
    "    average_train_loss = epoch_train_loss / len(plaque_loader['train'])\n",
    "    train_losses.append(average_train_loss)\n",
    "    \n",
    "    # Validation Phase\n",
    "    vae.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_bar = tqdm(plaque_loader['val'], desc=f\"Epoch {epoch}/{num_epochs} [Val]\", leave=False)\n",
    "        for data in val_bar:\n",
    "            data = data.to(device)\n",
    "            latent_dist = vae.encode(data).latent_dist  # Obtain latent distribution\n",
    "            recon_data = vae.decode(latent_dist.sample()).sample  # Decode sampled latent vectors\n",
    "            \n",
    "            # Extract mu and logvar\n",
    "            mu = latent_dist.mean\n",
    "            logvar = latent_dist.logvar\n",
    "            \n",
    "            # Compute Loss\n",
    "            recon_loss = reconstruction_loss_fn(recon_data, data)\n",
    "            kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "            loss = recon_loss + kl_loss * annealing_factor\n",
    "            \n",
    "            # Update validation loss\n",
    "            epoch_val_loss += loss.item()\n",
    "            \n",
    "            # Update tqdm progress bar with loss\n",
    "            val_bar.set_postfix({'Loss': loss.item(), 'Recon': recon_loss.item(), 'KL': kl_loss.item()})\n",
    "    \n",
    "    # Calculate average validation loss for the epoch\n",
    "    average_val_loss = epoch_val_loss / len(plaque_loader['val'])\n",
    "    val_losses.append(average_val_loss)\n",
    "    \n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] | Train Loss: {average_train_loss:.4f} | Val Loss: {average_val_loss:.4f}\")\n",
    "    \n",
    "    # # Step the scheduler\n",
    "    # scheduler.step()\n",
    "    \n",
    "    # Save checkpoint periodically\n",
    "    if epoch % save_interval == 0 or epoch == num_epochs:\n",
    "        checkpoint_path = f'vae_checkpoints/vae_bce_hidden16_epoch_{epoch}.pth'\n",
    "        torch.save(vae.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved model checkpoint at {checkpoint_path}\")\n",
    "    \n",
    "    # Visualize Reconstructions at specified intervals\n",
    "    if epoch % visualize_interval == 0 or epoch == num_epochs:\n",
    "        visualize_reconstructions(vae, plaque_loader['val'], device, epoch, num_images=5)\n",
    "\n",
    "# Plot Training and Validation Losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP + Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "from diffusers import AutoencoderKL\n",
    "from torch_geometric.nn import GATConv\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "############################################\n",
    "# 1) 定义 GAT 模型\n",
    "############################################\n",
    "class GAT_Embedder(nn.Module):\n",
    "    \"\"\"\n",
    "    多层 GAT + LayerNorm + skip + MLP投影到 embed_dim.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_features=2766,  # 你的节点特征维度\n",
    "                 gat_hidden=128,    # 增加隐藏单元数\n",
    "                 heads=8,           # 增加注意力头数\n",
    "                 dropout=0.5,\n",
    "                 embed_dim=256):    # 增加 embedding 维度\n",
    "        super().__init__()\n",
    "        self.gat1 = GATConv(in_channels=in_features, out_channels=gat_hidden,\n",
    "                            heads=heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(gat_hidden * heads)\n",
    "\n",
    "        self.gat2 = GATConv(in_channels=gat_hidden * heads, out_channels=gat_hidden,\n",
    "                            heads=1, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(gat_hidden)\n",
    "\n",
    "        # 最终投影到 embedding 维度\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(gat_hidden, embed_dim),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        )\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # GATConv block 1\n",
    "        x1 = self.gat1(x, edge_index)  # [N, gat_hidden*heads]\n",
    "        x1 = self.norm1(x1)\n",
    "        x1 = F.elu(x1)\n",
    "        x1 = F.dropout(x1, p=self.dropout, training=self.training)\n",
    "\n",
    "        # GATConv block 2\n",
    "        x2 = self.gat2(x1, edge_index)  # [N, gat_hidden]\n",
    "        if x2.size(-1) == x1.size(-1):\n",
    "            x2 = x2 + x1  # skip\n",
    "        x2 = self.norm2(x2)\n",
    "        x2 = F.elu(x2)\n",
    "        x2 = F.dropout(x2, p=self.dropout, training=self.training)\n",
    "\n",
    "        # MLP投影\n",
    "        out = self.mlp(x2)  # [N, embed_dim]\n",
    "        return out\n",
    "\n",
    "############################################\n",
    "# 2) 定义对比损失 (CLIP-style)\n",
    "############################################\n",
    "def clip_style_loss(node_emb, img_emb, temperature=1.0):\n",
    "    \"\"\"\n",
    "    基于对称 CrossEntropy 的CLIP-style对比损失：\n",
    "      - sim[i,j] = dot( normalize(node_emb[i]), normalize(img_emb[j]) ) / temperature\n",
    "      - target: (i,i) 为正样本\n",
    "    \"\"\"\n",
    "    B = node_emb.size(0)\n",
    "    node_norm = F.normalize(node_emb, dim=-1)\n",
    "    img_norm = F.normalize(img_emb, dim=-1)\n",
    "\n",
    "    sim = node_norm @ img_norm.t()  # [B,B]\n",
    "    sim = sim / temperature  # 调整温度\n",
    "\n",
    "    target = torch.arange(B, device=node_emb.device)\n",
    "\n",
    "    loss_g2i = F.cross_entropy(sim, target)       # graph->image\n",
    "    loss_i2g = F.cross_entropy(sim.t(), target)   # image->graph\n",
    "    return 0.5 * (loss_g2i + loss_i2g)\n",
    "\n",
    "############################################\n",
    "# 3) 定义 训练器 (GraphCLIPTrainer)\n",
    "############################################\n",
    "class GraphCLIPTrainer:\n",
    "    \"\"\"\n",
    "    一个包含:\n",
    "      - 模型 (GAT_Embedder)\n",
    "      - VAE (只encode)\n",
    "      - 训练/验证逻辑\n",
    "      - 学习率调度 & 早停 & checkpoint 保存\n",
    "      - methodB: 每个 mini-batch 都重新 forward 整张图\n",
    "\n",
    "    注意: 如果图很大，这种做法会非常耗时和显存。\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        vae: AutoencoderKL,\n",
    "        graph_data_dict: dict,  # {key: Data(...)}\n",
    "        device: torch.device,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-5,\n",
    "        max_epochs=10,\n",
    "        batch_size=128,\n",
    "        val_batch_size=256,   # 验证时 VAE 编码的 batch_size\n",
    "        temperature=0.07,\n",
    "        patience=5,\n",
    "        save_dir=\"./graphclip_checkpoints\",\n",
    "        log_dir=\"./logs\"       # TensorBoard log directory\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.vae = vae\n",
    "        self.graph_data_dict = graph_data_dict\n",
    "        self.device = device\n",
    "\n",
    "        self.max_epochs = max_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.val_batch_size = val_batch_size  # 用于分批验证\n",
    "        self.temperature = temperature\n",
    "        self.patience = patience\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        # (A) 优化器(第一组)\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        # (B) 第二组 param: 图像投影层\n",
    "        #   => 这就意味着 self.optimizer.param_groups = 2\n",
    "        # 做图像投影层 ([16]->[256])，可训练\n",
    "        self.img_projector = nn.Sequential(\n",
    "            nn.Linear(16, model.mlp[-1].normalized_shape[0], bias=False),\n",
    "            nn.LayerNorm(model.mlp[-1].normalized_shape[0])\n",
    "        ).to(device)\n",
    "\n",
    "        self.optimizer.add_param_group({\"params\": self.img_projector.parameters()})\n",
    "\n",
    "        # (C) 学习率调度器 => 因为我们有2个 param group => 需要2个 min_lr\n",
    "        self.scheduler = ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            verbose=True,\n",
    "            min_lr=[1e-6, 1e-6]  # 确保与 param groups 数量一致\n",
    "        )\n",
    "\n",
    "        # 梯度裁剪\n",
    "        self.max_grad_norm = 5.0\n",
    "\n",
    "        # early stopping\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.no_improve_epochs = 0\n",
    "\n",
    "        # TensorBoard\n",
    "        self.writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "        # 创建保存目录\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    def _vae_encode_project(self, patches: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        给一批 patch 做 VAE 编码 => [B,16,h',w'] => pooled => [B,16] => 投影 => [B,256].\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            latent_dist = self.vae.encode(patches).latent_dist\n",
    "            z = latent_dist.sample()\n",
    "            z_pooled = z.mean(dim=[2,3])\n",
    "        img_emb = self.img_projector(z_pooled)\n",
    "        return img_emb\n",
    "\n",
    "    def visualize_alignment(self, node_embs, img_embs, epoch):\n",
    "        \"\"\"\n",
    "        使用 t-SNE 将高维嵌入降维到 2D，并绘制节点嵌入与图像嵌入的对齐情况。\n",
    "        将可视化图保存并记录到 TensorBoard。\n",
    "        \"\"\"\n",
    "        # 将嵌入从 GPU 转移到 CPU 并转换为 NumPy\n",
    "        node_embs = node_embs.detach().cpu().numpy()\n",
    "        img_embs = img_embs.detach().cpu().numpy()\n",
    "\n",
    "        # 使用 t-SNE 进行降维\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        all_embeds = np.concatenate([node_embs, img_embs], axis=0)\n",
    "        all_embeds_2d = tsne.fit_transform(all_embeds)\n",
    "\n",
    "        node_embs_2d = all_embeds_2d[:len(node_embs)]\n",
    "        img_embs_2d = all_embeds_2d[len(node_embs):]\n",
    "\n",
    "        # 绘制散点图\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.scatter(node_embs_2d[:,0], node_embs_2d[:,1], c='blue', label='Node Embeddings', alpha=0.5)\n",
    "        plt.scatter(img_embs_2d[:,0], img_embs_2d[:,1], c='red', label='Image Embeddings', alpha=0.5)\n",
    "\n",
    "        # 绘制对应关系的线条\n",
    "        for i in range(min(len(node_embs), len(img_embs))):\n",
    "            plt.plot(\n",
    "                [node_embs_2d[i,0], img_embs_2d[i,0]],\n",
    "                [node_embs_2d[i,1], img_embs_2d[i,1]],\n",
    "                c='gray',\n",
    "                linewidth=0.5,\n",
    "                alpha=0.3\n",
    "            )\n",
    "\n",
    "        plt.legend()\n",
    "        plt.title(f'CLIP Alignment at Epoch {epoch}')\n",
    "        plt.xlabel('t-SNE Dimension 1')\n",
    "        plt.ylabel('t-SNE Dimension 2')\n",
    "\n",
    "        # 将图形保存到 TensorBoard\n",
    "        self.writer.add_figure('CLIP/Alignment', plt.gcf(), global_step=epoch)\n",
    "        plt.close()\n",
    "\n",
    "    def train_one_graph_fullbatch_strictB(self, data, epoch_idx) -> float:\n",
    "        \"\"\"\n",
    "        Method B (严格版):\n",
    "          - 对于同一个 data\n",
    "          - 每次 mini-batch:\n",
    "             1) 先整图 forward => node_emb_all\n",
    "             2) 取 batch节点 => 计算对比损失 => backward & step\n",
    "          - 重复 step 若干次\n",
    "        返回: 该图所有 mini-batch 的平均loss\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.img_projector.train()\n",
    "        \n",
    "        x = data.x.to(self.device)\n",
    "        edge_index = data.edge_index.to(self.device)\n",
    "        patches = data.patches.permute(0,3,1,2).float().to(self.device)\n",
    "        y = data.y.view(-1).to(self.device)\n",
    "\n",
    "        pos_idx = (y > 0).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if pos_idx.numel() == 0:\n",
    "            # 若全是负 => 无法对比\n",
    "            return 0.0\n",
    "\n",
    "        # 随机打乱\n",
    "        pos_idx = pos_idx[torch.randperm(pos_idx.numel())]\n",
    "\n",
    "        # 计算 steps\n",
    "        steps = pos_idx.numel() // self.batch_size\n",
    "\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for step in range(steps):\n",
    "            # 1) 整图 forward => [N,256]\n",
    "            node_emb_all = self.model(x, edge_index)\n",
    "\n",
    "            # 2) 取 mini-batch\n",
    "            batch_nodes = pos_idx[step*self.batch_size : (step+1)*self.batch_size]  # [B]\n",
    "\n",
    "            batch_node_emb = node_emb_all[batch_nodes]  # [B,256]\n",
    "\n",
    "            # 3) VAE encode => [B,256]\n",
    "            batch_patches = patches[batch_nodes]\n",
    "            batch_img_emb = self._vae_encode_project(batch_patches)\n",
    "\n",
    "            # 4) contrastive loss\n",
    "            loss = clip_style_loss(batch_node_emb, batch_img_emb, temperature=self.temperature)\n",
    "\n",
    "            # 5) backward & update\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # 梯度裁剪\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.max_grad_norm)\n",
    "            nn.utils.clip_grad_norm_(self.img_projector.parameters(), max_norm=self.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / max(steps, 1)\n",
    "        return avg_loss\n",
    "\n",
    "    def train_one_epoch(self, epoch_idx):\n",
    "        \"\"\"\n",
    "        对 graph_data_dict 中的每个 Data 调用 train_one_graph_fullbatch_strictB\n",
    "        并返回此 epoch 的平均loss\n",
    "\n",
    "        - 在这里不使用 tqdm\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.img_projector.train()\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        count = 0\n",
    "\n",
    "        for key, data in self.graph_data_dict.items():\n",
    "            data_loss = self.train_one_graph_fullbatch_strictB(data, epoch_idx)\n",
    "            epoch_loss += data_loss\n",
    "            count += 1\n",
    "            # 记录到 TensorBoard\n",
    "            self.writer.add_scalar('Train/Loss_per_graph', data_loss, epoch_idx * len(self.graph_data_dict) + count)\n",
    "\n",
    "        avg_loss = epoch_loss / max(count, 1)\n",
    "        self.writer.add_scalar('Train/Loss_avg', avg_loss, epoch_idx)\n",
    "        print(f\"  => [Epoch {epoch_idx}] train avg contrastive loss = {avg_loss:.4f}\")\n",
    "        return avg_loss\n",
    "\n",
    "    def validate(self, epoch_idx) -> float:\n",
    "        \"\"\"\n",
    "        在验证时，只采样一小部分positive节点来做对比损失，避免显存爆。\n",
    "        同时收集 embeddings 以用于可视化。\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.img_projector.eval()\n",
    "\n",
    "        val_loss = 0.0\n",
    "        val_count = 0\n",
    "\n",
    "        # 用于可视化的嵌入\n",
    "        visualize_node_embs = []\n",
    "        visualize_img_embs = []\n",
    "\n",
    "        for key, data in self.graph_data_dict.items():\n",
    "            x = data.x.to(self.device)\n",
    "            edge_index = data.edge_index.to(self.device)\n",
    "            patches = data.patches.permute(0,3,1,2).float().to(self.device)\n",
    "            y = data.y.view(-1).to(self.device)\n",
    "\n",
    "            pos_idx = (y > 0).nonzero(as_tuple=True)[0]\n",
    "\n",
    "            if pos_idx.numel() == 0:\n",
    "                # 全是负，跳过\n",
    "                continue\n",
    "\n",
    "            N = pos_idx.numel()\n",
    "            # 只随机采样 M 个正节点\n",
    "            M = min(N, 200)  # 例如最多200个正节点\n",
    "            sampled_pos_idx = pos_idx[torch.randperm(N)[:M]]\n",
    "\n",
    "            # 整图 forward (Method B 想法)\n",
    "            node_emb_all = self.model(x, edge_index)  # [N,256]\n",
    "\n",
    "            # 只取这 M 个节点\n",
    "            node_emb_sample = node_emb_all[sampled_pos_idx]  # [M,256]\n",
    "            patches_sample = patches[sampled_pos_idx]\n",
    "\n",
    "            # 做 VAE 编码\n",
    "            img_emb_sample = self._vae_encode_project(patches_sample)\n",
    "\n",
    "            if M > 1:\n",
    "                loss = clip_style_loss(node_emb_sample, img_emb_sample, temperature=self.temperature)\n",
    "                val_loss += loss.item()\n",
    "                val_count += 1\n",
    "                # 记录到 TensorBoard\n",
    "                self.writer.add_scalar('Validate/Loss_per_graph', loss.item(), epoch_idx * len(self.graph_data_dict) + val_count)\n",
    "\n",
    "                # 收集嵌入以用于可视化\n",
    "                visualize_node_embs.append(node_emb_sample)\n",
    "                visualize_img_embs.append(img_emb_sample)\n",
    "\n",
    "        avg_val_loss = val_loss / max(val_count, 1)\n",
    "        self.writer.add_scalar('Validate/Loss_avg', avg_val_loss, epoch_idx)\n",
    "        print(f\"  => [Validate] avg contrastive loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "        # 如果有收集到嵌入，则进行可视化\n",
    "        if visualize_node_embs and visualize_img_embs:\n",
    "            # 将列表中的张量拼接成一个大的张量\n",
    "            node_embs = torch.cat(visualize_node_embs, dim=0)\n",
    "            img_embs = torch.cat(visualize_img_embs, dim=0)\n",
    "            # 可视化对齐\n",
    "            self.visualize_alignment(node_embs, img_embs, epoch_idx)\n",
    "\n",
    "        return avg_val_loss\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        主训练循环: \n",
    "          for epoch in range(1, max_epochs+1):\n",
    "            train_one_epoch -> validate -> scheduler -> early_stop\n",
    "        \"\"\"\n",
    "        progress_bar = tqdm(range(1, self.max_epochs+1), desc=\"Training Progress\")\n",
    "        for epoch in progress_bar:\n",
    "            train_loss = self.train_one_epoch(epoch)\n",
    "            val_loss = self.validate(epoch)\n",
    "\n",
    "            # 调度器\n",
    "            self.scheduler.step(val_loss)\n",
    "            progress_bar.set_postfix({\"train_loss\": train_loss, \"val_loss\": val_loss})\n",
    "\n",
    "            # 记录到 TensorBoard\n",
    "            self.writer.add_scalar('Train/Loss_avg_epoch', train_loss, epoch)\n",
    "            self.writer.add_scalar('Validate/Loss_avg_epoch', val_loss, epoch)\n",
    "\n",
    "            # early stopping\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.no_improve_epochs = 0\n",
    "                best_path = os.path.join(self.save_dir, f\"best_model_epoch_{epoch}.pth\")\n",
    "                torch.save(self.model.state_dict(), best_path)\n",
    "                print(f\"  => Best model updated, saved at {best_path}\")\n",
    "                # 记录到 TensorBoard\n",
    "                self.writer.add_scalar('Validate/Best_Loss', val_loss, epoch)\n",
    "            else:\n",
    "                self.no_improve_epochs += 1\n",
    "                if self.no_improve_epochs >= self.patience:\n",
    "                    print(\"Early stopping triggered!\")\n",
    "                    break\n",
    "\n",
    "        # save last model\n",
    "        last_path = os.path.join(self.save_dir, f\"last_model_epoch_{epoch}.pth\")\n",
    "        torch.save(self.model.state_dict(), last_path)\n",
    "        print(f\"Training finished. Final model saved to {last_path}\")\n",
    "        # 关闭 TensorBoard\n",
    "        self.writer.close()\n",
    "\n",
    "###############################################\n",
    "# 在Notebook中使用:\n",
    "###############################################\n",
    "\n",
    "# 1) 加载数据\n",
    "#    {'8months-disease-replicate_1': Data(...),\n",
    "#     '13months-disease-replicate_1': Data(...),\n",
    "#      ... }\n",
    "\n",
    "\n",
    "vae.eval()\n",
    "for p in vae.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# 3) 初始化 GAT 模型\n",
    "clip_model = GAT_Embedder(\n",
    "    in_features=2766,   # 你的节点特征维度\n",
    "    gat_hidden=128,     # 增加隐藏单元数\n",
    "    heads=8,            # 增加注意力头数\n",
    "    dropout=0.5,\n",
    "    embed_dim=256       # 增加 embedding 维度\n",
    ").to(device)\n",
    "\n",
    "# 4) 创建并启动 Trainer\n",
    "trainer = GraphCLIPTrainer(\n",
    "    model=clip_model,\n",
    "    vae=vae,\n",
    "    graph_data_dict=graph_data_dict,\n",
    "    device=device,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5,\n",
    "    max_epochs=50,       # 可自行调大\n",
    "    batch_size=128,      # 每次对比时的总节点数\n",
    "    temperature=0.07,    # 常用温度\n",
    "    patience=10,         # 根据需要调整\n",
    "    save_dir=\"./graphclip_checkpoints\",\n",
    "    log_dir=\"./logs\"     # TensorBoard log directory\n",
    ")\n",
    "\n",
    "trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from diffusers import UNet2DConditionModel, DDPMScheduler, AutoencoderKL\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Load your pretrained CLIP-like GAT embedder + projection\n",
    "#    We'll call it \"GraphClipEncoder\" for clarity.\n",
    "#    This can be your GAT_Embedder plus the projector you used.\n",
    "#    We'll assume you have something like this:\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "class GraphClipEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps your GAT_Embedder + projector into one module.\n",
    "    \"\"\"\n",
    "    def __init__(self, gat_model, img_projector, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.gat_model = gat_model\n",
    "        self.img_projector = img_projector\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        # GAT forward pass\n",
    "        node_emb = self.gat_model(x, edge_index)  # [N, embed_dim]\n",
    "        # If you had an image projector for alignment, you might not need it here\n",
    "        # but if your final node_emb depends on that, then you'd apply it.\n",
    "        # For standard usage, let's assume the 'node_emb' is already the final 256-dim\n",
    "        return node_emb\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Define your GraphConditionedLDM model/trainer\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "class GraphConditionedLDMTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph_data_dict,\n",
    "        vae: AutoencoderKL,\n",
    "        graph_clip_encoder: GraphClipEncoder,\n",
    "        unet: UNet2DConditionModel,\n",
    "        noise_scheduler: DDPMScheduler,\n",
    "        device=\"cuda\",\n",
    "        batch_size=16,\n",
    "        lr=1e-4,\n",
    "        max_epochs=10,\n",
    "        save_dir=\"./checkpoints_ldm\",\n",
    "        freeze_graph_encoder=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            graph_data_dict: dict of {key: PyG Data}, each .patches = (N, H, W, 1),\n",
    "                             .x = (N, in_features), .edge_index = ...\n",
    "            vae: A pretrained VAE (AutoencoderKL) used to encode/decode the image patches\n",
    "            graph_clip_encoder: The GAT+projector model you trained (CLIP-like). \n",
    "                                If freeze_graph_encoder=True, we won't update it.\n",
    "            unet: A UNet2DConditionModel from diffusers with cross_attention_dim matching the GAT embedding size\n",
    "            noise_scheduler: A DDPMScheduler (or other) from diffusers\n",
    "        \"\"\"\n",
    "        self.graph_data_dict = graph_data_dict\n",
    "        self.vae = vae.eval().to(device)\n",
    "        self.graph_clip_encoder = graph_clip_encoder.to(device)\n",
    "        self.unet = unet.to(device)\n",
    "        self.noise_scheduler = noise_scheduler\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.max_epochs = max_epochs\n",
    "        \n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        # Freeze VAE encoder & Graph encoder (common practice)\n",
    "        for p in self.vae.parameters():\n",
    "            p.requires_grad = False\n",
    "        if freeze_graph_encoder:\n",
    "            for p in self.graph_clip_encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "        \n",
    "        # Optimizer (only UNet’s parameters by default)\n",
    "        self.optimizer = Adam(self.unet.parameters(), lr=self.lr)\n",
    "    \n",
    "        # For convenience, gather all nodes from all graphs in one big list.\n",
    "        # Or you can create a custom Dataset/DataLoader that does multi-graph sampling.\n",
    "        self.all_nodes = []\n",
    "        self._prepare_all_nodes()\n",
    "\n",
    "    def _prepare_all_nodes(self):\n",
    "        \"\"\"\n",
    "        Collect (graph_key, node_idx) pairs in a list so we can sample them in a DataLoader-like manner.\n",
    "        \"\"\"\n",
    "        for gkey, data in self.graph_data_dict.items():\n",
    "            N = data.x.size(0)\n",
    "            for i in range(N):\n",
    "                self.all_nodes.append((gkey, i))\n",
    "        print(f\"Total nodes across all graphs: {len(self.all_nodes)}\")\n",
    "    \n",
    "    def _get_batch(self, indices):\n",
    "        \"\"\"\n",
    "        Given a list of (graph_key, node_idx), return:\n",
    "          - node_features batch: [B, in_features]\n",
    "          - edge_index (we might need the entire graph's edge_index if we do a full forward, \n",
    "            but for speed you might pre-embed offline. For simplicity, do full forward.)\n",
    "          - patches: [B, H, W, 1]\n",
    "        We'll group them by graph_key and fetch the data in a minimal way.\n",
    "        \"\"\"\n",
    "        # A simple approach: group by graph_key, fetch the entire graph, then pick those node_idx\n",
    "        # If your graphs are large, you may prefer a different approach or an offline pre-embedding.\n",
    "        from collections import defaultdict\n",
    "        batch_by_gkey = defaultdict(list)\n",
    "        for (gkey, idx) in indices:\n",
    "            batch_by_gkey[gkey].append(idx)\n",
    "        \n",
    "        x_list = []\n",
    "        edge_index_list = []\n",
    "        patch_list = []\n",
    "        # We'll also store all node indices in CPU for final stitching\n",
    "        for gkey, node_idxs in batch_by_gkey.items():\n",
    "            data = self.graph_data_dict[gkey]\n",
    "            # data.x: [N, in_features]\n",
    "            # data.edge_index: [2, E]\n",
    "            # data.patches: [N, H, W, 1]\n",
    "            # We do a GAT forward on the entire graph to get [N, embed_dim], then pick out the needed nodes.\n",
    "            # So let's fetch everything first (on device):\n",
    "            x_full = data.x.to(self.device)\n",
    "            edge_index_full = data.edge_index.to(self.device)\n",
    "            # Then we embed with the GAT:\n",
    "            with torch.no_grad():\n",
    "                node_emb_all = self.graph_clip_encoder(x_full, edge_index_full)  # [N, embed_dim]\n",
    "            \n",
    "            # We only need the embedding for the chosen nodes:\n",
    "            node_idxs_tensor = torch.LongTensor(node_idxs).to(self.device)\n",
    "            cond_emb = node_emb_all[node_idxs_tensor]  # [len(node_idxs), embed_dim]\n",
    "            \n",
    "            # For the UNet cross-attention, shape should be [B, sequence_length, cross_attention_dim].\n",
    "            # We'll pick sequence_length=1:\n",
    "            cond_emb = cond_emb.unsqueeze(1)  # [B, 1, embed_dim]\n",
    "\n",
    "            # Collect patches for these nodes\n",
    "            all_patches = data.patches.to(self.device)  # [N, 128, 128, 1]\n",
    "            chosen_patches = all_patches[node_idxs_tensor]  # [b, 128,128,1]\n",
    "\n",
    "            x_list.append(cond_emb)\n",
    "            patch_list.append(chosen_patches)\n",
    "        \n",
    "        # Concatenate along batch dimension\n",
    "        cond_emb_batch = torch.cat(x_list, dim=0)         # [B, 1, embed_dim]\n",
    "        patch_batch = torch.cat(patch_list, dim=0)        # [B, 128,128,1]\n",
    "        \n",
    "        return cond_emb_batch, patch_batch\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        self.unet.train()\n",
    "        # 随机打乱所有节点顺序\n",
    "        indices = torch.randperm(len(self.all_nodes))\n",
    "        num_batches = (len(indices) + self.batch_size - 1) // self.batch_size\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        # 使用 tqdm 包裹 mini-batch 循环显示进度条\n",
    "        batch_iter = tqdm(range(num_batches), desc=\"Training Batches\", leave=False)\n",
    "        for b in batch_iter:\n",
    "            batch_indices = indices[b*self.batch_size : (b+1)*self.batch_size]\n",
    "            batch_pairs = [self.all_nodes[i.item()] for i in batch_indices]\n",
    "            \n",
    "            # 1) 获取条件 embedding 以及对应 patch\n",
    "            cond_emb_batch, patch_batch = self._get_batch(batch_pairs)\n",
    "            B = patch_batch.size(0)\n",
    "\n",
    "            # 2) 使用 VAE 编码 patch 为 latent 表示\n",
    "            with torch.no_grad():\n",
    "                patch_batch = patch_batch.permute(0, 3, 1, 2)  # [B, 1, H, W]\n",
    "                latent_dist = self.vae.encode(patch_batch).latent_dist\n",
    "                latents = latent_dist.sample()\n",
    "            \n",
    "            # 3) 随机采样时间步，并为 latents 添加噪声\n",
    "            t = torch.randint(0, self.noise_scheduler.num_train_timesteps, (B,), device=self.device).long()\n",
    "            noise = torch.randn_like(latents)\n",
    "            noisy_latents = self.noise_scheduler.add_noise(latents, noise, t)\n",
    "\n",
    "            # 4) UNet 前向传播：注意我们将关键字 'timesteps' 改为 'timestep'\n",
    "            model_out = self.unet(\n",
    "                sample=noisy_latents,\n",
    "                timestep=t,\n",
    "                encoder_hidden_states=cond_emb_batch,  # [B, 1, cross_attention_dim]\n",
    "            )\n",
    "            pred_noise = model_out.sample  # [B, C, H_latent, W_latent]\n",
    "\n",
    "            # 5) 计算 MSE 损失\n",
    "            loss = F.mse_loss(pred_noise, noise)\n",
    "            \n",
    "            # 6) 更新参数\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            batch_iter.set_postfix(loss=loss.item())\n",
    "\n",
    "        return epoch_loss / num_batches\n",
    "\n",
    "    def fit(self):\n",
    "        # 使用 tqdm 包裹 epoch 循环\n",
    "        for epoch in tqdm(range(1, self.max_epochs + 1), desc=\"Epochs\"):\n",
    "            train_loss = self.train_one_epoch()\n",
    "            print(f\"[Epoch {epoch}/{self.max_epochs}] | Train Loss: {train_loss:.6f}\")\n",
    "            if epoch % 5 == 0:\n",
    "                save_path = os.path.join(self.save_dir, f\"unet_epoch_{epoch}.pth\")\n",
    "                torch.save(self.unet.state_dict(), save_path)\n",
    "                print(f\"Saved checkpoint at {save_path}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Putting it all together (example usage)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Assume you've already loaded the following:\n",
    "#   graph_data_dict: dict of {key: PyG Data}\n",
    "#   pretrained_vae: an AutoencoderKL instance with weights loaded\n",
    "#   pretrained_graphclip_encoder: your GAT_Embedder + projection, loaded and optionally frozen\n",
    "#   or you can reconstruct it similarly.\n",
    "\n",
    "# Example: UNet with cross_attention_dim = 256 to match your GAT embed_dim\n",
    "unet_config = {\n",
    "    \"sample_size\": 16,             # e.g. if your latent resolution is 16x16\n",
    "    \"in_channels\": 16,             # if your VAE latent_channels=16\n",
    "    \"out_channels\": 16,            # same as in_channels for DDPM\n",
    "    \"layers_per_block\": 2,\n",
    "    \"block_out_channels\": (320, 640, 640, 1280),\n",
    "    \"down_block_types\": (\n",
    "        \"CrossAttnDownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    \"up_block_types\": (\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"CrossAttnUpBlock2D\",\n",
    "    ),\n",
    "    # This is crucial: cross_attention_dim must match your GAT embed_dim\n",
    "    \"cross_attention_dim\": 256,\n",
    "}\n",
    "unet = UNet2DConditionModel(**unet_config)\n",
    "\n",
    "# A basic DDPM Scheduler\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.0001,\n",
    "    beta_end=0.02,\n",
    "    beta_schedule=\"linear\",\n",
    ")\n",
    "\n",
    "# Build the trainer\n",
    "trainer = GraphConditionedLDMTrainer(\n",
    "    graph_data_dict=graph_data_dict,\n",
    "    vae=vae,\n",
    "    graph_clip_encoder=clip_model,\n",
    "    unet=unet,\n",
    "    noise_scheduler=noise_scheduler,\n",
    "    device=device,\n",
    "    batch_size=16,\n",
    "    lr=1e-4,\n",
    "    max_epochs=20,\n",
    "    save_dir=\"./checkpoints_ldm\",\n",
    "    freeze_graph_encoder=True,  # freeze the GAT+CLIP if desired\n",
    ")\n",
    "\n",
    "# Train!\n",
    "trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改进后（pos only）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 假设你已经导入或定义了以下模块：\n",
    "# - AutoencoderKL (作为 VAE 模型)\n",
    "# - UNet2DConditionModel (作为扩散模型中的 UNet)\n",
    "# - GAT_Embedder (你的图编码器)\n",
    "\n",
    "# -------------------------\n",
    "# GraphConditionedLDMTrainer 类\n",
    "# -------------------------\n",
    "class GraphConditionedLDMTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph_data_dict,         # dict, 每个值均为一个 PyG Data，包含 data.x, data.edge_index, data.patches, data.y\n",
    "        vae: nn.Module,          # 预训练好的 AutoencoderKL（VAE），用于 patch 到 latent 的映射（冻结不训练）\n",
    "        graph_clip_encoder: nn.Module,  # 已训练好的 GAT + Projection 模型（CLIP 风格），用于图节点编码\n",
    "        unet: nn.Module,         # UNet2DConditionModel，用于预测噪声，包含 cross-attention（其 cross_attention_dim 应与图编码器输出匹配）\n",
    "        noise_scheduler,         # 扩散模型的调度器，例如 DDPMScheduler\n",
    "        device=\"cuda\",\n",
    "        batch_size=16,\n",
    "        lr=1e-4,\n",
    "        max_epochs=10,\n",
    "        save_dir=\"./checkpoints_ldm\",\n",
    "        freeze_graph_encoder=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            graph_data_dict: dict {graph_key: Data}, 每个 Data 包含：\n",
    "                             - data.x: [N, in_features]\n",
    "                             - data.edge_index: [2, E]\n",
    "                             - data.patches: [N, H, W, 1]  (图像 patch)\n",
    "                             - data.y: [N, 1]  (节点标签，正样本为 label > 0)\n",
    "            vae: 已训练好的 AutoencoderKL（VAE），用于将 patch 编码为 latent 表示\n",
    "            graph_clip_encoder: 已训练好的 GAT_Embedder + projection 模型，用于将图中节点编码为 embedding\n",
    "            unet: UNet2DConditionModel，扩散模型中预测噪声使用\n",
    "            noise_scheduler: 如 DDPMScheduler，用于向 latent 中添加噪声和进行时间步调度\n",
    "            device: \"cuda\" 或 \"cpu\"\n",
    "            batch_size: 每个 mini-batch 正样本数量\n",
    "            lr: 学习率\n",
    "            max_epochs: 总共训练多少个 epoch\n",
    "            save_dir: 模型存储目录\n",
    "            freeze_graph_encoder: 是否冻结图编码器（通常冻结以加速训练）\n",
    "        \"\"\"\n",
    "        self.graph_data_dict = graph_data_dict\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.max_epochs = max_epochs\n",
    "\n",
    "        # 模型迁移到 device\n",
    "        self.vae = vae.eval().to(device)\n",
    "        self.graph_clip_encoder = graph_clip_encoder.to(device)\n",
    "        self.unet = unet.to(device)\n",
    "        self.noise_scheduler = noise_scheduler\n",
    "\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        # 冻结 VAE 参数（其仅用于预计算 latent 表示）\n",
    "        for p in self.vae.parameters():\n",
    "            p.requires_grad = False\n",
    "        # 根据需要冻结图编码器\n",
    "        if freeze_graph_encoder:\n",
    "            for p in self.graph_clip_encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # 优化器仅更新 UNet 参数\n",
    "        self.optimizer = Adam(self.unet.parameters(), lr=self.lr)\n",
    "        # 混合精度训练的梯度缩放器\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        # 对每个图提前预计算 VAE latent 表示（固定不变）\n",
    "        self.precompute_latents()\n",
    "        # 仅收集正样本节点 (label > 0)\n",
    "        self.all_nodes = []\n",
    "        self._prepare_positive_nodes()\n",
    "\n",
    "    def precompute_latents(self):\n",
    "        \"\"\"对所有图中的 patch 预计算 latent 表示，并存入每个 data.latents 中\n",
    "           为防止一次性占用过多内存，对每个图分批处理 patch 编码。\n",
    "        \"\"\"\n",
    "        print(\"预计算 VAE latent 表示...\")\n",
    "        # 设置一个较小的批次大小，防止内存爆掉\n",
    "        latent_batch_size = 64  \n",
    "        for key, data in self.graph_data_dict.items():\n",
    "            # data.patches: [N, H, W, 1]\n",
    "            patches = data.patches.to(self.device).permute(0, 3, 1, 2).float()\n",
    "            N = patches.size(0)\n",
    "            latent_chunks = []\n",
    "            for start in tqdm(range(0, N, latent_batch_size),\n",
    "                              desc=f\"预计算图 {key} 的 latent\", leave=False):\n",
    "                end = start + latent_batch_size\n",
    "                patch_batch = patches[start:end]\n",
    "                with torch.no_grad():\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        latent_dist = self.vae.encode(patch_batch).latent_dist\n",
    "                        latent_chunk = latent_dist.sample()\n",
    "                        latent_chunks.append(latent_chunk)\n",
    "                # 清理缓存\n",
    "                torch.cuda.empty_cache()\n",
    "            # 将所有小批次拼接为一个 tensor\n",
    "            data.latents = torch.cat(latent_chunks, dim=0)\n",
    "        print(\"预计算完成。\")\n",
    "\n",
    "    def _prepare_positive_nodes(self):\n",
    "        \"\"\"\n",
    "        遍历所有图数据，仅收集 label > 0 的节点：(graph_key, node_index)\n",
    "        \"\"\"\n",
    "        for gkey, data in self.graph_data_dict.items():\n",
    "            # 假设 data.y 形状为 [N, 1]\n",
    "            y = data.y.view(-1)\n",
    "            pos_idx = (y > 0).nonzero(as_tuple=True)[0]\n",
    "            for idx in pos_idx.tolist():\n",
    "                self.all_nodes.append((gkey, idx))\n",
    "        print(f\"所有图中正样本节点总数：{len(self.all_nodes)}\")\n",
    "\n",
    "    def _get_batch(self, indices):\n",
    "        \"\"\"\n",
    "        根据一批 (graph_key, node_index) 返回：\n",
    "          - 条件 embedding: [B, 1, embed_dim]（通过整个图的 GAT forward 得到，并只选择 positive 节点）\n",
    "          - 对应的预计算 latent 表示: [B, latent_channels, H_lat, W_lat]\n",
    "        \"\"\"\n",
    "        batch_by_gkey = defaultdict(list)\n",
    "        for (gkey, idx) in indices:\n",
    "            batch_by_gkey[gkey].append(idx)\n",
    "\n",
    "        cond_emb_list = []\n",
    "        latent_list = []\n",
    "        for gkey, node_idxs in batch_by_gkey.items():\n",
    "            data = self.graph_data_dict[gkey]\n",
    "            # 整图前向传播：计算所有节点的 embedding（GAT 计算整个图）\n",
    "            x_full = data.x.to(self.device)\n",
    "            edge_index_full = data.edge_index.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                node_emb_all = self.graph_clip_encoder(x_full, edge_index_full)  # [N, embed_dim]\n",
    "            node_idxs_tensor = torch.LongTensor(node_idxs).to(self.device)\n",
    "            # 选取对应 positive 节点的 embedding，并扩展 token 维度使其适应 UNet 的交叉注意力，形状：[b, 1, embed_dim]\n",
    "            cond_emb = node_emb_all[node_idxs_tensor].unsqueeze(1)\n",
    "            cond_emb_list.append(cond_emb)\n",
    "            # 使用预先存入 data.latents 的 latent 表示\n",
    "            latents = data.latents[node_idxs_tensor]  # 形状：[b, latent_channels, H_lat, W_lat]\n",
    "            latent_list.append(latents)\n",
    "\n",
    "        cond_emb_batch = torch.cat(cond_emb_list, dim=0)\n",
    "        latent_batch = torch.cat(latent_list, dim=0)\n",
    "        return cond_emb_batch, latent_batch\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        self.unet.train()\n",
    "        # 随机打乱所有正样本节点顺序\n",
    "        indices = torch.randperm(len(self.all_nodes))\n",
    "        num_batches = (len(indices) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        batch_iter = tqdm(range(num_batches), desc=\"Training Batches\", leave=False)\n",
    "        for b in batch_iter:\n",
    "            batch_indices = indices[b * self.batch_size : (b + 1) * self.batch_size]\n",
    "            batch_pairs = [self.all_nodes[i.item()] for i in batch_indices]\n",
    "\n",
    "            # 获取条件 embedding 和预计算好的 latent 表示（作为 ground truth）\n",
    "            cond_emb_batch, latents = self._get_batch(batch_pairs)\n",
    "            B = latents.size(0)\n",
    "\n",
    "            # 随机采样时间步 t，并生成同尺寸噪声\n",
    "            t = torch.randint(0, self.noise_scheduler.num_train_timesteps, (B,), device=self.device).long()\n",
    "            noise = torch.randn_like(latents)\n",
    "            # 为 latent 添加噪声\n",
    "            noisy_latents = self.noise_scheduler.add_noise(latents, noise, t)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                # 注意：UNet 的 forward 接口使用关键字参数 'timestep'（而非 'timesteps'）\n",
    "                model_out = self.unet(\n",
    "                    sample=noisy_latents,\n",
    "                    timestep=t,\n",
    "                    encoder_hidden_states=cond_emb_batch,  # [B, 1, cross_attention_dim]\n",
    "                )\n",
    "                pred_noise = model_out.sample  # 输出预测噪声，形状 [B, latent_channels, H_lat, W_lat]\n",
    "                loss = F.mse_loss(pred_noise, noise)\n",
    "\n",
    "            # 反向传播采用混合精度\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            batch_iter.set_postfix(loss=loss.item())\n",
    "\n",
    "        return epoch_loss / num_batches\n",
    "\n",
    "    def fit(self):\n",
    "        for epoch in tqdm(range(1, self.max_epochs + 1), desc=\"Epochs\"):\n",
    "            train_loss = self.train_one_epoch()\n",
    "            print(f\"[Epoch {epoch}/{self.max_epochs}] | Train Loss: {train_loss:.6f}\")\n",
    "            if epoch % 5 == 0:\n",
    "                save_path = os.path.join(self.save_dir, f\"unet_epoch_{epoch}.pth\")\n",
    "                torch.save(self.unet.state_dict(), save_path)\n",
    "                print(f\"Saved checkpoint at {save_path}\")\n",
    "\n",
    "# -------------------------\n",
    "# 示例使用代码\n",
    "# -------------------------\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 请确保下列变量已经正确加载或初始化：\n",
    "# graph_data_dict: dict, 如 { 'graph1': data1, 'graph2': data2, ... }\n",
    "#   每个 data 包含 data.x, data.edge_index, data.y, data.patches\n",
    "# vae: 已加载的 AutoencoderKL 模型，权重已加载\n",
    "# graph_clip_encoder: 已训练好的 GAT_Embedder + projection 模型\n",
    "# unet: 一个 UNet2DConditionModel 模型，其 cross_attention_dim 应与 graph_clip_encoder 输出维度匹配\n",
    "# noise_scheduler: 例如 DDPMScheduler 的实例\n",
    "#\n",
    "# 示例：\n",
    "# from diffusers import UNet2DConditionModel, DDPMScheduler, AutoencoderKL\n",
    "#\n",
    "# unet = UNet2DConditionModel(**unet_config)\n",
    "# noise_scheduler = DDPMScheduler(...)\n",
    "# vae = AutoencoderKL(...)  # 并加载预训练权重\n",
    "#\n",
    "# graph_data_dict 已由你的数据预处理模块生成\n",
    "\n",
    "# 这里假设上述模型和 graph_data_dict 均已加载\n",
    "trainer = GraphConditionedLDMTrainer(\n",
    "    graph_data_dict=graph_data_dict,\n",
    "    vae=vae,\n",
    "    graph_clip_encoder=clip_model,\n",
    "    unet=unet,\n",
    "    noise_scheduler=noise_scheduler,\n",
    "    device=device,\n",
    "    batch_size=16,\n",
    "    lr=1e-4,\n",
    "    max_epochs=20,\n",
    "    save_dir=\"./checkpoints_ldm\",\n",
    "    freeze_graph_encoder=True,  # 冻结图编码器以加速训练\n",
    ")\n",
    "\n",
    "trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def generate_predicted_image(trainer, graph_key, node_index, num_inference_steps=50):\n",
    "    \"\"\"\n",
    "    Generate the predicted image (patch) by sampling from the trained diffusion model.\n",
    "    \n",
    "    Parameters:\n",
    "      - trainer: An instance of GraphConditionedLDMTrainer containing trained models (vae, unet, noise_scheduler, etc.)\n",
    "      - graph_key: str, key indicating the target graph.\n",
    "      - node_index: int, the index of the target node in the graph.\n",
    "      - num_inference_steps: int, number of inference steps for the reverse diffusion process.\n",
    "      \n",
    "    Returns:\n",
    "      - image: Tensor of the generated image with shape [1, C, H, W].\n",
    "    \"\"\"\n",
    "    device = trainer.device\n",
    "    data = trainer.graph_data_dict[graph_key]\n",
    "    # Use precomputed latent's shape (for the specified node) to determine the latent shape.\n",
    "    latent_shape = data.latents[node_index:node_index+1].shape  # [1, latent_channels, H_lat, W_lat]\n",
    "    \n",
    "    # Compute full graph node embeddings, then select the condition for the specified node.\n",
    "    x_full = data.x.to(device)\n",
    "    edge_index = data.edge_index.to(device)\n",
    "    with torch.no_grad():\n",
    "        node_emb_all = trainer.graph_clip_encoder(x_full, edge_index)  # shape: [N, embed_dim]\n",
    "    condition = node_emb_all[node_index].unsqueeze(0).unsqueeze(1)  # shape: [1, 1, embed_dim]\n",
    "    \n",
    "    # Initialize latent representation with random noise.\n",
    "    sample = torch.randn(latent_shape, device=device)\n",
    "    \n",
    "    trainer.unet.eval()\n",
    "    trainer.noise_scheduler.set_timesteps(num_inference_steps)\n",
    "    \n",
    "    # Reverse diffusion process: iterate through timesteps to denoise.\n",
    "    for t in tqdm(trainer.noise_scheduler.timesteps, desc=f\"Sampling (Graph: {graph_key} Node: {node_index})\", leave=False):\n",
    "        t_tensor = torch.tensor([t], device=device).long()\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                model_out = trainer.unet(\n",
    "                    sample=sample,\n",
    "                    timestep=t_tensor,\n",
    "                    encoder_hidden_states=condition  # [1, 1, embed_dim]\n",
    "                )\n",
    "            pred_noise = model_out.sample  # predicted noise, same shape as sample.\n",
    "            step_output = trainer.noise_scheduler.step(pred_noise, t, sample)\n",
    "            sample = step_output[\"prev_sample\"]\n",
    "    \n",
    "    # Decode the latent representation into an image patch.\n",
    "    with torch.no_grad():\n",
    "        decoded = trainer.vae.decode(sample)\n",
    "        if hasattr(decoded, \"sample\"):\n",
    "            image = decoded.sample\n",
    "        else:\n",
    "            image = decoded\n",
    "    return image  # Expected shape: [1, C, H, W]\n",
    "\n",
    "def get_original_image(data, node_index):\n",
    "    \"\"\"\n",
    "    Retrieve the original image patch for a specified node from the graph data.\n",
    "    \n",
    "    Parameters:\n",
    "      - data: The graph data object containing data.patches (assumed shape [N, H, W, 1]).\n",
    "      - node_index: int, the index of the target node.\n",
    "      \n",
    "    Returns:\n",
    "      - image: Tensor of the original patch with shape [1, C, H, W].\n",
    "      \n",
    "    Note:\n",
    "      The original patch is in shape [H, W, 1]. We convert it to [1, 1, H, W] (batch, channel, H, W).\n",
    "    \"\"\"\n",
    "    patch = data.patches[node_index]  # shape: [H, W, 1]\n",
    "    # Convert patch to tensor and permute channels: from [H, W, 1] to [1, 1, H, W]\n",
    "    patch_tensor = torch.tensor(patch).permute(2, 0, 1).unsqueeze(0).float()\n",
    "    return patch_tensor\n",
    "\n",
    "def postprocess_image(image_tensor):\n",
    "    \"\"\"\n",
    "    Postprocess the image tensor to a numpy array normalized to [0,1], ready for visualization.\n",
    "    \n",
    "    Parameters:\n",
    "      - image_tensor: Tensor with shape [1, C, H, W].\n",
    "      \n",
    "    Returns:\n",
    "      - image_np: numpy array in shape [H, W, C].\n",
    "      \n",
    "    If the image has only one channel, replicate it to 3 channels for color visualization.\n",
    "    \"\"\"\n",
    "    img = image_tensor.squeeze(0).cpu()  # shape becomes [C, H, W]\n",
    "    if img.shape[0] == 1:\n",
    "        img = img.repeat(3, 1, 1)  # Convert grayscale to 3 channels.\n",
    "    image_np = img.permute(1, 2, 0).numpy()  # shape: [H, W, 3]\n",
    "    image_np = (image_np - image_np.min()) / (image_np.max() - image_np.min() + 1e-8)\n",
    "    return image_np\n",
    "\n",
    "def visualize_comparison(trainer, num_samples=5, num_inference_steps=50):\n",
    "    \"\"\"\n",
    "    Randomly select num_samples positive nodes and visualize their original and predicted image patches.\n",
    "    The visualization is arranged in 2 rows and 5 columns:\n",
    "      - The first row shows the Original images for the selected samples.\n",
    "      - The second row shows the corresponding Predicted images.\n",
    "      \n",
    "    Parameters:\n",
    "      - trainer: The GraphConditionedLDMTrainer instance (with attributes: all_nodes, graph_data_dict, etc.)\n",
    "      - num_samples: int, number of samples (columns) to display.\n",
    "      - num_inference_steps: int, number of inference steps during reverse diffusion sampling.\n",
    "    \"\"\"\n",
    "    device = trainer.device\n",
    "    selected_nodes = random.sample(trainer.all_nodes, num_samples)\n",
    "    \n",
    "    # Create subplots: 2 rows x num_samples columns.\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(4*num_samples, 8))\n",
    "    \n",
    "    # Prepare lists to store images for debugging if needed\n",
    "    original_images = []\n",
    "    predicted_images = []\n",
    "    \n",
    "    for i, (graph_key, node_index) in enumerate(selected_nodes):\n",
    "        data = trainer.graph_data_dict[graph_key]\n",
    "        \n",
    "        # Get the original patch image.\n",
    "        original = get_original_image(data, node_index)  # shape: [1, 1, H, W]\n",
    "        original_np = postprocess_image(original)  # shape: [H, W, 3]\n",
    "        \n",
    "        # Generate the predicted image via the diffusion process.\n",
    "        predicted = generate_predicted_image(trainer, graph_key, node_index, num_inference_steps=num_inference_steps)\n",
    "        predicted_np = postprocess_image(predicted)  # shape: [H, W, 3]\n",
    "        \n",
    "        original_images.append(original_np)\n",
    "        predicted_images.append(predicted_np)\n",
    "        \n",
    "        # Place Original image in first row at column i.\n",
    "        axes[0, i].imshow(original_np)\n",
    "        axes[0, i].set_title(f\"Original\\n(Graph: {graph_key}, Node: {node_index})\", fontsize=12)\n",
    "        axes[0, i].axis(\"off\")\n",
    "        \n",
    "        # Place Predicted image in second row at column i.\n",
    "        axes[1, i].imshow(predicted_np)\n",
    "        axes[1, i].set_title(f\"Predicted\\n(Graph: {graph_key}, Node: {node_index})\", fontsize=6)\n",
    "        axes[1, i].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    # Assume that trainer, graph_data_dict, and trainer.all_nodes have been properly constructed.\n",
    "    # trainer.all_nodes is a list of tuples: [(graph_key, node_index), ...]\n",
    "    visualize_comparison(trainer, num_samples=5, num_inference_steps=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def generate_predicted_image(trainer, graph_key, node_index, num_inference_steps=50):\n",
    "    \"\"\"\n",
    "    Generate the predicted image (patch) by sampling from the trained diffusion model.\n",
    "    \n",
    "    Parameters:\n",
    "      - trainer: An instance of GraphConditionedLDMTrainer containing trained models (vae, unet, noise_scheduler, etc.)\n",
    "      - graph_key: str, key indicating the target graph.\n",
    "      - node_index: int, the index of the target node in the graph.\n",
    "      - num_inference_steps: int, number of inference steps for the reverse diffusion process.\n",
    "      \n",
    "    Returns:\n",
    "      - image: Tensor of the generated image with shape [1, C, H, W].\n",
    "    \"\"\"\n",
    "    device = trainer.device\n",
    "    data = trainer.graph_data_dict[graph_key]\n",
    "    # Use precomputed latent's shape (for the specified node) to determine the latent shape.\n",
    "    latent_shape = data.latents[node_index:node_index+1].shape  # [1, latent_channels, H_lat, W_lat]\n",
    "    \n",
    "    # Compute full graph node embeddings, then select the condition for the specified node.\n",
    "    x_full = data.x.to(device)\n",
    "    edge_index = data.edge_index.to(device)\n",
    "    with torch.no_grad():\n",
    "        node_emb_all = trainer.graph_clip_encoder(x_full, edge_index)  # shape: [N, embed_dim]\n",
    "    condition = node_emb_all[node_index].unsqueeze(0).unsqueeze(1)  # shape: [1, 1, embed_dim]\n",
    "    \n",
    "    # Initialize latent representation with random noise.\n",
    "    sample = torch.randn(latent_shape, device=device)\n",
    "    \n",
    "    trainer.unet.eval()\n",
    "    trainer.noise_scheduler.set_timesteps(num_inference_steps)\n",
    "    \n",
    "    # Reverse diffusion process: iterate through timesteps to denoise.\n",
    "    for t in tqdm(trainer.noise_scheduler.timesteps, desc=f\"Sampling (Graph: {graph_key} Node: {node_index})\", leave=False):\n",
    "        t_tensor = torch.tensor([t], device=device).long()\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                model_out = trainer.unet(\n",
    "                    sample=sample,\n",
    "                    timestep=t_tensor,\n",
    "                    encoder_hidden_states=condition  # [1, 1, embed_dim]\n",
    "                )\n",
    "            pred_noise = model_out.sample  # predicted noise, same shape as sample.\n",
    "            step_output = trainer.noise_scheduler.step(pred_noise, t, sample)\n",
    "            sample = step_output[\"prev_sample\"]\n",
    "    \n",
    "    # Decode the latent representation into an image patch.\n",
    "    with torch.no_grad():\n",
    "        decoded = trainer.vae.decode(sample)\n",
    "        if hasattr(decoded, \"sample\"):\n",
    "            image = decoded.sample\n",
    "        else:\n",
    "            image = decoded\n",
    "    return image  # Expected shape: [1, C, H, W]\n",
    "\n",
    "def get_original_image(data, node_index):\n",
    "    \"\"\"\n",
    "    Retrieve the original image patch for a specified node from the graph data.\n",
    "    \n",
    "    Parameters:\n",
    "      - data: The graph data object containing data.patches (assumed shape [N, H, W, 1]).\n",
    "      - node_index: int, the index of the target node.\n",
    "      \n",
    "    Returns:\n",
    "      - image: Tensor of the original patch with shape [1, C, H, W].\n",
    "      \n",
    "    Note:\n",
    "      The original patch is in shape [H, W, 1]. We convert it to [1, 1, H, W] (batch, channel, H, W).\n",
    "    \"\"\"\n",
    "    patch = data.patches[node_index]  # shape: [H, W, 1]\n",
    "    # Convert patch to tensor and permute channels: from [H, W, 1] to [1, 1, H, W]\n",
    "    patch_tensor = torch.tensor(patch).permute(2, 0, 1).unsqueeze(0).float()\n",
    "    return patch_tensor\n",
    "\n",
    "def postprocess_image(image_tensor):\n",
    "    \"\"\"\n",
    "    Postprocess the image tensor to a numpy array normalized to [0,1], ready for visualization.\n",
    "    \n",
    "    Parameters:\n",
    "      - image_tensor: Tensor with shape [1, C, H, W].\n",
    "      \n",
    "    Returns:\n",
    "      - image_np: numpy array in shape [H, W, C].\n",
    "      \n",
    "    If the image has only one channel, replicate it to 3 channels for color visualization.\n",
    "    \"\"\"\n",
    "    img = image_tensor.squeeze(0).cpu()  # shape becomes [C, H, W]\n",
    "    if img.shape[0] == 1:\n",
    "        img = img.repeat(3, 1, 1)  # Convert grayscale to 3 channels.\n",
    "    image_np = img.permute(1, 2, 0).numpy()  # shape: [H, W, 3]\n",
    "    image_np = (image_np - image_np.min()) / (image_np.max() - image_np.min() + 1e-8)\n",
    "    return image_np\n",
    "\n",
    "def visualize_comparison(trainer, num_samples=5, num_inference_steps=50):\n",
    "    \"\"\"\n",
    "    Randomly select num_samples positive nodes and visualize their original and predicted image patches.\n",
    "    The visualization is arranged in 2 rows and 5 columns:\n",
    "      - The first row shows the Original images for the selected samples.\n",
    "      - The second row shows the corresponding Predicted images.\n",
    "      \n",
    "    Parameters:\n",
    "      - trainer: The GraphConditionedLDMTrainer instance (with attributes: all_nodes, graph_data_dict, etc.)\n",
    "      - num_samples: int, number of samples (columns) to display.\n",
    "      - num_inference_steps: int, number of inference steps during reverse diffusion sampling.\n",
    "    \"\"\"\n",
    "    device = trainer.device\n",
    "    selected_nodes = random.sample(trainer.all_nodes, num_samples)\n",
    "    \n",
    "    # Create subplots: 2 rows x num_samples columns.\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(4*num_samples, 8))\n",
    "    \n",
    "    # Prepare lists to store images for debugging if needed\n",
    "    original_images = []\n",
    "    predicted_images = []\n",
    "    \n",
    "    for i, (graph_key, node_index) in enumerate(selected_nodes):\n",
    "        data = trainer.graph_data_dict[graph_key]\n",
    "        \n",
    "        # Get the original patch image.\n",
    "        original = get_original_image(data, node_index)  # shape: [1, 1, H, W]\n",
    "        original_np = postprocess_image(original)  # shape: [H, W, 3]\n",
    "        \n",
    "        # Generate the predicted image via the diffusion process.\n",
    "        predicted = generate_predicted_image(trainer, graph_key, node_index, num_inference_steps=num_inference_steps)\n",
    "        predicted_np = postprocess_image(predicted)  # shape: [H, W, 3]\n",
    "        \n",
    "        original_images.append(original_np)\n",
    "        predicted_images.append(predicted_np)\n",
    "        graph_name = graph_key.split(\"-\")[0] + \"...\" + graph_key.split(\"-\")[-1]\n",
    "        \n",
    "        # Place Original image in first row at column i.\n",
    "        axes[0, i].imshow(original_np)\n",
    "        axes[0, i].set_title(f\"Original\\n(Graph: {graph_name}, Node: {node_index})\", fontsize=12)\n",
    "        axes[0, i].axis(\"off\")\n",
    "        \n",
    "        # Place Predicted image in second row at column i.\n",
    "        axes[1, i].imshow(predicted_np)\n",
    "        \n",
    "        axes[1, i].set_title(f\"Predicted\\n(Graph: {graph_name}, Node: {node_index})\", fontsize=12)\n",
    "        axes[1, i].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    # Assume that trainer, graph_data_dict, and trainer.all_nodes have been properly constructed.\n",
    "    # trainer.all_nodes is a list of tuples: [(graph_key, node_index), ...]\n",
    "    visualize_comparison(trainer, num_samples=5, num_inference_steps=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_comparison(trainer, num_samples=20, num_inference_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_comparison(trainer, num_samples=50, num_inference_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_comparison(trainer, num_samples=50, num_inference_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_comparison(trainer, num_samples=10, num_inference_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_comparison(trainer, num_samples=5, num_inference_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python GPU",
   "language": "python",
   "name": "gpu_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
