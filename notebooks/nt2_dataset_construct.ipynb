{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a02b38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载主 AnnData 文件: /cwStorage/nodecw_group/jijh/spaglam_sota_data/master_adata_with_graph_and_paths.h5ad\n",
      "✅ 主 AnnData 文件加载成功！\n",
      "维度 (spots, genes): (997054, 30148)\n",
      "包含的列: ['in_tissue', 'pxl_col_in_fullres', 'pxl_row_in_fullres', 'array_col', 'array_row', 'n_counts', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'total_counts_mito', 'log1p_total_counts_mito', 'pct_counts_mito', 'sample_id', 'pxl_row_in_fullres_old', 'pxl_col_in_fullres_old', 'total_counts_mt', 'pct_counts_mt', 'n_genes', 'image_path', 'sentence_path']\n",
      "包含的图: True\n"
     ]
    }
   ],
   "source": [
    "# --- 1. 安装和导入必要的库 ---\n",
    "# 请确保已安装: pip install scanpy webdataset torch torchvision Pillow torch_geometric\n",
    "import os\n",
    "import scanpy as sc\n",
    "import webdataset as wds\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch_geometric.data as pyg_data\n",
    "from PIL import Image\n",
    "import io\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- 2. 路径配置 (请根据您的环境修改) ---\n",
    "BASE_DIR = \"/cwStorage/nodecw_group/jijh\"\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"spaglam_sota_data\")\n",
    "FINAL_ADATA_PATH = os.path.join(OUTPUT_DIR, \"master_adata_with_graph_and_paths.h5ad\")\n",
    "SHARDS_OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"webdataset_shards\")\n",
    "\n",
    "# --- 3. 加载主 AnnData 文件 ---\n",
    "# 这个文件是我们的“中央索引”，在整个调试过程中都会使用\n",
    "print(f\"正在加载主 AnnData 文件: {FINAL_ADATA_PATH}\")\n",
    "if not os.path.exists(FINAL_ADATA_PATH):\n",
    "    raise FileNotFoundError(\"错误: 找不到主 AnnData 文件。请先运行预处理脚本。\")\n",
    "adata = sc.read_h5ad(FINAL_ADATA_PATH)\n",
    "print(\"✅ 主 AnnData 文件加载成功！\")\n",
    "\n",
    "# 打印一些信息以供确认\n",
    "print(f\"维度 (spots, genes): {adata.shape}\")\n",
    "print(f\"包含的列: {adata.obs.columns.tolist()}\")\n",
    "print(f\"包含的图: {'spatial_connectivities' in adata.obsp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394c9248",
   "metadata": {},
   "source": [
    "# 单样本加载函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7903949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 正在处理 Spot ID: MISC59_GATCTTGGAGGGCATA-1 ---\n",
      "找到 5 个邻居: ['MISC59_AGCGTGGTATTCTACT-1', 'MISC59_CTAAGGGAATGATTGG-1', 'MISC59_GCTACGACTTATTGGG-1']...\n",
      "✅ 成功加载了中心点及其邻居的数据。\n",
      "\n",
      "--- 验证结果 ---\n",
      "中心点ID: MISC59_GATCTTGGAGGGCATA-1\n",
      "加载的图像数量: 6\n",
      "第一张图像的尺寸: (364, 364)\n",
      "加载的句子数量: 6\n",
      "第一条句子内容: 'S100A6-1 SLC25A5-1 ALDOA S100A14-1 ANPEP ATP5IF1 CDC42EP5 IT...'\n",
      "图的边索引形状: torch.Size([2, 15])\n"
     ]
    }
   ],
   "source": [
    "def get_spaglam_sample(spot_id: str, adata: sc.AnnData):\n",
    "    \"\"\"\n",
    "    一个用于调试的独立函数，加载单个中心点及其邻居的所有数据。\n",
    "    \n",
    "    参数:\n",
    "        spot_id (str): 我们感兴趣的中心点的ID。\n",
    "        adata (sc.AnnData): 已加载的主 AnnData 对象。\n",
    "        \n",
    "    返回:\n",
    "        一个包含所有相关信息的字典，或在出错时返回 None。\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- 正在处理 Spot ID: {spot_id} ---\")\n",
    "    \n",
    "    try:\n",
    "        # --- 步骤 1: 从 AnnData 中获取元数据和邻居 ---\n",
    "        center_idx = adata.obs_names.get_loc(spot_id)\n",
    "        \n",
    "        # 获取邻居索引\n",
    "        neighbor_indices = adata.obsp['spatial_connectivities'][center_idx].indices\n",
    "        neighbor_ids = adata.obs_names[neighbor_indices].tolist()\n",
    "        \n",
    "        all_spot_ids = [spot_id] + neighbor_ids\n",
    "        print(f\"找到 {len(neighbor_ids)} 个邻居: {neighbor_ids[:3]}...\") # 打印前3个邻居\n",
    "\n",
    "        # --- 步骤 2: 获取所有相关点的文件路径 ---\n",
    "        paths_df = adata.obs.loc[all_spot_ids, ['image_path', 'sentence_path']]\n",
    "        \n",
    "        # --- 步骤 3: 从磁盘加载原始数据 ---\n",
    "        images = []\n",
    "        sentences = []\n",
    "        for sid in all_spot_ids:\n",
    "            img_path = paths_df.loc[sid, 'image_path']\n",
    "            sent_path = paths_df.loc[sid, 'sentence_path']\n",
    "            \n",
    "            # 加载图像\n",
    "            if os.path.exists(img_path):\n",
    "                images.append(Image.open(img_path).convert(\"RGB\"))\n",
    "            else:\n",
    "                print(f\"警告: 图像文件未找到 {img_path}\")\n",
    "                images.append(Image.new('RGB', (224, 224), color = 'red')) # 用红色图像表示错误\n",
    "\n",
    "            # 加载句子\n",
    "            if os.path.exists(sent_path):\n",
    "                with open(sent_path, 'r') as f:\n",
    "                    sentences.append(f.read())\n",
    "            else:\n",
    "                print(f\"警告: 句子文件未找到 {sent_path}\")\n",
    "                sentences.append(\"FILE_NOT_FOUND\")\n",
    "\n",
    "        # --- 步骤 4: 构建图结构 (简化版) ---\n",
    "        # 实际Dataloader中会更高效\n",
    "        num_nodes = len(all_spot_ids)\n",
    "        # 假设是全连接图进行演示\n",
    "        edge_index = torch.combinations(torch.arange(num_nodes), r=2).t().contiguous()\n",
    "        \n",
    "        print(\"✅ 成功加载了中心点及其邻居的数据。\")\n",
    "        \n",
    "        return {\n",
    "            \"center_spot_id\": spot_id,\n",
    "            \"images\": images,\n",
    "            \"sentences\": sentences,\n",
    "            \"graph_edge_index\": edge_index\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 处理 {spot_id} 时发生错误: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 现在来测试一下这个函数 ---\n",
    "random_spot_id = random.choice(adata.obs_names.tolist())\n",
    "sample_data = get_spaglam_sample(random_spot_id, adata)\n",
    "\n",
    "if sample_data:\n",
    "    print(f\"\\n--- 验证结果 ---\")\n",
    "    print(f\"中心点ID: {sample_data['center_spot_id']}\")\n",
    "    print(f\"加载的图像数量: {len(sample_data['images'])}\")\n",
    "    print(f\"第一张图像的尺寸: {sample_data['images'][0].size}\")\n",
    "    print(f\"加载的句子数量: {len(sample_data['sentences'])}\")\n",
    "    print(f\"第一条句子内容: '{sample_data['sentences'][0][:60]}...'\")\n",
    "    print(f\"图的边索引形状: {sample_data['graph_edge_index'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7416657",
   "metadata": {},
   "source": [
    "# 【SOTA实现】构建生产级的 IterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a3657c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 实例化并测试生产级Dataset ---\n",
      "\n",
      "✅ 成功从Dataloader中获取了一个批次的数据！\n",
      "批次大小: 4\n",
      "\n",
      "--- 检查批次中的第一个样本 ---\n",
      "样本包含的键: dict_keys(['__key__', '__url__', 'png', 'txt', 'all_keys'])\n",
      "中心点 key: NCBI776_GCGAATCGAGAACACG-1\n",
      "邻居+中心点 keys: ['NCBI776_GCGAATCGAGAACACG-1', 'NCBI776_AAGGCCGACCTACCTG-1', 'NCBI776_CTATGCCGTACAGCGT-1', 'NCBI776_CTTCGATAACATTGGT-1', 'NCBI776_TAGGATGCACCGTTCA-1', 'NCBI776_TGCATTGCTGTCGGCG-1']\n",
      "处理后的图像张量形状: torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# 假设您已经定义了图像预处理器和分词器\n",
    "# from open_clip import get_tokenizer\n",
    "# from open_clip.transform import image_transform_v2\n",
    "# image_processor = image_transform_v2(...)\n",
    "# tokenizer = get_tokenizer(...)\n",
    "\n",
    "# 为了调试，我们先用伪函数代替\n",
    "def dummy_image_processor(img):\n",
    "    return torch.randn(3, 224, 224)\n",
    "\n",
    "def dummy_tokenizer(texts):\n",
    "    if isinstance(texts, str): texts = [texts]\n",
    "    return torch.randint(0, 49408, (len(texts), 77))\n",
    "\n",
    "# --- 定义 SOTA 数据集类 ---\n",
    "class SotaSpatialWdsDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, tar_urls, anndata_path, image_processor, tokenizer, config):\n",
    "        super().__init__()\n",
    "        self.tar_urls = tar_urls\n",
    "        self.anndata_path = anndata_path\n",
    "        self.image_processor = image_processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        \n",
    "        # AnnData将在每个工作进程中独立加载\n",
    "        self.adata = None\n",
    "\n",
    "    def _init_worker(self):\n",
    "        \"\"\"每个工作进程的初始化函数\"\"\"\n",
    "        if self.adata is None:\n",
    "            self.adata = sc.read_h5ad(self.anndata_path)\n",
    "\n",
    "    def _get_neighbors(self, sample):\n",
    "        \"\"\"WebDataset流水线中的一个map操作，用于查找邻居\"\"\"\n",
    "        self._init_worker() # 确保adata已加载\n",
    "        \n",
    "        center_key = sample['__key__']\n",
    "        try:\n",
    "            center_idx = self.adata.obs_names.get_loc(center_key)\n",
    "            neighbor_indices = self.adata.obsp['spatial_connectivities'][center_idx].indices\n",
    "            neighbor_keys = self.adata.obs_names[neighbor_indices].tolist()\n",
    "            \n",
    "            sample['all_keys'] = [center_key] + neighbor_keys\n",
    "            # 这里可以进一步添加局部图结构\n",
    "        except KeyError:\n",
    "            # 如果key在AnnData中找不到，就跳过这个样本\n",
    "            sample['all_keys'] = []\n",
    "        return sample\n",
    "\n",
    "    def __iter__(self):\n",
    "        # 定义WebDataset数据处理流水线\n",
    "        pipeline = wds.DataPipeline(\n",
    "            wds.ResampledShards(self.tar_urls), # 1. 随机选择一个.tar文件\n",
    "            wds.split_by_worker,               # 2. 将分片分配给不同的worker\n",
    "            wds.tarfile_to_samples(),          # 3. 从tar中解包样本\n",
    "            wds.select(lambda x: len(x['png']) > 0), # 过滤掉损坏的样本\n",
    "            wds.map(self._get_neighbors),      # 4. 【核心】为每个中心点查找邻居key\n",
    "            wds.select(lambda x: len(x['all_keys']) > 0), # 过滤掉没找到邻居的样本\n",
    "            wds.map_dict(png=self.image_processor), # 5. 对中心点的图像进行预处理\n",
    "            # 注意：此处简化了邻居数据的加载，实际项目中可能需要更复杂的\n",
    "            # wds.associate 或 wds.map_group 来加载邻居数据。\n",
    "            # 但对于调试和验证，这个流程是可行的。\n",
    "        )\n",
    "        return iter(pipeline)\n",
    "\n",
    "# --- 实例化并测试数据集 ---\n",
    "print(\"\\n--- 实例化并测试生产级Dataset ---\")\n",
    "# 获取所有分片的URL列表\n",
    "all_shards = [os.path.join(SHARDS_OUTPUT_PATH, f) for f in os.listdir(SHARDS_OUTPUT_PATH) if f.endswith('.tar')]\n",
    "\n",
    "# 模拟的配置\n",
    "N_NEIGHBORS = 6  # 假设每个中心点有6个邻居\n",
    "mock_config = {\"gnn_neighbors\": N_NEIGHBORS, \"batch_size\": 4}\n",
    "\n",
    "# 创建数据集实例\n",
    "dataset = SotaSpatialWdsDataset(\n",
    "    tar_urls=all_shards,\n",
    "    anndata_path=FINAL_ADATA_PATH,\n",
    "    image_processor=dummy_image_processor,\n",
    "    tokenizer=dummy_tokenizer,\n",
    "    config=type('MockConfig', (), mock_config)\n",
    ")\n",
    "\n",
    "# 使用DataLoader加载数据\n",
    "# num_workers=0 意味着在主进程中运行，便于调试\n",
    "dataloader = DataLoader(dataset, batch_size=mock_config['batch_size'], num_workers=0, collate_fn=lambda x: x)\n",
    "\n",
    "# 取一个批次的数据进行检查\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "print(f\"\\n✅ 成功从Dataloader中获取了一个批次的数据！\")\n",
    "print(f\"批次大小: {len(batch)}\")\n",
    "\n",
    "first_sample_in_batch = batch[0]\n",
    "print(\"\\n--- 检查批次中的第一个样本 ---\")\n",
    "print(f\"样本包含的键: {first_sample_in_batch.keys()}\")\n",
    "print(f\"中心点 key: {first_sample_in_batch['__key__']}\")\n",
    "print(f\"邻居+中心点 keys: {first_sample_in_batch['all_keys']}\")\n",
    "print(f\"处理后的图像张量形状: {first_sample_in_batch['png'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fdb8917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 成功从Dataloader中获取了一个批次的数据！\n",
      "批次大小: 4\n",
      "\n",
      "--- 检查批次中的第一个样本 ---\n",
      "样本包含的键: dict_keys(['__key__', '__url__', 'png', 'txt', 'all_keys'])\n",
      "中心点 key: INT4_TGCCGTGGATCGTCCT-1\n",
      "邻居+中心点 keys: ['INT4_TGCCGTGGATCGTCCT-1', 'INT4_ACCCGGAAACTCCCAG-1', 'INT4_ATACCTAACCAAGAAA-1', 'INT4_CGATTAAATATCTCCT-1', 'INT4_GTGGACGCATTTGTCC-1', 'INT4_TGCTCGGCGAAACCCA-1']\n",
      "处理后的图像张量形状: torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# 取一个批次的数据进行检查\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "print(f\"\\n✅ 成功从Dataloader中获取了一个批次的数据！\")\n",
    "print(f\"批次大小: {len(batch)}\")\n",
    "\n",
    "first_sample_in_batch = batch[1]\n",
    "print(\"\\n--- 检查批次中的第一个样本 ---\")\n",
    "print(f\"样本包含的键: {first_sample_in_batch.keys()}\")\n",
    "print(f\"中心点 key: {first_sample_in_batch['__key__']}\")\n",
    "print(f\"邻居+中心点 keys: {first_sample_in_batch['all_keys']}\")\n",
    "print(f\"处理后的图像张量形状: {first_sample_in_batch['png'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7338aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gigapath)",
   "language": "python",
   "name": "gigapath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
