{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = '/cwStorage/nodecw_group/jijh/hest_1k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_dataset in module datasets.load:\n",
      "\n",
      "load_dataset(path: str, name: Optional[str] = None, data_dir: Optional[str] = None, data_files: Union[str, Sequence[str], Mapping[str, Union[str, Sequence[str]]], NoneType] = None, split: Union[str, datasets.splits.Split, NoneType] = None, cache_dir: Optional[str] = None, features: Optional[datasets.features.features.Features] = None, download_config: Optional[datasets.download.download_config.DownloadConfig] = None, download_mode: Union[datasets.download.download_manager.DownloadMode, str, NoneType] = None, verification_mode: Union[datasets.utils.info_utils.VerificationMode, str, NoneType] = None, keep_in_memory: Optional[bool] = None, save_infos: bool = False, revision: Union[str, datasets.utils.version.Version, NoneType] = None, token: Union[bool, str, NoneType] = None, streaming: bool = False, num_proc: Optional[int] = None, storage_options: Optional[Dict] = None, trust_remote_code: bool = None, **config_kwargs) -> Union[datasets.dataset_dict.DatasetDict, datasets.arrow_dataset.Dataset, datasets.dataset_dict.IterableDatasetDict, datasets.iterable_dataset.IterableDataset]\n",
      "    Load a dataset from the Hugging Face Hub, or a local dataset.\n",
      "\n",
      "    You can find the list of datasets on the [Hub](https://huggingface.co/datasets) or with [`huggingface_hub.list_datasets`].\n",
      "\n",
      "    A dataset is a directory that contains some data files in generic formats (JSON, CSV, Parquet, etc.) and possibly\n",
      "    in a generic structure (Webdataset, ImageFolder, AudioFolder, VideoFolder, etc.)\n",
      "\n",
      "    This function does the following under the hood:\n",
      "\n",
      "        1. Load a dataset builder:\n",
      "\n",
      "            * Find the most common data format in the dataset and pick its associated builder (JSON, CSV, Parquet, Webdataset, ImageFolder, AudioFolder, etc.)\n",
      "            * Find which file goes into which split (e.g. train/test) based on file and directory names or on the YAML configuration\n",
      "            * It is also possible to specify `data_files` manually, and which dataset builder to use (e.g. \"parquet\").\n",
      "\n",
      "        2. Run the dataset builder:\n",
      "\n",
      "            In the general case:\n",
      "\n",
      "            * Download the data files from the dataset if they are not already available locally or cached.\n",
      "            * Process and cache the dataset in typed Arrow tables for caching.\n",
      "\n",
      "                Arrow table are arbitrarily long, typed tables which can store nested objects and be mapped to numpy/pandas/python generic types.\n",
      "                They can be directly accessed from disk, loaded in RAM or even streamed over the web.\n",
      "\n",
      "            In the streaming case:\n",
      "\n",
      "            * Don't download or cache anything. Instead, the dataset is lazily loaded and will be streamed on-the-fly when iterating on it.\n",
      "\n",
      "        3. Return a dataset built from the requested splits in `split` (default: all).\n",
      "\n",
      "    It can also use a custom dataset builder if the dataset contains a dataset script, but this feature is mostly for backward compatibility.\n",
      "    In this case the dataset script file must be named after the dataset repository or directory and end with \".py\".\n",
      "\n",
      "    Args:\n",
      "\n",
      "        path (`str`):\n",
      "            Path or name of the dataset.\n",
      "\n",
      "            - if `path` is a dataset repository on the HF hub (list all available datasets with [`huggingface_hub.list_datasets`])\n",
      "              -> load the dataset from supported files in the repository (csv, json, parquet, etc.)\n",
      "              e.g. `'username/dataset_name'`, a dataset repository on the HF hub containing the data files.\n",
      "\n",
      "            - if `path` is a local directory\n",
      "              -> load the dataset from supported files in the directory (csv, json, parquet, etc.)\n",
      "              e.g. `'./path/to/directory/with/my/csv/data'`.\n",
      "\n",
      "            - if `path` is the name of a dataset builder and `data_files` or `data_dir` is specified\n",
      "              (available builders are \"json\", \"csv\", \"parquet\", \"arrow\", \"text\", \"xml\", \"webdataset\", \"imagefolder\", \"audiofolder\", \"videofolder\")\n",
      "              -> load the dataset from the files in `data_files` or `data_dir`\n",
      "              e.g. `'parquet'`.\n",
      "\n",
      "            It can also point to a local dataset script but this is not recommended.\n",
      "        name (`str`, *optional*):\n",
      "            Defining the name of the dataset configuration.\n",
      "        data_dir (`str`, *optional*):\n",
      "            Defining the `data_dir` of the dataset configuration. If specified for the generic builders (csv, text etc.) or the Hub datasets and `data_files` is `None`,\n",
      "            the behavior is equal to passing `os.path.join(data_dir, **)` as `data_files` to reference all the files in a directory.\n",
      "        data_files (`str` or `Sequence` or `Mapping`, *optional*):\n",
      "            Path(s) to source data file(s).\n",
      "        split (`Split` or `str`):\n",
      "            Which split of the data to load.\n",
      "            If `None`, will return a `dict` with all splits (typically `datasets.Split.TRAIN` and `datasets.Split.TEST`).\n",
      "            If given, will return a single Dataset.\n",
      "            Splits can be combined and specified like in tensorflow-datasets.\n",
      "        cache_dir (`str`, *optional*):\n",
      "            Directory to read/write data. Defaults to `\"~/.cache/huggingface/datasets\"`.\n",
      "        features (`Features`, *optional*):\n",
      "            Set the features type to use for this dataset.\n",
      "        download_config ([`DownloadConfig`], *optional*):\n",
      "            Specific download configuration parameters.\n",
      "        download_mode ([`DownloadMode`] or `str`, defaults to `REUSE_DATASET_IF_EXISTS`):\n",
      "            Download/generate mode.\n",
      "        verification_mode ([`VerificationMode`] or `str`, defaults to `BASIC_CHECKS`):\n",
      "            Verification mode determining the checks to run on the downloaded/processed dataset information (checksums/size/splits/...).\n",
      "\n",
      "            <Added version=\"2.9.1\"/>\n",
      "        keep_in_memory (`bool`, defaults to `None`):\n",
      "            Whether to copy the dataset in-memory. If `None`, the dataset\n",
      "            will not be copied in-memory unless explicitly enabled by setting `datasets.config.IN_MEMORY_MAX_SIZE` to\n",
      "            nonzero. See more details in the [improve performance](../cache#improve-performance) section.\n",
      "        save_infos (`bool`, defaults to `False`):\n",
      "            Save the dataset information (checksums/size/splits/...).\n",
      "        revision ([`Version`] or `str`, *optional*):\n",
      "            Version of the dataset script to load.\n",
      "            As datasets have their own git repository on the Datasets Hub, the default version \"main\" corresponds to their \"main\" branch.\n",
      "            You can specify a different version than the default \"main\" by using a commit SHA or a git tag of the dataset repository.\n",
      "        token (`str` or `bool`, *optional*):\n",
      "            Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n",
      "            If `True`, or not specified, will get token from `\"~/.huggingface\"`.\n",
      "        streaming (`bool`, defaults to `False`):\n",
      "            If set to `True`, don't download the data files. Instead, it streams the data progressively while\n",
      "            iterating on the dataset. An [`IterableDataset`] or [`IterableDatasetDict`] is returned instead in this case.\n",
      "\n",
      "            Note that streaming works for datasets that use data formats that support being iterated over like txt, csv, jsonl for example.\n",
      "            Json files may be downloaded completely. Also streaming from remote zip or gzip files is supported but other compressed formats\n",
      "            like rar and xz are not yet supported. The tgz format doesn't allow streaming.\n",
      "        num_proc (`int`, *optional*, defaults to `None`):\n",
      "            Number of processes when downloading and generating the dataset locally.\n",
      "            Multiprocessing is disabled by default.\n",
      "\n",
      "            <Added version=\"2.7.0\"/>\n",
      "        storage_options (`dict`, *optional*, defaults to `None`):\n",
      "            **Experimental**. Key/value pairs to be passed on to the dataset file-system backend, if any.\n",
      "\n",
      "            <Added version=\"2.11.0\"/>\n",
      "        trust_remote_code (`bool`, defaults to `False`):\n",
      "            Whether or not to allow for datasets defined on the Hub using a dataset script. This option\n",
      "            should only be set to `True` for repositories you trust and in which you have read the code, as it will\n",
      "            execute code present on the Hub on your local machine.\n",
      "\n",
      "            <Added version=\"2.16.0\"/>\n",
      "\n",
      "            <Changed version=\"2.20.0\">\n",
      "\n",
      "            `trust_remote_code` defaults to `False` if not specified.\n",
      "\n",
      "            </Changed>\n",
      "\n",
      "        **config_kwargs (additional keyword arguments):\n",
      "            Keyword arguments to be passed to the `BuilderConfig`\n",
      "            and used in the [`DatasetBuilder`].\n",
      "\n",
      "    Returns:\n",
      "        [`Dataset`] or [`DatasetDict`]:\n",
      "        - if `split` is not `None`: the dataset requested,\n",
      "        - if `split` is `None`, a [`~datasets.DatasetDict`] with each split.\n",
      "\n",
      "        or [`IterableDataset`] or [`IterableDatasetDict`]: if `streaming=True`\n",
      "\n",
      "        - if `split` is not `None`, the dataset is requested\n",
      "        - if `split` is `None`, a [`~datasets.streaming.IterableDatasetDict`] with each split.\n",
      "\n",
      "    Example:\n",
      "\n",
      "    Load a dataset from the Hugging Face Hub:\n",
      "\n",
      "    ```py\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('cornell-movie-review-data/rotten_tomatoes', split='train')\n",
      "\n",
      "    # Load a subset or dataset configuration (here 'sst2')\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('nyu-mll/glue', 'sst2', split='train')\n",
      "\n",
      "    # Manual mapping of data files to splits\n",
      "    >>> data_files = {'train': 'train.csv', 'test': 'test.csv'}\n",
      "    >>> ds = load_dataset('namespace/your_dataset_name', data_files=data_files)\n",
      "\n",
      "    # Manual selection of a directory to load\n",
      "    >>> ds = load_dataset('namespace/your_dataset_name', data_dir='folder_name')\n",
      "    ```\n",
      "\n",
      "    Load a local dataset:\n",
      "\n",
      "    ```py\n",
      "    # Load a CSV file\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('csv', data_files='path/to/local/my_dataset.csv')\n",
      "\n",
      "    # Load a JSON file\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('json', data_files='path/to/local/my_dataset.json')\n",
      "\n",
      "    # Load from a local loading script (not recommended)\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('path/to/local/loading_script/loading_script.py', split='train')\n",
      "    ```\n",
      "\n",
      "    Load an [`~datasets.IterableDataset`]:\n",
      "\n",
      "    ```py\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('cornell-movie-review-data/rotten_tomatoes', split='train', streaming=True)\n",
      "    ```\n",
      "\n",
      "    Load an image dataset with the `ImageFolder` dataset builder:\n",
      "\n",
      "    ```py\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('imagefolder', data_dir='/path/to/images', split='train')\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(datasets.load_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24cb4472b344f65b595efea336835d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating examples: 100%|██████████| 44883/44883 [00:00<00:00, 139178.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['path'],\n",
      "    num_rows: 44883\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "# 数据集脚本的本地路径\n",
    "dataset_script_path = '/cwStorage/nodecw_group/jijh/hest_1k/hest.py'\n",
    "\n",
    "# 本地数据集路径\n",
    "data_dir = '/cwStorage/nodecw_group/jijh/hest_1k'\n",
    "\n",
    "# 加载本地数据集脚本\n",
    "dataset = datasets.load_dataset(\n",
    "    dataset_script_path,\n",
    "    data_dir=data_dir,\n",
    "    name=\"custom_config\"  # 使用自定义配置名称\n",
    ")\n",
    "\n",
    "# 示例：查看数据集的训练集\n",
    "print(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['path'],\n",
       "    num_rows: 44883\n",
       "})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python GPU",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
