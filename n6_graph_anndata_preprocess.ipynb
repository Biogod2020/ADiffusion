{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7474d271",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35688888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# ==============================================================================\n",
    "# SOTA数据预处理工作流 for SpaGLaM (已修复多进程问题)\n",
    "#\n",
    "# 核心策略:\n",
    "# 1. 加载预先合并好的AnnData缓存文件。\n",
    "# 2. **拆分-计算-合并**: 遍历每个样本(sample_id)，独立计算其内部的空间\n",
    "#    邻接图，然后将所有图合并成一个全局的稀疏矩阵。\n",
    "# 3. 将所有独立的图像(.png)和基因语句(.txt)文件打包成高效的.tar分片。\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import webdataset as wds\n",
    "from tqdm.auto import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from scipy.sparse import lil_matrix\n",
    "from itertools import repeat # 导入 repeat 函数\n",
    "\n",
    "# --- 1. 全局设置 ---\n",
    "\n",
    "# --- 路径配置 (请根据您的环境修改) ---\n",
    "BASE_DIR = \"/cwStorage/nodecw_group/jijh\" \n",
    "EXISTING_CACHE_PATH = os.path.join(BASE_DIR, \"hest_sentences_human_all/cache/adata_preprocessed_canonical_v2.h5ad\")\n",
    "IMAGE_BASE_DIR = os.path.join(BASE_DIR, \"hest_output\")\n",
    "SENTENCE_BASE_DIR = os.path.join(BASE_DIR, \"hest_sentences_human_all\")\n",
    "\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"spaglam_sota_data\")\n",
    "FINAL_ADATA_PATH = os.path.join(OUTPUT_DIR, \"master_adata_with_graph.h5ad\")\n",
    "SHARDS_OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"webdataset_shards\")\n",
    "\n",
    "# --- 参数配置 ---\n",
    "N_NEIGHBORS = 6\n",
    "SAMPLES_PER_SHARD = 10000\n",
    "NUM_WORKERS_PACKAGING = 16 \n",
    "\n",
    "# --- 辅助函数 ---\n",
    "def format_time(seconds: float) -> str:\n",
    "    \"\"\"将秒数格式化为易读的“分-秒”字符串\"\"\"\n",
    "    mins, secs = divmod(seconds, 60)\n",
    "    return f\"{int(mins)}分 {secs:.2f}秒\"\n",
    "\n",
    "# --- 核心功能函数 (已修改) ---\n",
    "\n",
    "def add_spatial_graph_to_adata_no_squidpy(adata: sc.AnnData, n_neighbors: int) -> sc.AnnData:\n",
    "    # ... 此函数保持不变 ...\n",
    "    print(\"\\n--- 步骤 1: 计算并添加空间邻接图 (无Squidpy替代方案) ---\")\n",
    "    start_time = time.time()\n",
    "    if 'spatial_connectivities' in adata.obsp:\n",
    "        print(\"✅ 空间邻接图 'spatial_connectivities' 已存在，跳过计算。\")\n",
    "        return adata\n",
    "    unique_samples = adata.obs['sample_id'].unique()\n",
    "    print(f\"检测到 {len(unique_samples)} 个独立样本。将逐个计算空间图...\")\n",
    "    global_conn_matrix = lil_matrix((adata.n_obs, adata.n_obs), dtype=np.float32)\n",
    "    for sample_id in tqdm(unique_samples, desc=\"处理每个样本\"):\n",
    "        adata_sample = adata[adata.obs['sample_id'] == sample_id]\n",
    "        sc.pp.neighbors(adata_sample, n_neighbors=n_neighbors, use_rep='spatial', key_added='spatial')\n",
    "        local_conn = adata_sample.obsp['spatial_connectivities']\n",
    "        global_indices = np.where(adata.obs['sample_id'] == sample_id)[0]\n",
    "        rows, cols = local_conn.nonzero()\n",
    "        global_rows = global_indices[rows]\n",
    "        global_cols = global_indices[cols]\n",
    "        global_conn_matrix[global_rows, global_cols] = local_conn.data.reshape(-1, 1)\n",
    "    adata.obsp['spatial_connectivities'] = global_conn_matrix.tocsr()\n",
    "    print(\"\\n✅ 所有样本的空间邻接图已计算并合并！\")\n",
    "    end_time = time.time()\n",
    "    print(f\"🕒 耗时: {format_time(end_time - start_time)}\")\n",
    "    return adata\n",
    "\n",
    "# ==============================================================================\n",
    "# 【【【关键修改点】】】\n",
    "# 将 process_chunk 函数定义在全局作用域中，使其成为一个顶层函数\n",
    "# ==============================================================================\n",
    "def _process_chunk_worker(args):\n",
    "    \"\"\"\n",
    "    这是一个独立的工作函数，设计用于被多进程池调用。\n",
    "    它接收一个元组作为参数，包含了所有需要的信息。\n",
    "    \"\"\"\n",
    "    # 从元组中解包参数\n",
    "    spot_ids_chunk, chunk_index, all_spot_info, output_pattern, samples_per_shard = args\n",
    "    \n",
    "    shard_path = output_pattern % chunk_index\n",
    "    \n",
    "    with wds.TarWriter(shard_path) as sink:\n",
    "        for spot_id in spot_ids_chunk:\n",
    "            # 使用 all_spot_info 来获取路径，避免访问全局变量\n",
    "            paths = all_spot_info.get(spot_id)\n",
    "            if not paths:\n",
    "                continue\n",
    "\n",
    "            img_path = paths['image_path']\n",
    "            sentence_path = paths['sentence_path']\n",
    "\n",
    "            if not (os.path.exists(img_path) and os.path.exists(sentence_path)):\n",
    "                continue\n",
    "\n",
    "            with open(img_path, \"rb\") as f_img:\n",
    "                image_data = f_img.read()\n",
    "            with open(sentence_path, \"rb\") as f_txt:\n",
    "                sentence_data = f_txt.read()\n",
    "            \n",
    "            sample = {\"__key__\": spot_id, \"png\": image_data, \"txt\": sentence_data}\n",
    "            sink.write(sample)\n",
    "    return chunk_index # 返回一个结果，方便tqdm跟踪进度\n",
    "\n",
    "def create_webdataset_shards(adata: sc.AnnData, output_pattern: str):\n",
    "    \"\"\"\n",
    "    将所有独立的 .png 和 .txt 文件打包成 WebDataset 的 .tar 分片。\n",
    "    【已修改】: 现在调用顶层的 _process_chunk_worker 函数。\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 步骤 2: 将原始数据打包成 WebDataset 分片 (.tar) ---\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    if 'image_path' not in adata.obs.columns or 'sentence_path' not in adata.obs.columns:\n",
    "        print(\"警告: AnnData中缺少 'image_path' 或 'sentence_path'。正在尝试重新构建...\")\n",
    "        # (这部分逻辑与之前相同，用于构建路径)\n",
    "        paths_df = pd.DataFrame(index=adata.obs_names)\n",
    "        paths_df[['image_path', 'sentence_path']] = [\n",
    "            (\n",
    "                os.path.join(IMAGE_BASE_DIR, f\"{sid.split('_')[0]}_tiles\", f\"{sid}.png\"),\n",
    "                os.path.join(SENTENCE_BASE_DIR, f\"{sid.split('_')[0]}_sentences_hvg\", f\"{sid}.txt\")\n",
    "            ) for sid in adata.obs_names\n",
    "        ]\n",
    "        adata.obs = adata.obs.join(paths_df)\n",
    "\n",
    "    all_spot_info = adata.obs[['image_path', 'sentence_path']].dropna().to_dict('index')\n",
    "    all_spot_ids = list(all_spot_info.keys())\n",
    "    \n",
    "    total_files = len(all_spot_ids)\n",
    "    print(f\"准备打包 {total_files} 个 spots 的数据...\")\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_pattern), exist_ok=True)\n",
    "\n",
    "    # 将所有spot_ids分块\n",
    "    spot_id_chunks = [all_spot_ids[i:i + SAMPLES_PER_SHARD] for i in range(0, len(all_spot_ids), SAMPLES_PER_SHARD)]\n",
    "    \n",
    "    # 【【【关键修改点】】】\n",
    "    # 为每个任务准备一个包含所有必要参数的元组\n",
    "    tasks = [\n",
    "        (chunk, i, all_spot_info, output_pattern, SAMPLES_PER_SHARD) \n",
    "        for i, chunk in enumerate(spot_id_chunks)\n",
    "    ]\n",
    "    \n",
    "    # 使用多进程并行处理\n",
    "    print(f\"启动 {NUM_WORKERS_PACKAGING} 个工作进程进行打包...\")\n",
    "    with ProcessPoolExecutor(max_workers=NUM_WORKERS_PACKAGING) as executor:\n",
    "        # 使用tqdm来显示处理进度\n",
    "        results = list(tqdm(executor.map(_process_chunk_worker, tasks), total=len(tasks), desc=\"打包 .tar 分片\"))\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"✅ 数据打包完成！成功处理了 {len(results)} 个分片。\")\n",
    "    print(f\"🕒 耗时: {format_time(end_time - start_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dacc027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 加载成功！AnnData 维度: (997054, 30148)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "master_adata = sc.read_h5ad(EXISTING_CACHE_PATH)\n",
    "print(f\"✅ 加载成功！AnnData 维度: {master_adata.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42d1e921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 步骤 1: 计算并添加空间邻接图 (无Squidpy替代方案) ---\n",
      "检测到 505 个独立样本。将逐个计算空间图...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516dff4c48a249c68c9c91b2e0e38896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "处理每个样本:   0%|          | 0/505 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 所有样本的空间邻接图已计算并合并！\n",
      "🕒 耗时: 2分 41.06秒\n"
     ]
    }
   ],
   "source": [
    "# 使用新的、无squidpy依赖的函数来添加空间图\n",
    "master_adata_with_graph = add_spatial_graph_to_adata_no_squidpy(master_adata, n_neighbors=N_NEIGHBORS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7f2766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "正在保存带有图结构的最终主 AnnData 文件至: /cwStorage/nodecw_group/jijh/spaglam_sota_data/master_adata_with_graph.h5ad\n",
      "✅ 保存成功！\n",
      "\n",
      "--- 步骤 2: 将原始数据打包成 WebDataset 分片 (.tar) ---\n",
      "正在重建文件路径...\n",
      "准备打包 997054 个 spots 的数据...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47a0c6779b444249278f626ba25a113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "打包 .tar 分片:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't get local object 'create_webdataset_shards.<locals>.process_chunk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_RemoteTraceback\u001b[39m                          Traceback (most recent call last)",
      "\u001b[31m_RemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/multiprocessing/queues.py\", line 262, in _feed\n    obj = _ForkingPickler.dumps(obj)\n  File \"/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/multiprocessing/reduction.py\", line 51, in dumps\n    cls(buf, protocol).dump(obj)\n    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\nAttributeError: Can't get local object 'create_webdataset_shards.<locals>.process_chunk'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ 保存成功！\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m shard_pattern = os.path.join(SHARDS_OUTPUT_PATH, \u001b[33m\"\u001b[39m\u001b[33mdataset-\u001b[39m\u001b[38;5;132;01m%06d\u001b[39;00m\u001b[33m.tar\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mcreate_webdataset_shards\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaster_adata_with_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_pattern\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🎉🎉🎉 恭喜！SOTA 数据预处理全部完成！🎉🎉🎉\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m您现在拥有：\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 176\u001b[39m, in \u001b[36mcreate_webdataset_shards\u001b[39m\u001b[34m(adata, output_pattern)\u001b[39m\n\u001b[32m    173\u001b[39m spot_id_chunks = [all_spot_ids[i:i + SAMPLES_PER_SHARD] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(all_spot_ids), SAMPLES_PER_SHARD)]\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ProcessPoolExecutor(max_workers=NUM_WORKERS_PACKAGING) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspot_id_chunks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mspot_id_chunks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m打包 .tar 分片\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m end_time = time.time()\n\u001b[32m    179\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ 数据打包完成！生成了 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(spot_id_chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m 个 .tar 文件。\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/site-packages/tqdm/notebook.py:250\u001b[39m, in \u001b[36mtqdm_notebook.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    249\u001b[39m     it = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__iter__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/concurrent/futures/process.py:617\u001b[39m, in \u001b[36m_chain_from_iterable_of_lists\u001b[39m\u001b[34m(iterable)\u001b[39m\n\u001b[32m    611\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[32m    612\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    613\u001b[39m \u001b[33;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[32m    614\u001b[39m \u001b[33;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[32m    615\u001b[39m \u001b[33;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[32m    616\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m617\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m        \u001b[49m\u001b[43melement\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwhile\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/concurrent/futures/_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/concurrent/futures/_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/multiprocessing/queues.py:262\u001b[39m, in \u001b[36mQueue._feed\u001b[39m\u001b[34m(buffer, notempty, send_bytes, writelock, reader_close, writer_close, ignore_epipe, onerror, queue_sem)\u001b[39m\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[38;5;66;03m# serialize the data before acquiring the lock\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m obj = \u001b[43m_ForkingPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wacquire \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    264\u001b[39m     send_bytes(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/multiprocessing/reduction.py:51\u001b[39m, in \u001b[36mForkingPickler.dumps\u001b[39m\u001b[34m(cls, obj, protocol)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdumps\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, protocol=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     50\u001b[39m     buf = io.BytesIO()\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m buf.getbuffer()\n",
      "\u001b[31mAttributeError\u001b[39m: Can't get local object 'create_webdataset_shards.<locals>.process_chunk'"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"\\n正在保存带有图结构的最终主 AnnData 文件至: {FINAL_ADATA_PATH}\")\n",
    "os.makedirs(os.path.dirname(FINAL_ADATA_PATH), exist_ok=True)\n",
    "master_adata_with_graph.write_h5ad(FINAL_ADATA_PATH)\n",
    "print(\"✅ 保存成功！\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50d3c901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 步骤 2: 将原始数据打包成 WebDataset 分片 (.tar) ---\n",
      "准备打包 997054 个 spots 的数据...\n",
      "启动 16 个工作进程进行打包...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9487328b5b042be9010c88453eaa01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "打包 .tar 分片:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 数据打包完成！成功处理了 100 个分片。\n",
      "🕒 耗时: 2分 33.69秒\n",
      "\n",
      "\n",
      "🎉🎉🎉 恭喜！SOTA 数据预处理全部完成！🎉🎉🎉\n",
      "您现在拥有：\n",
      "1. 一个包含所有元数据和空间图的中央索引文件: /cwStorage/nodecw_group/jijh/spaglam_sota_data/master_adata_with_graph.h5ad\n",
      "2. 一组用于高效训练的数据仓库分片文件位于: /cwStorage/nodecw_group/jijh/spaglam_sota_data/webdataset_shards\n"
     ]
    }
   ],
   "source": [
    "shard_pattern = os.path.join(SHARDS_OUTPUT_PATH, \"dataset-%06d.tar\")\n",
    "create_webdataset_shards(master_adata_with_graph, shard_pattern)\n",
    "\n",
    "print(\"\\n\\n🎉🎉🎉 恭喜！SOTA 数据预处理全部完成！🎉🎉🎉\")\n",
    "print(\"您现在拥有：\")\n",
    "print(f\"1. 一个包含所有元数据和空间图的中央索引文件: {FINAL_ADATA_PATH}\")\n",
    "print(f\"2. 一组用于高效训练的数据仓库分片文件位于: {SHARDS_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d810f85d",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e219ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  1. 文件存在性检查\n",
      "================================================================================\n",
      "✅ [成功] 主 AnnData 文件存在: /cwStorage/nodecw_group/jijh/spaglam_sota_data/master_adata_with_graph.h5ad\n",
      "✅ [成功] WebDataset 分片目录存在，并找到 100 个 .tar 文件。\n",
      "\n",
      "================================================================================\n",
      "  2. 主 ANNDATA 文件完整性检查\n",
      "================================================================================\n",
      "✅ [成功] 成功加载 AnnData 文件。\n",
      "ℹ️ [信息] AnnData 维度 (spots, genes): 997054 x 30148\n",
      "❌ [失败] '.obs' 中缺少关键列: 'image_path'\n",
      "\n",
      "❌ 主 AnnData 文件存在问题，验证终止。\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# ==============================================================================\n",
    "# SpaGLaM SOTA 数据集验证脚本\n",
    "# \n",
    "# 用途:\n",
    "#   - 验证预处理流程是否成功。\n",
    "#   - 检查主 AnnData 文件和 WebDataset 分片的完整性与一致性。\n",
    "#   - 在开始昂贵的模型训练前，进行一次快速的健全性检查 (Sanity Check)。\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import scanpy as sc\n",
    "import webdataset as wds\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# --- 1. 配置区域 (请根据您的环境修改) ---\n",
    "\n",
    "# --- 路径配置 ---\n",
    "BASE_DIR = \"/cwStorage/nodecw_group/jijh\"\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"spaglam_sota_data\")\n",
    "FINAL_ADATA_PATH = os.path.join(OUTPUT_DIR, \"master_adata_with_graph.h5ad\")\n",
    "SHARDS_OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"webdataset_shards\")\n",
    "\n",
    "# --- 验证参数 ---\n",
    "# 随机抽样检查的spot数量\n",
    "NUM_SAMPLES_TO_CHECK = 5\n",
    "\n",
    "# --- 辅助函数 ---\n",
    "def print_header(title):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"  {title.upper()}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "def print_status(message, success=True):\n",
    "    prefix = \"✅ [成功]\" if success else \"❌ [失败]\"\n",
    "    print(f\"{prefix} {message}\")\n",
    "\n",
    "def print_info(message):\n",
    "    print(f\"ℹ️ [信息] {message}\")\n",
    "\n",
    "\n",
    "# --- 验证函数 ---\n",
    "\n",
    "def check_file_existence():\n",
    "    \"\"\"检查所有关键文件和目录是否存在\"\"\"\n",
    "    print_header(\"1. 文件存在性检查\")\n",
    "    all_exist = True\n",
    "    \n",
    "    # 检查主 AnnData 文件\n",
    "    if os.path.exists(FINAL_ADATA_PATH):\n",
    "        print_status(f\"主 AnnData 文件存在: {FINAL_ADATA_PATH}\")\n",
    "    else:\n",
    "        print_status(f\"主 AnnData 文件缺失: {FINAL_ADATA_PATH}\", success=False)\n",
    "        all_exist = False\n",
    "\n",
    "    # 检查 WebDataset 分片目录\n",
    "    if os.path.isdir(SHARDS_OUTPUT_PATH):\n",
    "        shards = [f for f in os.listdir(SHARDS_OUTPUT_PATH) if f.endswith('.tar')]\n",
    "        if shards:\n",
    "            print_status(f\"WebDataset 分片目录存在，并找到 {len(shards)} 个 .tar 文件。\")\n",
    "            # 随机选择一个分片路径用于后续检查\n",
    "            random_shard_path = os.path.join(SHARDS_OUTPUT_PATH, random.choice(shards))\n",
    "        else:\n",
    "            print_status(\"WebDataset 分片目录存在，但其中没有 .tar 文件。\", success=False)\n",
    "            all_exist = False\n",
    "            random_shard_path = None\n",
    "    else:\n",
    "        print_status(f\"WebDataset 分片目录缺失: {SHARDS_OUTPUT_PATH}\", success=False)\n",
    "        all_exist = False\n",
    "        random_shard_path = None\n",
    "        \n",
    "    return all_exist, random_shard_path\n",
    "\n",
    "\n",
    "def inspect_master_adata():\n",
    "    \"\"\"检查主 AnnData 文件的内部结构\"\"\"\n",
    "    print_header(\"2. 主 AnnData 文件完整性检查\")\n",
    "    try:\n",
    "        adata = sc.read_h5ad(FINAL_ADATA_PATH)\n",
    "        print_status(f\"成功加载 AnnData 文件。\")\n",
    "        print_info(f\"AnnData 维度 (spots, genes): {adata.n_obs} x {adata.n_vars}\")\n",
    "        \n",
    "        # 检查关键列\n",
    "        required_obs = ['sample_id', 'image_path', 'sentence_path']\n",
    "        for col in required_obs:\n",
    "            if col not in adata.obs.columns:\n",
    "                print_status(f\"'.obs' 中缺少关键列: '{col}'\", success=False)\n",
    "                return None\n",
    "        print_status(\"'.obs' 中的关键列均存在。\")\n",
    "        \n",
    "        # 检查空间坐标\n",
    "        if 'spatial' not in adata.obsm:\n",
    "            print_status(\"'.obsm' 中缺少空间坐标 'spatial'\", success=False)\n",
    "            return None\n",
    "        print_status(\"空间坐标 '.obsm['spatial']' 存在。\")\n",
    "\n",
    "        # 检查空间邻接图 (最重要)\n",
    "        if 'spatial_connectivities' not in adata.obsp:\n",
    "            print_status(\"'.obsp' 中缺少空间邻接图 'spatial_connectivities'\", success=False)\n",
    "            return None\n",
    "        print_status(\"空间邻接图 '.obsp['spatial_connectivities']' 存在。\")\n",
    "        \n",
    "        return adata\n",
    "    except Exception as e:\n",
    "        print_status(f\"加载或检查 AnnData 文件时出错: {e}\", success=False)\n",
    "        return None\n",
    "\n",
    "\n",
    "def inspect_tar_shard(shard_path):\n",
    "    \"\"\"检查单个 .tar 分片的内部结构\"\"\"\n",
    "    print_header(\"3. WebDataset 分片结构检查\")\n",
    "    if not shard_path:\n",
    "        print_status(\"没有可供检查的分片文件。\", success=False)\n",
    "        return False\n",
    "        \n",
    "    try:\n",
    "        print_info(f\"正在抽样检查分片: {os.path.basename(shard_path)}\")\n",
    "        dataset = wds.WebDataset(shard_path)\n",
    "        \n",
    "        sample_count = 0\n",
    "        for i, sample in enumerate(dataset):\n",
    "            if i >= 3: break  # 只检查前3个样本\n",
    "            \n",
    "            # 检查关键键\n",
    "            expected_keys = {'__key__', 'png', 'txt'}\n",
    "            if not expected_keys.issubset(sample.keys()):\n",
    "                print_status(f\"样本 {sample['__key__']} 缺少关键键。期望: {expected_keys}, 实际: {sample.keys()}\", success=False)\n",
    "                return False\n",
    "            \n",
    "            # 尝试解码\n",
    "            Image.open(io.BytesIO(sample['png'])).convert(\"RGB\")\n",
    "            sample['txt'].decode('utf-8')\n",
    "            sample_count += 1\n",
    "\n",
    "        if sample_count > 0:\n",
    "            print_status(f\"成功检查了 {sample_count} 个样本，结构正确。\")\n",
    "            return True\n",
    "        else:\n",
    "            print_status(\"分片文件为空或无法读取样本。\", success=False)\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print_status(f\"检查 .tar 分片时出错: {e}\", success=False)\n",
    "        return False\n",
    "\n",
    "\n",
    "def perform_end_to_end_check(adata, tar_urls):\n",
    "    \"\"\"对随机样本进行端到端一致性检查\"\"\"\n",
    "    print_header(\"4. 端到端一致性检查\")\n",
    "    \n",
    "    try:\n",
    "        spot_ids_to_check = random.sample(adata.obs_names.tolist(), k=NUM_SAMPLES_TO_CHECK)\n",
    "        print_info(f\"将随机抽样检查以下 {NUM_SAMPLES_TO_CHECK} 个 spots: {spot_ids_to_check}\")\n",
    "\n",
    "        # 使用WebDataset查找并加载这些样本\n",
    "        dataset = wds.WebDataset(tar_urls).select(lambda x: x['__key__'] in spot_ids_to_check)\n",
    "        \n",
    "        found_count = 0\n",
    "        for sample in tqdm(dataset, total=len(spot_ids_to_check), desc=\"端到端检查\"):\n",
    "            spot_id = sample['__key__']\n",
    "            print(f\"\\n--- 正在检查 Spot: {spot_id} ---\")\n",
    "            \n",
    "            # 1. 验证 AnnData 中的邻居信息\n",
    "            try:\n",
    "                idx = adata.obs_names.get_loc(spot_id)\n",
    "                neighbors_indices = adata.obsp['spatial_connectivities'][idx].indices\n",
    "                neighbor_ids = adata.obs_names[neighbors_indices].tolist()\n",
    "                print_status(f\"AnnData: 找到 {len(neighbor_ids)} 个邻居。示例: {neighbor_ids[:3]}\")\n",
    "            except Exception as e:\n",
    "                print_status(f\"AnnData: 查找邻居失败: {e}\", success=False)\n",
    "                return False\n",
    "\n",
    "            # 2. 验证 WebDataset 中的数据\n",
    "            try:\n",
    "                img = Image.open(io.BytesIO(sample['png']))\n",
    "                sentence = sample['txt'].decode('utf-8')\n",
    "                print_status(\"WebDataset: 图像和文本数据成功解码。\")\n",
    "                print_info(f\"图像尺寸: {img.size}, 句子预览: '{sentence[:50]}...'\")\n",
    "            except Exception as e:\n",
    "                print_status(f\"WebDataset: 解码数据失败: {e}\", success=False)\n",
    "                return False\n",
    "            \n",
    "            found_count += 1\n",
    "        \n",
    "        if found_count == len(spot_ids_to_check):\n",
    "            print_status(f\"\\n所有 {found_count} 个抽样检查的 spot 均通过了端到端一致性验证！\")\n",
    "            return True\n",
    "        else:\n",
    "            print_status(f\"只找到了 {found_count}/{len(spot_ids_to_check)} 个抽样样本。数据可能不完整或key不匹配。\", success=False)\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print_status(f\"端到端检查过程中发生严重错误: {e}\", success=False)\n",
    "        return False\n",
    "\n",
    "\n",
    "# --- 主执行流程 ---\n",
    "def main():\n",
    "    \"\"\"主验证函数\"\"\"\n",
    "    files_ok, random_shard = check_file_existence()\n",
    "    if not files_ok:\n",
    "        print(\"\\n❌ 基础文件缺失，验证终止。请先成功运行预处理脚本。\")\n",
    "        return\n",
    "\n",
    "    adata = inspect_master_adata()\n",
    "    if adata is None:\n",
    "        print(\"\\n❌ 主 AnnData 文件存在问题，验证终止。\")\n",
    "        return\n",
    "        \n",
    "    tar_ok = inspect_tar_shard(random_shard)\n",
    "    if not tar_ok:\n",
    "        print(\"\\n❌ WebDataset 分片文件存在问题，验证终止。\")\n",
    "        return\n",
    "    \n",
    "    # 获取所有分片的URL列表\n",
    "    all_shards = [os.path.join(SHARDS_OUTPUT_PATH, f) for f in os.listdir(SHARDS_OUTPUT_PATH) if f.endswith('.tar')]\n",
    "    \n",
    "    e2e_ok = perform_end_to_end_check(adata, all_shards)\n",
    "    if not e2e_ok:\n",
    "        print(\"\\n❌ 端到端一致性检查失败，请检查数据生成逻辑。\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\\n\" + \"*\"*25 + \"  数据集验证通过  \" + \"*\"*25)\n",
    "    print(\"✅ 您的数据集已准备就绪，可以用于SpaGLaM模型训练！\")\n",
    "    print(\"*\"*80)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2ec6aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "in_tissue",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "pxl_col_in_fullres",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pxl_row_in_fullres",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "array_col",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "array_row",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "n_counts",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "n_genes_by_counts",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "log1p_n_genes_by_counts",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_counts",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "log1p_total_counts",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pct_counts_in_top_50_genes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pct_counts_in_top_100_genes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pct_counts_in_top_200_genes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pct_counts_in_top_500_genes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_counts_mito",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "log1p_total_counts_mito",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pct_counts_mito",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sample_id",
         "rawType": "category",
         "type": "unknown"
        },
        {
         "name": "pxl_row_in_fullres_old",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pxl_col_in_fullres_old",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_counts_mt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pct_counts_mt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "n_genes",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentence_path",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "f7367c2c-6570-4903-92bc-1f5eb40bb834",
       "rows": [
        [
         "TENX158_000x017",
         "True",
         "7727.328900500986",
         "1043.8628877020124",
         "17.0",
         "0.0",
         "9334.0",
         "980",
         "6.90875477931522",
         "9056.0",
         "9.141526055975822",
         "57.874437540175705",
         "69.305763874009",
         "81.24062566959502",
         "93.59331476323119",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "980",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_000x017.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_000x017.txt"
        ],
        [
         "TENX158_000x018",
         "True",
         "8092.620791887509",
         "1043.8628877020124",
         "18.0",
         "0.0",
         "2166.0",
         "447",
         "6.12029741895095",
         "2079.0",
         "7.681099001536359",
         "67.91320406278855",
         "77.37765466297323",
         "87.6269621421976",
         "100.0",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "447",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_000x018.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_000x018.txt"
        ],
        [
         "TENX158_000x020",
         "True",
         "8823.204574660553",
         "1043.8628877020124",
         "20.0",
         "0.0",
         "5275.0",
         "866",
         "6.778784897685177",
         "5081.0",
         "8.570923513837204",
         "56.51184834123223",
         "67.82938388625593",
         "78.93838862559241",
         "92.83412322274881",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "866",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_000x020.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_000x020.txt"
        ],
        [
         "TENX158_000x021",
         "True",
         "9188.496466047076",
         "1043.8628877020124",
         "21.0",
         "0.0",
         "5838.0",
         "1268",
         "7.163946684342547",
         "5674.0",
         "8.672314828283538",
         "44.43302500856458",
         "54.727646454265155",
         "66.18705035971223",
         "83.07639602603632",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "1268",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_000x021.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_000x021.txt"
        ],
        [
         "TENX158_001x016",
         "True",
         "7362.037009114462",
         "1409.154779088535",
         "16.0",
         "1.0",
         "24396.0",
         "1840",
         "7.554334823725747",
         "23763.0",
         "10.102215452902549",
         "46.49942613543204",
         "57.48073454664699",
         "69.58927693064437",
         "85.64928676832267",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "1840",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_001x016.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_001x016.txt"
        ],
        [
         "TENX158_001x017",
         "True",
         "7727.328900500986",
         "1409.154779088535",
         "17.0",
         "1.0",
         "56326.0",
         "2673",
         "7.943072717277933",
         "54944.0",
         "10.938929272881317",
         "35.867982814330865",
         "45.199375066576714",
         "57.190285125874375",
         "76.05723822036005",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "2673",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_001x017.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_001x017.txt"
        ],
        [
         "TENX158_001x018",
         "True",
         "8092.620791887509",
         "1409.154779088535",
         "18.0",
         "1.0",
         "50483.0",
         "2438",
         "7.835974581721565",
         "49211.0",
         "10.829411733378521",
         "38.56743854366817",
         "48.19642255808886",
         "60.20244438721946",
         "78.42442010181645",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "2438",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_001x018.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_001x018.txt"
        ],
        [
         "TENX158_001x019",
         "True",
         "8457.91268327403",
         "1409.154779088535",
         "19.0",
         "1.0",
         "42604.0",
         "2354",
         "7.792348924113037",
         "41522.0",
         "10.659726896262056",
         "39.40240353018496",
         "49.234813632522766",
         "61.52473946108346",
         "79.49253591212093",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "2354",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_001x019.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_001x019.txt"
        ],
        [
         "TENX158_001x020",
         "True",
         "8823.204574660553",
         "1409.154779088535",
         "20.0",
         "1.0",
         "64559.0",
         "2771",
         "7.957877358489813",
         "62952.0",
         "11.07535030294383",
         "34.506420483588656",
         "43.688718846326616",
         "55.80321876113322",
         "74.61856596291764",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "2771",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_001x020.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_001x020.txt"
        ],
        [
         "TENX158_001x021",
         "True",
         "9188.496466047076",
         "1409.154779088535",
         "21.0",
         "1.0",
         "26516.0",
         "2444",
         "7.8168199657645525",
         "25927.0",
         "10.185541315729058",
         "30.566450445014333",
         "40.13425856086891",
         "52.05536279981898",
         "72.21300346960325",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "2444",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_001x021.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_001x021.txt"
        ],
        [
         "TENX158_002x012",
         "True",
         "5900.869443568371",
         "1774.4466704750578",
         "12.0",
         "2.0",
         "15813.0",
         "1626",
         "7.411556287811163",
         "15376.0",
         "9.668650902630873",
         "47.73920192246885",
         "58.198950230822746",
         "69.94877632327831",
         "85.54354012521344",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "1626",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_002x012.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_002x012.txt"
        ],
        [
         "TENX158_002x013",
         "True",
         "6266.161334954894",
         "1774.4466704750578",
         "13.0",
         "2.0",
         "30949.0",
         "2292",
         "7.757478766584179",
         "30104.0",
         "10.340128278112742",
         "38.63775889366377",
         "47.72367443213028",
         "58.993828556657725",
         "76.99117903647938",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "2292",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_002x013.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_002x013.txt"
        ],
        [
         "TENX158_002x014",
         "True",
         "6631.453226341418",
         "1774.4466704750578",
         "14.0",
         "2.0",
         "33356.0",
         "2288",
         "7.75833346749091",
         "32500.0",
         "10.415022924371359",
         "38.194028060918576",
         "47.58064516129033",
         "59.65043770236239",
         "77.81808370308191",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "2288",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_002x014.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_002x014.txt"
        ],
        [
         "TENX158_002x015",
         "True",
         "6996.745117727941",
         "1774.4466704750578",
         "15.0",
         "2.0",
         "40692.0",
         "2445",
         "7.827240901752811",
         "39682.0",
         "10.61381136646669",
         "36.1102919492775",
         "45.54703627248599",
         "57.43880861102919",
         "76.2090828664111",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "2445",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_002x015.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_002x015.txt"
        ],
        [
         "TENX158_002x016",
         "True",
         "7362.037009114462",
         "1774.4466704750578",
         "16.0",
         "2.0",
         "58904.0",
         "2833",
         "7.978310969867721",
         "57460.0",
         "10.983681255680219",
         "30.78398750509303",
         "40.25872606274616",
         "51.92177101724841",
         "70.87973652044003",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "2833",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_002x016.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_002x016.txt"
        ],
        [
         "TENX158_002x017",
         "True",
         "7727.328900500986",
         "1774.4466704750578",
         "17.0",
         "2.0",
         "76181.0",
         "3193",
         "8.110427237575024",
         "74824.0",
         "11.240880493297846",
         "24.49560914138696",
         "33.26551239810451",
         "44.499284598521946",
         "63.29399719090062",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "3193",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_002x017.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_002x017.txt"
        ],
        [
         "TENX158_002x018",
         "True",
         "8092.620791887509",
         "1774.4466704750578",
         "18.0",
         "2.0",
         "69990.0",
         "3201",
         "8.106514516255185",
         "68756.0",
         "11.15612194133691",
         "25.61651664523503",
         "34.42777539648521",
         "45.3493356193742",
         "64.08201171595942",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "3201",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_002x018.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_002x018.txt"
        ],
        [
         "TENX158_002x019",
         "True",
         "8457.91268327403",
         "1774.4466704750578",
         "19.0",
         "2.0",
         "64028.0",
         "2967",
         "8.022568946988255",
         "62775.0",
         "11.067091384711677",
         "26.971012681951645",
         "36.166989442119075",
         "47.454238770537884",
         "66.71456237895922",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "2967",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_002x019.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_002x019.txt"
        ],
        [
         "TENX158_002x020",
         "True",
         "8823.204574660553",
         "1774.4466704750578",
         "20.0",
         "2.0",
         "73442.0",
         "3360",
         "8.15908865466791",
         "72182.0",
         "11.20426487413325",
         "22.522534789357586",
         "30.963209062934016",
         "42.026360937882956",
         "60.61926418125868",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "3360",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_002x020.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_002x020.txt"
        ],
        [
         "TENX158_002x021",
         "True",
         "9188.496466047076",
         "1774.4466704750578",
         "21.0",
         "2.0",
         "36221.0",
         "2837",
         "7.965545573129991",
         "35579.0",
         "10.497421948024481",
         "26.404571933408793",
         "35.28615996245272",
         "46.76568841279921",
         "65.72982523950195",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "2837",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_002x021.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_002x021.txt"
        ],
        [
         "TENX158_003x009",
         "True",
         "4804.993769408804",
         "2139.7385618615804",
         "9.0",
         "3.0",
         "15860.0",
         "1481",
         "7.320526962272739",
         "15386.0",
         "9.67161854490356",
         "51.7906683480454",
         "62.036569987389655",
         "73.45523329129887",
         "88.36696090794452",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "1481",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_003x009.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_003x009.txt"
        ],
        [
         "TENX158_003x010",
         "True",
         "5170.285660795326",
         "2139.7385618615804",
         "10.0",
         "3.0",
         "26512.0",
         "1986",
         "7.620214770574455",
         "25808.0",
         "10.185390457723868",
         "44.48928786964393",
         "54.85063367531684",
         "66.50573325286663",
         "82.96997585998793",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "1986",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_003x010.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_003x010.txt"
        ],
        [
         "TENX158_003x011",
         "True",
         "5535.57755218185",
         "2139.7385618615804",
         "11.0",
         "3.0",
         "46400.0",
         "2528",
         "7.862497197230544",
         "45239.0",
         "10.74507628970625",
         "36.53879310344828",
         "46.07112068965517",
         "57.51077586206896",
         "75.76508620689654",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "2528",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_003x011.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_003x011.txt"
        ],
        [
         "TENX158_003x012",
         "True",
         "5900.869443568371",
         "2139.7385618615804",
         "12.0",
         "3.0",
         "58312.0",
         "2849",
         "7.979338895262327",
         "57156.0",
         "10.973580332043829",
         "29.609685827959943",
         "38.522088077925645",
         "50.25037728083414",
         "69.05611194951297",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "2849",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_003x012.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_003x012.txt"
        ],
        [
         "TENX158_003x013",
         "True",
         "6266.161334954894",
         "2139.7385618615804",
         "13.0",
         "3.0",
         "69813.0",
         "3433",
         "8.183397369998433",
         "68743.0",
         "11.153589841704278",
         "20.844255367911423",
         "29.156460831077307",
         "39.97249795883288",
         "58.63807600303669",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "3433",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_003x013.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_003x013.txt"
        ],
        [
         "TENX158_003x014",
         "True",
         "6631.453226341418",
         "2139.7385618615804",
         "14.0",
         "3.0",
         "60717.0",
         "3136",
         "8.078067881815437",
         "59606.0",
         "11.013995473450624",
         "25.597443878979526",
         "34.21117644152379",
         "45.09610158604674",
         "63.60821516214569",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "3136",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_003x014.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_003x014.txt"
        ],
        [
         "TENX158_003x015",
         "True",
         "6996.745117727941",
         "2139.7385618615804",
         "15.0",
         "3.0",
         "61325.0",
         "3007",
         "8.038189179973202",
         "60227.0",
         "11.023959175557131",
         "25.78230737871994",
         "34.793314309009375",
         "45.992662046473704",
         "64.94741133306155",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "3007",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_003x015.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_003x015.txt"
        ],
        [
         "TENX158_003x016",
         "True",
         "7362.037009114462",
         "2139.7385618615804",
         "16.0",
         "3.0",
         "77150.0",
         "3424",
         "8.187855443695623",
         "75906.0",
         "11.253519819470103",
         "21.89889825016202",
         "30.264419961114715",
         "41.018794556059625",
         "59.718729747245625",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "3424",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_003x016.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_003x016.txt"
        ],
        [
         "TENX158_003x017",
         "True",
         "7727.328900500986",
         "2139.7385618615804",
         "17.0",
         "3.0",
         "60022.0",
         "3541",
         "8.21932609390609",
         "59207.0",
         "11.002483101084119",
         "17.905101462796974",
         "25.872180200593114",
         "36.52160874346073",
         "55.36636566592249",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "3541",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_003x017.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_003x017.txt"
        ],
        [
         "TENX158_003x018",
         "True",
         "8092.620791887509",
         "2139.7385618615804",
         "18.0",
         "3.0",
         "62236.0",
         "3514",
         "8.204398418149381",
         "61349.0",
         "11.038704957176451",
         "18.97294170576515",
         "27.157915033099812",
         "37.73700109261521",
         "56.75171926216338",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "3514",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_003x018.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_003x018.txt"
        ],
        [
         "TENX158_003x019",
         "True",
         "8457.91268327403",
         "2139.7385618615804",
         "19.0",
         "3.0",
         "74779.0",
         "3329",
         "8.161660452056282",
         "73703.0",
         "11.222305748531104",
         "21.11555383195817",
         "29.54572807873868",
         "40.53410716912502",
         "59.93795049412268",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "3329",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_003x019.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_003x019.txt"
        ],
        [
         "TENX158_003x020",
         "True",
         "8823.204574660553",
         "2139.7385618615804",
         "20.0",
         "3.0",
         "55981.0",
         "3642",
         "8.24117615049496",
         "55319.0",
         "10.932785489476622",
         "17.27371786856255",
         "24.60656294099784",
         "35.186938425537235",
         "53.786106000250086",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "3642",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_003x020.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_003x020.txt"
        ],
        [
         "TENX158_003x021",
         "True",
         "9188.496466047076",
         "2139.7385618615804",
         "21.0",
         "3.0",
         "42760.0",
         "2959",
         "8.03073492409854",
         "42075.0",
         "10.6633817512079",
         "21.211412535079514",
         "29.93217960710945",
         "41.37277829747428",
         "61.19971936389149",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "2959",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_003x021.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_003x021.txt"
        ],
        [
         "TENX158_004x008",
         "True",
         "4439.701878022282",
         "2505.0304532481027",
         "8.0",
         "4.0",
         "20992.0",
         "1853",
         "7.549609165154531",
         "20508.0",
         "9.951944327804322",
         "46.12233231707317",
         "56.3548018292683",
         "68.13548018292683",
         "83.71284298780488",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "1853",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_004x008.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_004x008.txt"
        ],
        [
         "TENX158_004x009",
         "True",
         "4804.993769408804",
         "2505.0304532481027",
         "9.0",
         "4.0",
         "55581.0",
         "2816",
         "7.977625098784593",
         "54460.0",
         "10.925614686816047",
         "29.90770227235926",
         "38.71826703369857",
         "50.409312534859026",
         "69.87819578632987",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "2816",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_004x009.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_004x009.txt"
        ],
        [
         "TENX158_004x010",
         "True",
         "5170.285660795326",
         "2505.0304532481027",
         "10.0",
         "4.0",
         "55286.0",
         "2862",
         "7.9892214088152755",
         "54293.0",
         "10.920293078500146",
         "28.368845639040625",
         "37.53391455341316",
         "49.178815613356",
         "67.86166479759794",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "2862",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_004x010.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_004x010.txt"
        ],
        [
         "TENX158_004x011",
         "True",
         "5535.57755218185",
         "2505.0304532481027",
         "11.0",
         "4.0",
         "51952.0",
         "2883",
         "7.989560449333865",
         "51063.0",
         "10.858094742694487",
         "26.584154604250077",
         "35.36341238065907",
         "46.72774869109947",
         "65.2544656606098",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "2883",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_004x011.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_004x011.txt"
        ],
        [
         "TENX158_004x012",
         "True",
         "5900.869443568371",
         "2505.0304532481027",
         "12.0",
         "4.0",
         "65513.0",
         "3493",
         "8.193953023563742",
         "64730.0",
         "11.090019139242122",
         "19.270984384778593",
         "27.548730786256165",
         "38.077938729717765",
         "57.10927602155298",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "3493",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_004x012.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_004x012.txt"
        ],
        [
         "TENX158_004x013",
         "True",
         "6266.161334954894",
         "2505.0304532481027",
         "13.0",
         "4.0",
         "46305.0",
         "3415",
         "8.17131687471973",
         "45708.0",
         "10.743026821311144",
         "17.589893100097182",
         "25.455134434726272",
         "35.98315516682863",
         "55.51668286362164",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "3415",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_004x013.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_004x013.txt"
        ],
        [
         "TENX158_004x014",
         "True",
         "6631.453226341418",
         "2505.0304532481027",
         "14.0",
         "4.0",
         "56063.0",
         "3519",
         "8.203577736937952",
         "55370.0",
         "10.934249174296063",
         "17.237750387956407",
         "25.27156948433013",
         "35.85252305442092",
         "54.95781531491358",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "3519",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_004x014.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_004x014.txt"
        ],
        [
         "TENX158_004x015",
         "True",
         "6996.745117727941",
         "2505.0304532481027",
         "15.0",
         "4.0",
         "63570.0",
         "3479",
         "8.203304026795282",
         "62727.0",
         "11.059912670497305",
         "18.21142048135913",
         "26.26238791882964",
         "37.085103036023284",
         "56.31901840490797",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "3479",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_004x015.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_004x015.txt"
        ],
        [
         "TENX158_004x016",
         "True",
         "7362.037009114462",
         "2505.0304532481027",
         "16.0",
         "4.0",
         "51704.0",
         "3572",
         "8.22362717580548",
         "51125.0",
         "10.853309767619075",
         "17.211434318427973",
         "24.61511681881479",
         "34.69170663778431",
         "53.43880550827789",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "3572",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_004x016.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_004x016.txt"
        ],
        [
         "TENX158_004x017",
         "True",
         "7727.328900500986",
         "2505.0304532481027",
         "17.0",
         "4.0",
         "33847.0",
         "3399",
         "8.157657015196472",
         "33504.0",
         "10.429635192399903",
         "20.040180813661475",
         "27.55931101722457",
         "37.74337459745325",
         "55.768605784855374",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "3399",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_004x017.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_004x017.txt"
        ],
        [
         "TENX158_004x018",
         "True",
         "8092.620791887509",
         "2505.0304532481027",
         "18.0",
         "4.0",
         "49619.0",
         "3456",
         "8.191740021277457",
         "48990.0",
         "10.812149257245792",
         "18.146274612547614",
         "26.161349483060924",
         "36.95761704185897",
         "56.27078336927387",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "3456",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_004x018.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_004x018.txt"
        ],
        [
         "TENX158_004x019",
         "True",
         "8457.91268327403",
         "2505.0304532481027",
         "19.0",
         "4.0",
         "41008.0",
         "3529",
         "8.201934351194222",
         "40589.0",
         "10.621546833792289",
         "17.520971517752635",
         "24.53667577058135",
         "34.771264143581746",
         "53.23839250877877",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "3529",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_004x019.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_004x019.txt"
        ],
        [
         "TENX158_004x020",
         "True",
         "8823.204574660553",
         "2505.0304532481027",
         "20.0",
         "4.0",
         "29577.0",
         "3321",
         "8.130942302231878",
         "29315.0",
         "10.294786120726714",
         "20.955472157419617",
         "28.53230550765798",
         "38.69560807384116",
         "57.12208810900362",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "3321",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_004x020.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_004x020.txt"
        ],
        [
         "TENX158_004x021",
         "True",
         "9188.496466047076",
         "2505.0304532481027",
         "21.0",
         "4.0",
         "24865.0",
         "3211",
         "8.09346227450118",
         "24640.0",
         "10.121256687512883",
         "16.67001809772773",
         "23.828674844158456",
         "33.86688115825457",
         "52.91775588176151",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "3211",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_004x021.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_004x021.txt"
        ],
        [
         "TENX158_005x001",
         "True",
         "1882.658638316622",
         "2870.322344634626",
         "1.0",
         "5.0",
         "302.0",
         "205",
         "5.6240175061873385",
         "224.0",
         "5.713732805509369",
         "25.165562913907287",
         "41.72185430463576",
         "74.83443708609272",
         "100.0",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "205",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_005x001.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_005x001.txt"
        ],
        [
         "TENX158_005x005",
         "True",
         "3343.8262038627126",
         "2870.322344634626",
         "5.0",
         "5.0",
         "3397.0",
         "692",
         "6.573680166960646",
         "3281.0",
         "8.130942302231878",
         "54.22431557256403",
         "66.4115395937592",
         "79.5702090079482",
         "93.67088607594937",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "692",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_005x005.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_005x005.txt"
        ],
        [
         "TENX158_005x006",
         "True",
         "3709.118095249235",
         "2870.322344634626",
         "6.0",
         "5.0",
         "13495.0",
         "1556",
         "7.383989457978508",
         "13118.0",
         "9.510148624225804",
         "47.73619859207114",
         "58.05113004816599",
         "69.20340866987773",
         "84.94998147462023",
         "0.0",
         "0.0",
         "0.0",
         "TENX158",
         null,
         null,
         "0.0",
         "0.0",
         "1556",
         "/cwStorage/nodecw_group/jijh/hest_output/TENX158_tiles/TENX158_005x006.png",
         "/cwStorage/nodecw_group/jijh/hest_sentences_human_all/TENX158_sentences_hvg/TENX158_005x006.txt"
        ]
       ],
       "shape": {
        "columns": 25,
        "rows": 997054
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_tissue</th>\n",
       "      <th>pxl_col_in_fullres</th>\n",
       "      <th>pxl_row_in_fullres</th>\n",
       "      <th>array_col</th>\n",
       "      <th>array_row</th>\n",
       "      <th>n_counts</th>\n",
       "      <th>n_genes_by_counts</th>\n",
       "      <th>log1p_n_genes_by_counts</th>\n",
       "      <th>total_counts</th>\n",
       "      <th>log1p_total_counts</th>\n",
       "      <th>...</th>\n",
       "      <th>log1p_total_counts_mito</th>\n",
       "      <th>pct_counts_mito</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>pxl_row_in_fullres_old</th>\n",
       "      <th>pxl_col_in_fullres_old</th>\n",
       "      <th>total_counts_mt</th>\n",
       "      <th>pct_counts_mt</th>\n",
       "      <th>n_genes</th>\n",
       "      <th>image_path</th>\n",
       "      <th>sentence_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TENX158_000x017</th>\n",
       "      <td>True</td>\n",
       "      <td>7727.328901</td>\n",
       "      <td>1043.862888</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9334.0</td>\n",
       "      <td>980</td>\n",
       "      <td>6.908755</td>\n",
       "      <td>9056.0</td>\n",
       "      <td>9.141526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>TENX158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>980</td>\n",
       "      <td>/cwStorage/nodecw_group/jijh/hest_output/TENX1...</td>\n",
       "      <td>/cwStorage/nodecw_group/jijh/hest_sentences_hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TENX158_000x018</th>\n",
       "      <td>True</td>\n",
       "      <td>8092.620792</td>\n",
       "      <td>1043.862888</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2166.0</td>\n",
       "      <td>447</td>\n",
       "      <td>6.120297</td>\n",
       "      <td>2079.0</td>\n",
       "      <td>7.681099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>TENX158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>447</td>\n",
       "      <td>/cwStorage/nodecw_group/jijh/hest_output/TENX1...</td>\n",
       "      <td>/cwStorage/nodecw_group/jijh/hest_sentences_hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TENX158_000x020</th>\n",
       "      <td>True</td>\n",
       "      <td>8823.204575</td>\n",
       "      <td>1043.862888</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5275.0</td>\n",
       "      <td>866</td>\n",
       "      <td>6.778785</td>\n",
       "      <td>5081.0</td>\n",
       "      <td>8.570924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>TENX158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>866</td>\n",
       "      <td>/cwStorage/nodecw_group/jijh/hest_output/TENX1...</td>\n",
       "      <td>/cwStorage/nodecw_group/jijh/hest_sentences_hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TENX158_000x021</th>\n",
       "      <td>True</td>\n",
       "      <td>9188.496466</td>\n",
       "      <td>1043.862888</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5838.0</td>\n",
       "      <td>1268</td>\n",
       "      <td>7.163947</td>\n",
       "      <td>5674.0</td>\n",
       "      <td>8.672315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>TENX158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1268</td>\n",
       "      <td>/cwStorage/nodecw_group/jijh/hest_output/TENX1...</td>\n",
       "      <td>/cwStorage/nodecw_group/jijh/hest_sentences_hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TENX158_001x016</th>\n",
       "      <td>True</td>\n",
       "      <td>7362.037009</td>\n",
       "      <td>1409.154779</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24396.0</td>\n",
       "      <td>1840</td>\n",
       "      <td>7.554335</td>\n",
       "      <td>23763.0</td>\n",
       "      <td>10.102215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>TENX158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1840</td>\n",
       "      <td>/cwStorage/nodecw_group/jijh/hest_output/TENX1...</td>\n",
       "      <td>/cwStorage/nodecw_group/jijh/hest_sentences_hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC1_TTGTGTTTCCCGAAAG-1</th>\n",
       "      <td>True</td>\n",
       "      <td>6663.460818</td>\n",
       "      <td>8760.178788</td>\n",
       "      <td>59.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1456</td>\n",
       "      <td>7.312553</td>\n",
       "      <td>2394.0</td>\n",
       "      <td>7.803027</td>\n",
       "      <td>...</td>\n",
       "      <td>5.755742</td>\n",
       "      <td>12.872906</td>\n",
       "      <td>MISC1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>315.0</td>\n",
       "      <td>13.157895</td>\n",
       "      <td>1456</td>\n",
       "      <td>/cwStorage/nodecw_group/jijh/hest_output/MISC1...</td>\n",
       "      <td>/cwStorage/nodecw_group/jijh/hest_sentences_hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC1_TTGTTGTGTGTCAAGA-1</th>\n",
       "      <td>True</td>\n",
       "      <td>7922.034775</td>\n",
       "      <td>6375.541625</td>\n",
       "      <td>77.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1587</td>\n",
       "      <td>7.401842</td>\n",
       "      <td>2813.0</td>\n",
       "      <td>7.965893</td>\n",
       "      <td>...</td>\n",
       "      <td>6.165418</td>\n",
       "      <td>16.493055</td>\n",
       "      <td>MISC1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>475.0</td>\n",
       "      <td>16.885887</td>\n",
       "      <td>1587</td>\n",
       "      <td>/cwStorage/nodecw_group/jijh/hest_output/MISC1...</td>\n",
       "      <td>/cwStorage/nodecw_group/jijh/hest_sentences_hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC1_TTGTTTCACATCCAGG-1</th>\n",
       "      <td>True</td>\n",
       "      <td>5486.172801</td>\n",
       "      <td>9589.796448</td>\n",
       "      <td>42.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1476</td>\n",
       "      <td>7.331060</td>\n",
       "      <td>2577.0</td>\n",
       "      <td>7.878913</td>\n",
       "      <td>...</td>\n",
       "      <td>5.771441</td>\n",
       "      <td>12.121212</td>\n",
       "      <td>MISC1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320.0</td>\n",
       "      <td>12.417540</td>\n",
       "      <td>1476</td>\n",
       "      <td>/cwStorage/nodecw_group/jijh/hest_output/MISC1...</td>\n",
       "      <td>/cwStorage/nodecw_group/jijh/hest_sentences_hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC1_TTGTTTCATTAGTCTA-1</th>\n",
       "      <td>True</td>\n",
       "      <td>4657.957578</td>\n",
       "      <td>9823.488712</td>\n",
       "      <td>30.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1241</td>\n",
       "      <td>7.154615</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>7.632401</td>\n",
       "      <td>...</td>\n",
       "      <td>5.572154</td>\n",
       "      <td>12.699951</td>\n",
       "      <td>MISC1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>262.0</td>\n",
       "      <td>12.963879</td>\n",
       "      <td>1241</td>\n",
       "      <td>/cwStorage/nodecw_group/jijh/hest_output/MISC1...</td>\n",
       "      <td>/cwStorage/nodecw_group/jijh/hest_sentences_hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC1_TTGTTTCCATACAACT-1</th>\n",
       "      <td>True</td>\n",
       "      <td>4465.720971</td>\n",
       "      <td>8027.292313</td>\n",
       "      <td>27.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>727</td>\n",
       "      <td>6.618739</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>6.939254</td>\n",
       "      <td>...</td>\n",
       "      <td>5.087596</td>\n",
       "      <td>15.615908</td>\n",
       "      <td>MISC1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>161.0</td>\n",
       "      <td>15.972222</td>\n",
       "      <td>727</td>\n",
       "      <td>/cwStorage/nodecw_group/jijh/hest_output/MISC1...</td>\n",
       "      <td>/cwStorage/nodecw_group/jijh/hest_sentences_hu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>997054 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          in_tissue  pxl_col_in_fullres  pxl_row_in_fullres  \\\n",
       "TENX158_000x017                True         7727.328901         1043.862888   \n",
       "TENX158_000x018                True         8092.620792         1043.862888   \n",
       "TENX158_000x020                True         8823.204575         1043.862888   \n",
       "TENX158_000x021                True         9188.496466         1043.862888   \n",
       "TENX158_001x016                True         7362.037009         1409.154779   \n",
       "...                             ...                 ...                 ...   \n",
       "MISC1_TTGTGTTTCCCGAAAG-1       True         6663.460818         8760.178788   \n",
       "MISC1_TTGTTGTGTGTCAAGA-1       True         7922.034775         6375.541625   \n",
       "MISC1_TTGTTTCACATCCAGG-1       True         5486.172801         9589.796448   \n",
       "MISC1_TTGTTTCATTAGTCTA-1       True         4657.957578         9823.488712   \n",
       "MISC1_TTGTTTCCATACAACT-1       True         4465.720971         8027.292313   \n",
       "\n",
       "                          array_col  array_row  n_counts  n_genes_by_counts  \\\n",
       "TENX158_000x017                17.0        0.0    9334.0                980   \n",
       "TENX158_000x018                18.0        0.0    2166.0                447   \n",
       "TENX158_000x020                20.0        0.0    5275.0                866   \n",
       "TENX158_000x021                21.0        0.0    5838.0               1268   \n",
       "TENX158_001x016                16.0        1.0   24396.0               1840   \n",
       "...                             ...        ...       ...                ...   \n",
       "MISC1_TTGTGTTTCCCGAAAG-1       59.0       51.0       NaN               1456   \n",
       "MISC1_TTGTTGTGTGTCAAGA-1       77.0       31.0       NaN               1587   \n",
       "MISC1_TTGTTTCACATCCAGG-1       42.0       58.0       NaN               1476   \n",
       "MISC1_TTGTTTCATTAGTCTA-1       30.0       60.0       NaN               1241   \n",
       "MISC1_TTGTTTCCATACAACT-1       27.0       45.0       NaN                727   \n",
       "\n",
       "                          log1p_n_genes_by_counts  total_counts  \\\n",
       "TENX158_000x017                          6.908755        9056.0   \n",
       "TENX158_000x018                          6.120297        2079.0   \n",
       "TENX158_000x020                          6.778785        5081.0   \n",
       "TENX158_000x021                          7.163947        5674.0   \n",
       "TENX158_001x016                          7.554335       23763.0   \n",
       "...                                           ...           ...   \n",
       "MISC1_TTGTGTTTCCCGAAAG-1                 7.312553        2394.0   \n",
       "MISC1_TTGTTGTGTGTCAAGA-1                 7.401842        2813.0   \n",
       "MISC1_TTGTTTCACATCCAGG-1                 7.331060        2577.0   \n",
       "MISC1_TTGTTTCATTAGTCTA-1                 7.154615        2021.0   \n",
       "MISC1_TTGTTTCCATACAACT-1                 6.618739        1008.0   \n",
       "\n",
       "                          log1p_total_counts  ...  log1p_total_counts_mito  \\\n",
       "TENX158_000x017                     9.141526  ...                 0.000000   \n",
       "TENX158_000x018                     7.681099  ...                 0.000000   \n",
       "TENX158_000x020                     8.570924  ...                 0.000000   \n",
       "TENX158_000x021                     8.672315  ...                 0.000000   \n",
       "TENX158_001x016                    10.102215  ...                 0.000000   \n",
       "...                                      ...  ...                      ...   \n",
       "MISC1_TTGTGTTTCCCGAAAG-1            7.803027  ...                 5.755742   \n",
       "MISC1_TTGTTGTGTGTCAAGA-1            7.965893  ...                 6.165418   \n",
       "MISC1_TTGTTTCACATCCAGG-1            7.878913  ...                 5.771441   \n",
       "MISC1_TTGTTTCATTAGTCTA-1            7.632401  ...                 5.572154   \n",
       "MISC1_TTGTTTCCATACAACT-1            6.939254  ...                 5.087596   \n",
       "\n",
       "                          pct_counts_mito  sample_id  pxl_row_in_fullres_old  \\\n",
       "TENX158_000x017                  0.000000    TENX158                     NaN   \n",
       "TENX158_000x018                  0.000000    TENX158                     NaN   \n",
       "TENX158_000x020                  0.000000    TENX158                     NaN   \n",
       "TENX158_000x021                  0.000000    TENX158                     NaN   \n",
       "TENX158_001x016                  0.000000    TENX158                     NaN   \n",
       "...                                   ...        ...                     ...   \n",
       "MISC1_TTGTGTTTCCCGAAAG-1        12.872906      MISC1                     NaN   \n",
       "MISC1_TTGTTGTGTGTCAAGA-1        16.493055      MISC1                     NaN   \n",
       "MISC1_TTGTTTCACATCCAGG-1        12.121212      MISC1                     NaN   \n",
       "MISC1_TTGTTTCATTAGTCTA-1        12.699951      MISC1                     NaN   \n",
       "MISC1_TTGTTTCCATACAACT-1        15.615908      MISC1                     NaN   \n",
       "\n",
       "                          pxl_col_in_fullres_old  total_counts_mt  \\\n",
       "TENX158_000x017                              NaN              0.0   \n",
       "TENX158_000x018                              NaN              0.0   \n",
       "TENX158_000x020                              NaN              0.0   \n",
       "TENX158_000x021                              NaN              0.0   \n",
       "TENX158_001x016                              NaN              0.0   \n",
       "...                                          ...              ...   \n",
       "MISC1_TTGTGTTTCCCGAAAG-1                     NaN            315.0   \n",
       "MISC1_TTGTTGTGTGTCAAGA-1                     NaN            475.0   \n",
       "MISC1_TTGTTTCACATCCAGG-1                     NaN            320.0   \n",
       "MISC1_TTGTTTCATTAGTCTA-1                     NaN            262.0   \n",
       "MISC1_TTGTTTCCATACAACT-1                     NaN            161.0   \n",
       "\n",
       "                          pct_counts_mt n_genes  \\\n",
       "TENX158_000x017                0.000000     980   \n",
       "TENX158_000x018                0.000000     447   \n",
       "TENX158_000x020                0.000000     866   \n",
       "TENX158_000x021                0.000000    1268   \n",
       "TENX158_001x016                0.000000    1840   \n",
       "...                                 ...     ...   \n",
       "MISC1_TTGTGTTTCCCGAAAG-1      13.157895    1456   \n",
       "MISC1_TTGTTGTGTGTCAAGA-1      16.885887    1587   \n",
       "MISC1_TTGTTTCACATCCAGG-1      12.417540    1476   \n",
       "MISC1_TTGTTTCATTAGTCTA-1      12.963879    1241   \n",
       "MISC1_TTGTTTCCATACAACT-1      15.972222     727   \n",
       "\n",
       "                                                                 image_path  \\\n",
       "TENX158_000x017           /cwStorage/nodecw_group/jijh/hest_output/TENX1...   \n",
       "TENX158_000x018           /cwStorage/nodecw_group/jijh/hest_output/TENX1...   \n",
       "TENX158_000x020           /cwStorage/nodecw_group/jijh/hest_output/TENX1...   \n",
       "TENX158_000x021           /cwStorage/nodecw_group/jijh/hest_output/TENX1...   \n",
       "TENX158_001x016           /cwStorage/nodecw_group/jijh/hest_output/TENX1...   \n",
       "...                                                                     ...   \n",
       "MISC1_TTGTGTTTCCCGAAAG-1  /cwStorage/nodecw_group/jijh/hest_output/MISC1...   \n",
       "MISC1_TTGTTGTGTGTCAAGA-1  /cwStorage/nodecw_group/jijh/hest_output/MISC1...   \n",
       "MISC1_TTGTTTCACATCCAGG-1  /cwStorage/nodecw_group/jijh/hest_output/MISC1...   \n",
       "MISC1_TTGTTTCATTAGTCTA-1  /cwStorage/nodecw_group/jijh/hest_output/MISC1...   \n",
       "MISC1_TTGTTTCCATACAACT-1  /cwStorage/nodecw_group/jijh/hest_output/MISC1...   \n",
       "\n",
       "                                                              sentence_path  \n",
       "TENX158_000x017           /cwStorage/nodecw_group/jijh/hest_sentences_hu...  \n",
       "TENX158_000x018           /cwStorage/nodecw_group/jijh/hest_sentences_hu...  \n",
       "TENX158_000x020           /cwStorage/nodecw_group/jijh/hest_sentences_hu...  \n",
       "TENX158_000x021           /cwStorage/nodecw_group/jijh/hest_sentences_hu...  \n",
       "TENX158_001x016           /cwStorage/nodecw_group/jijh/hest_sentences_hu...  \n",
       "...                                                                     ...  \n",
       "MISC1_TTGTGTTTCCCGAAAG-1  /cwStorage/nodecw_group/jijh/hest_sentences_hu...  \n",
       "MISC1_TTGTTGTGTGTCAAGA-1  /cwStorage/nodecw_group/jijh/hest_sentences_hu...  \n",
       "MISC1_TTGTTTCACATCCAGG-1  /cwStorage/nodecw_group/jijh/hest_sentences_hu...  \n",
       "MISC1_TTGTTTCATTAGTCTA-1  /cwStorage/nodecw_group/jijh/hest_sentences_hu...  \n",
       "MISC1_TTGTTTCCATACAACT-1  /cwStorage/nodecw_group/jijh/hest_sentences_hu...  \n",
       "\n",
       "[997054 rows x 25 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_adata.obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd69d3c2",
   "metadata": {},
   "source": [
    "# 改正之后"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3ba7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# ==============================================================================\n",
    "# SpaGLaM SOTA 数据预处理工作流 (V2 - 已修复路径和多进程问题)\n",
    "#\n",
    "# 核心策略:\n",
    "# 1. 加载预先合并好的AnnData缓存文件。\n",
    "# 2. **准备主AnnData**:\n",
    "#    a. 根据坐标和样本ID，重建并添加 image_path 和 sentence_path 列。\n",
    "#    b. 通过“拆分-计算-合并”策略计算并添加空间邻接图。\n",
    "# 3. 将所有独立的图像和基因语句文件打包成高效的.tar分片。\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import webdataset as wds\n",
    "from tqdm.auto import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "# --- 1. 全局设置 ---\n",
    "\n",
    "# --- 路径配置 (请根据您的环境修改) ---\n",
    "BASE_DIR = \"/cwStorage/nodecw_group/jijh\" \n",
    "EXISTING_CACHE_PATH = os.path.join(BASE_DIR, \"hest_cache/cache/adata_preprocessed_canonical_v2.h5ad\")\n",
    "IMAGE_BASE_DIR = os.path.join(BASE_DIR, \"hest_output\")\n",
    "SENTENCE_BASE_DIR = os.path.join(BASE_DIR, \"hest_sentences_human_all\")\n",
    "\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"spaglam_sota_data\")\n",
    "FINAL_ADATA_PATH = os.path.join(OUTPUT_DIR, \"master_adata_with_graph_and_paths.h5ad\") # 新文件名\n",
    "SHARDS_OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"webdataset_shards\")\n",
    "\n",
    "# --- 参数配置 ---\n",
    "N_NEIGHBORS = 6\n",
    "SAMPLES_PER_SHARD = 10000\n",
    "NUM_WORKERS_PACKAGING = 16\n",
    "\n",
    "# --- 辅助函数 ---\n",
    "def format_time(seconds: float) -> str:\n",
    "    \"\"\"将秒数格式化为易读的“分-秒”字符串\"\"\"\n",
    "    mins, secs = divmod(seconds, 60)\n",
    "    return f\"{int(mins)}分 {secs:.2f}秒\"\n",
    "\n",
    "def print_header(title):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"  {title.upper()}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# --- 核心功能函数 (已修改和重构) ---\n",
    "\n",
    "def prepare_master_adata(adata: sc.AnnData, n_neighbors: int) -> sc.AnnData:\n",
    "    \"\"\"\n",
    "    一个集总函数，用于对加载的 AnnData 对象进行最终准备。\n",
    "    包括添加文件路径和计算空间图。\n",
    "    \"\"\"\n",
    "    print_header(\"准备主 AnnData 文件\")\n",
    "    adata = add_file_paths(adata)\n",
    "    adata = add_spatial_graph(adata, n_neighbors)\n",
    "    return adata\n",
    "\n",
    "def add_file_paths(adata: sc.AnnData) -> sc.AnnData:\n",
    "    \"\"\"\n",
    "    【新增函数】根据坐标和样本ID，重建并添加 image_path 和 sentence_path 列。\n",
    "    \"\"\"\n",
    "    print(\"--> 步骤 1.1: 重建并添加文件路径...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if 'image_path' in adata.obs.columns and 'sentence_path' in adata.obs.columns:\n",
    "        # 检查路径是否有效，如果无效则重新生成\n",
    "        first_path = adata.obs['image_path'].iloc[0]\n",
    "        if os.path.exists(first_path):\n",
    "            print(\"✅ 文件路径列已存在且有效，跳过重建。\")\n",
    "            return adata\n",
    "        else:\n",
    "            print(\"⚠️ 检测到无效的文件路径，将重新构建...\")\n",
    "\n",
    "    # 从 spot ID 和坐标重建路径\n",
    "    def _get_path(row):\n",
    "        spot_id = row.name\n",
    "        sample_id = row['sample_id']\n",
    "        # 根据您的提示，文件名由坐标取整得到\n",
    "        x_coord = int(round(row['spatial_x']))\n",
    "        y_coord = int(round(row['spatial_y']))\n",
    "        \n",
    "        # 构建与您原始文件名格式一致的字符串\n",
    "        filename_base = f\"{sample_id}_{y_coord}_{x_coord}\"\n",
    "        \n",
    "        img_path = os.path.join(IMAGE_BASE_DIR, f\"{sample_id}_tiles\", f\"{filename_base}.png\")\n",
    "        sentence_path = os.path.join(SENTENCE_BASE_DIR, f\"{sample_id}_sentences_hvg\", f\"{filename_base}.txt\")\n",
    "        return img_path, sentence_path\n",
    "\n",
    "    # 为了高效处理，先将坐标提取出来\n",
    "    adata.obs['spatial_x'] = adata.obsm['spatial'][:, 0]\n",
    "    adata.obs['spatial_y'] = adata.obsm['spatial'][:, 1]\n",
    "    \n",
    "    # 使用.apply()方法高效地为每一行生成路径\n",
    "    paths_df = adata.obs.apply(_get_path, axis=1, result_type='expand')\n",
    "    paths_df.columns = ['image_path', 'sentence_path']\n",
    "    \n",
    "    # 将新生成的列合并回 .obs\n",
    "    adata.obs['image_path'] = paths_df['image_path']\n",
    "    adata.obs['sentence_path'] = paths_df['sentence_path']\n",
    "\n",
    "    # 清理临时列\n",
    "    adata.obs.drop(columns=['spatial_x', 'spatial_y'], inplace=True)\n",
    "\n",
    "    print(f\"✅ 文件路径已成功添加到 '.obs' 中。\")\n",
    "    end_time = time.time()\n",
    "    print(f\"🕒 耗时: {format_time(end_time - start_time)}\")\n",
    "    return adata\n",
    "\n",
    "\n",
    "def add_spatial_graph(adata: sc.AnnData, n_neighbors: int) -> sc.AnnData:\n",
    "    \"\"\"【无Squidpy替代方案】通过“拆分-计算-合并”策略，计算并添加空间邻接图。\"\"\"\n",
    "    print(\"--> 步骤 1.2: 计算并添加空间邻接图...\")\n",
    "    start_time = time.time()\n",
    "    if 'spatial_connectivities' in adata.obsp:\n",
    "        print(\"✅ 空间邻接图 'spatial_connectivities' 已存在，跳过计算。\")\n",
    "        return adata\n",
    "        \n",
    "    unique_samples = adata.obs['sample_id'].unique()\n",
    "    print(f\"检测到 {len(unique_samples)} 个独立样本，将逐个计算空间图...\")\n",
    "    \n",
    "    global_conn_matrix = lil_matrix((adata.n_obs, adata.n_obs), dtype=np.float32)\n",
    "\n",
    "    for sample_id in tqdm(unique_samples, desc=\"处理每个样本的图\"):\n",
    "        sample_mask = adata.obs['sample_id'] == sample_id\n",
    "        adata_sample = adata[sample_mask]\n",
    "        \n",
    "        sc.pp.neighbors(adata_sample, n_neighbors=n_neighbors, use_rep='spatial', key_added='spatial')\n",
    "        \n",
    "        local_conn = adata_sample.obsp['spatial_connectivities']\n",
    "        global_indices = np.where(sample_mask)[0]\n",
    "        \n",
    "        rows, cols = local_conn.nonzero()\n",
    "        global_rows = global_indices[rows]\n",
    "        global_cols = global_indices[cols]\n",
    "        \n",
    "        global_conn_matrix[global_rows, global_cols] = local_conn.data.reshape(-1, 1)\n",
    "\n",
    "    adata.obsp['spatial_connectivities'] = global_conn_matrix.tocsr()\n",
    "    \n",
    "    print(\"✅ 所有样本的空间邻接图已计算并合并！\")\n",
    "    end_time = time.time()\n",
    "    print(f\"🕒 耗时: {format_time(end_time - start_time)}\")\n",
    "    return adata\n",
    "\n",
    "# ... 多进程工作函数 _process_chunk_worker 保持不变 ...\n",
    "def _process_chunk_worker(args):\n",
    "    spot_ids_chunk, chunk_index, all_spot_info, output_pattern = args\n",
    "    shard_path = output_pattern % chunk_index\n",
    "    with wds.TarWriter(shard_path) as sink:\n",
    "        for spot_id in spot_ids_chunk:\n",
    "            paths = all_spot_info.get(spot_id)\n",
    "            if not paths: continue\n",
    "            img_path, sentence_path = paths['image_path'], paths['sentence_path']\n",
    "            if not (os.path.exists(img_path) and os.path.exists(sentence_path)): continue\n",
    "            with open(img_path, \"rb\") as f_img: image_data = f_img.read()\n",
    "            with open(sentence_path, \"rb\") as f_txt: sentence_data = f_txt.read()\n",
    "            sample = {\"__key__\": spot_id, \"png\": image_data, \"txt\": sentence_data}\n",
    "            sink.write(sample)\n",
    "    return chunk_index\n",
    "\n",
    "def create_webdataset_shards(adata: sc.AnnData, output_pattern: str):\n",
    "    \"\"\"【逻辑简化】现在假设adata中已包含所需路径。\"\"\"\n",
    "    print_header(\"2. 将原始数据打包成 WebDataset 分片\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 直接使用 AnnData 中的路径信息\n",
    "    all_spot_info = adata.obs[['image_path', 'sentence_path']].dropna().to_dict('index')\n",
    "    all_spot_ids = list(all_spot_info.keys())\n",
    "    \n",
    "    print(f\"准备打包 {len(all_spot_ids)} 个 spots 的数据...\")\n",
    "    os.makedirs(os.path.dirname(output_pattern), exist_ok=True)\n",
    "    \n",
    "    spot_id_chunks = [all_spot_ids[i:i + SAMPLES_PER_SHARD] for i in range(0, len(all_spot_ids), SAMPLES_PER_SHARD)]\n",
    "    \n",
    "    # 准备任务参数\n",
    "    tasks = [(chunk, i, all_spot_info, output_pattern) for i, chunk in enumerate(spot_id_chunks)]\n",
    "    \n",
    "    print(f\"启动 {NUM_WORKERS_PACKAGING} 个工作进程进行打包...\")\n",
    "    with ProcessPoolExecutor(max_workers=NUM_WORKERS_PACKAGING) as executor:\n",
    "        results = list(tqdm(executor.map(_process_chunk_worker, tasks), total=len(tasks), desc=\"打包 .tar 分片\"))\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"✅ 数据打包完成！成功处理了 {len(results)} 个分片。\")\n",
    "    print(f\"🕒 耗时: {format_time(end_time - start_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3a2aa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 开始SOTA数据预处理工作流 (V2 - 已修复路径和多进程问题)...\n",
      "\n",
      "正在从缓存加载主 AnnData 文件: /cwStorage/nodecw_group/jijh/hest_cache/cache/adata_preprocessed_canonical_v2.h5ad\n",
      "✅ 加载成功！AnnData 维度: (997054, 30148)\n",
      "\n",
      "================================================================================\n",
      "  准备主 ANNDATA 文件\n",
      "================================================================================\n",
      "--> 步骤 1.1: 重建并添加文件路径...\n",
      "✅ 文件路径已成功添加到 '.obs' 中。\n",
      "🕒 耗时: 0分 20.62秒\n",
      "--> 步骤 1.2: 计算并添加空间邻接图...\n",
      "检测到 505 个独立样本，将逐个计算空间图...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c1108d45984ad4bad9faf59400085c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "处理每个样本的图:   0%|          | 0/505 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 所有样本的空间邻接图已计算并合并！\n",
      "🕒 耗时: 2分 16.59秒\n",
      "\n",
      "正在保存最终的主 AnnData 文件至: /cwStorage/nodecw_group/jijh/spaglam_sota_data/master_adata_with_graph_and_paths.h5ad\n",
      "✅ 保存成功！\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 开始SOTA数据预处理工作流 (V2 - 已修复路径和多进程问题)...\")\n",
    "\n",
    "print(f\"\\n正在从缓存加载主 AnnData 文件: {EXISTING_CACHE_PATH}\")\n",
    "if not os.path.exists(EXISTING_CACHE_PATH):\n",
    "    print(f\"❌ 错误: 缓存文件未找到！请确认路径 '{EXISTING_CACHE_PATH}' 是否正确。\")\n",
    "    sys.exit(1)\n",
    "\n",
    "master_adata = sc.read_h5ad(EXISTING_CACHE_PATH)\n",
    "print(f\"✅ 加载成功！AnnData 维度: {master_adata.shape}\")\n",
    "\n",
    "# 【核心步骤】一步准备好主 AnnData\n",
    "final_master_adata = prepare_master_adata(master_adata, n_neighbors=N_NEIGHBORS)\n",
    "\n",
    "print(f\"\\n正在保存最终的主 AnnData 文件至: {FINAL_ADATA_PATH}\")\n",
    "os.makedirs(os.path.dirname(FINAL_ADATA_PATH), exist_ok=True)\n",
    "final_master_adata.write_h5ad(FINAL_ADATA_PATH)\n",
    "print(\"✅ 保存成功！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93e83a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  2. 将原始数据打包成 WEBDATASET 分片\n",
      "================================================================================\n",
      "准备打包 997054 个 spots 的数据...\n",
      "启动 16 个工作进程进行打包...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c87dfcaef3741ceb2cb15edd27f2b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "打包 .tar 分片:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 数据打包完成！成功处理了 100 个分片。\n",
      "🕒 耗时: 12分 43.16秒\n",
      "\n",
      "\n",
      "🎉🎉🎉 恭喜！SOTA 数据预处理全部完成！🎉🎉🎉\n",
      "您现在拥有：\n",
      "1. 一个包含所有元数据、文件路径和空间图的中央索引文件: /cwStorage/nodecw_group/jijh/spaglam_sota_data/master_adata_with_graph_and_paths.h5ad\n",
      "2. 一组用于高效训练的数据仓库分片文件位于: /cwStorage/nodecw_group/jijh/spaglam_sota_data/webdataset_shards\n"
     ]
    }
   ],
   "source": [
    "\n",
    "shard_pattern = os.path.join(SHARDS_OUTPUT_PATH, \"dataset-%06d.tar\")\n",
    "create_webdataset_shards(final_master_adata, shard_pattern)\n",
    "\n",
    "print(\"\\n\\n🎉🎉🎉 恭喜！SOTA 数据预处理全部完成！🎉🎉🎉\")\n",
    "print(\"您现在拥有：\")\n",
    "print(f\"1. 一个包含所有元数据、文件路径和空间图的中央索引文件: {FINAL_ADATA_PATH}\")\n",
    "print(f\"2. 一组用于高效训练的数据仓库分片文件位于: {SHARDS_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968f6746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  1. 文件存在性检查\n",
      "================================================================================\n",
      "✅ [成功] 主 AnnData 文件存在: /cwStorage/nodecw_group/jijh/spaglam_sota_data/master_adata_with_graph_and_paths.h5ad\n",
      "✅ [成功] WebDataset 分片目录存在，并找到 100 个 .tar 文件。\n",
      "\n",
      "================================================================================\n",
      "  2. 主 ANNDATA 文件完整性检查\n",
      "================================================================================\n",
      "✅ [成功] 成功加载 AnnData 文件。\n",
      "ℹ️ [信息] AnnData 维度 (spots, genes): 997054 x 30148\n",
      "✅ [成功] '.obs' 中的关键列均存在。\n",
      "✅ [成功] 空间坐标 '.obsm['spatial']' 存在。\n",
      "✅ [成功] 空间邻接图 '.obsp['spatial_connectivities']' 存在。\n",
      "\n",
      "================================================================================\n",
      "  3. WEBDATASET 分片结构检查\n",
      "================================================================================\n",
      "ℹ️ [信息] 正在抽样检查分片: dataset-000099.tar\n",
      "✅ [成功] 成功检查了 3 个样本，结构正确。\n",
      "\n",
      "================================================================================\n",
      "  4. 端到端一致性检查\n",
      "================================================================================\n",
      "ℹ️ [信息] 将随机抽样检查以下 5 个 spots: ['NCBI829_TCCCGGTCAGGAATTT-1', 'ZEN48_ACGTTAATGTCGAAGA-1', 'NCBI653_TGCAAACGTACTAGTT-1', 'ZEN46_ATATCGTTCCTCGAAC-1', 'MISC70_CAACTATATCGAATGC-1']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f2ab04591d416caa5571011c1de8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "端到端检查:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 正在检查 Spot: NCBI653_TGCAAACGTACTAGTT-1 ---\n",
      "✅ [成功] AnnData: 找到 4 个邻居。示例: ['NCBI653_CAAGTCGTTGAAATCT-1', 'NCBI653_GGCTGTCCTACTGCGG-1', 'NCBI653_TACTGCATGATTAAAT-1']\n",
      "✅ [成功] WebDataset: 图像和文本数据成功解码。\n",
      "ℹ️ [信息] 图像尺寸: (580, 580), 句子预览: 'CLU S100A6-1 SPARC-1 CMTM4 PLEKHB1-1 NUPR1-1 UQCRB...'\n",
      "\n",
      "--- 正在检查 Spot: NCBI829_TCCCGGTCAGGAATTT-1 ---\n",
      "✅ [成功] AnnData: 找到 5 个邻居。示例: ['NCBI829_ACGAGAACCCATCACG-1', 'NCBI829_CCAGCTACGCCTCATA-1', 'NCBI829_CGATATTAGCCGCAGG-1']\n",
      "✅ [成功] WebDataset: 图像和文本数据成功解码。\n",
      "ℹ️ [信息] 图像尺寸: (114, 114), 句子预览: 'ALB SAA1-1 SERPINA1-1 APOC1 MT2A-1 C3 SAA2-1 MT1G-...'\n",
      "\n",
      "--- 正在检查 Spot: MISC70_CAACTATATCGAATGC-1 ---\n",
      "✅ [成功] AnnData: 找到 6 个邻居。示例: ['MISC70_AGTTAAGCGGTCCCGG-1', 'MISC70_ATTTCATTATTTCGCG-1', 'MISC70_CATAGTACATTGAGAG-1']\n",
      "✅ [成功] WebDataset: 图像和文本数据成功解码。\n",
      "ℹ️ [信息] 图像尺寸: (362, 362), 句子预览: 'IGKC-1 SLC25A5-1 S100A6-1 LRATD1-1 ALDOA TPM2-1 IG...'\n",
      "\n",
      "--- 正在检查 Spot: ZEN48_ACGTTAATGTCGAAGA-1 ---\n",
      "✅ [成功] AnnData: 找到 5 个邻居。示例: ['ZEN48_AGGTTTCACACACCTT-1', 'ZEN48_CAAGGATCGCATGTTC-1', 'ZEN48_CCTAAATTAACGGTTC-1']\n",
      "✅ [成功] WebDataset: 图像和文本数据成功解码。\n",
      "ℹ️ [信息] 图像尺寸: (218, 218), 句子预览: 'PIGR-1 IGHA1-1 IGKC-1 B2M OLFM4-1 S100A6-1 PLA2G2A...'\n",
      "\n",
      "--- 正在检查 Spot: ZEN46_ATATCGTTCCTCGAAC-1 ---\n",
      "✅ [成功] AnnData: 找到 4 个邻居。示例: ['ZEN46_ACTGAAACGCCGTTAG-1', 'ZEN46_AGTCGTCGACCACCAA-1', 'ZEN46_CCTGTTTGAAGACACG-1']\n",
      "✅ [成功] WebDataset: 图像和文本数据成功解码。\n",
      "ℹ️ [信息] 图像尺寸: (218, 218), 句子预览: 'B2M S100A6-1 CD74 SFN-1 TMSB4X-1 COL1A1 COL1A2 ANX...'\n",
      "✅ [成功] \n",
      "所有 5 个抽样检查的 spot 均通过了端到端一致性验证！\n",
      "\n",
      "\n",
      "*************************  数据集验证通过  *************************\n",
      "✅ 您的数据集已准备就绪，可以用于SpaGLaM模型训练！\n",
      "********************************************************************************\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# ==============================================================================\n",
    "# SpaGLaM SOTA 数据集验证脚本\n",
    "# \n",
    "# 用途:\n",
    "#   - 验证预处理流程是否成功。\n",
    "#   - 检查主 AnnData 文件和 WebDataset 分片的完整性与一致性。\n",
    "#   - 在开始昂贵的模型训练前，进行一次快速的健全性检查 (Sanity Check)。\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import scanpy as sc\n",
    "import webdataset as wds\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# --- 1. 配置区域 (请根据您的环境修改) ---\n",
    "\n",
    "# --- 路径配置 ---\n",
    "BASE_DIR = \"/cwStorage/nodecw_group/jijh\"\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"spaglam_sota_data\")\n",
    "FINAL_ADATA_PATH = os.path.join(OUTPUT_DIR, \"master_adata_with_graph_and_paths.h5ad\")\n",
    "SHARDS_OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"webdataset_shards\")\n",
    "\n",
    "# --- 验证参数 ---\n",
    "# 随机抽样检查的spot数量\n",
    "NUM_SAMPLES_TO_CHECK = 5\n",
    "\n",
    "# --- 辅助函数 ---\n",
    "def print_header(title):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"  {title.upper()}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "def print_status(message, success=True):\n",
    "    prefix = \"✅ [成功]\" if success else \"❌ [失败]\"\n",
    "    print(f\"{prefix} {message}\")\n",
    "\n",
    "def print_info(message):\n",
    "    print(f\"ℹ️ [信息] {message}\")\n",
    "\n",
    "\n",
    "# --- 验证函数 ---\n",
    "\n",
    "def check_file_existence():\n",
    "    \"\"\"检查所有关键文件和目录是否存在\"\"\"\n",
    "    print_header(\"1. 文件存在性检查\")\n",
    "    all_exist = True\n",
    "    \n",
    "    # 检查主 AnnData 文件\n",
    "    if os.path.exists(FINAL_ADATA_PATH):\n",
    "        print_status(f\"主 AnnData 文件存在: {FINAL_ADATA_PATH}\")\n",
    "    else:\n",
    "        print_status(f\"主 AnnData 文件缺失: {FINAL_ADATA_PATH}\", success=False)\n",
    "        all_exist = False\n",
    "\n",
    "    # 检查 WebDataset 分片目录\n",
    "    if os.path.isdir(SHARDS_OUTPUT_PATH):\n",
    "        shards = [f for f in os.listdir(SHARDS_OUTPUT_PATH) if f.endswith('.tar')]\n",
    "        if shards:\n",
    "            print_status(f\"WebDataset 分片目录存在，并找到 {len(shards)} 个 .tar 文件。\")\n",
    "            # 随机选择一个分片路径用于后续检查\n",
    "            random_shard_path = os.path.join(SHARDS_OUTPUT_PATH, random.choice(shards))\n",
    "        else:\n",
    "            print_status(\"WebDataset 分片目录存在，但其中没有 .tar 文件。\", success=False)\n",
    "            all_exist = False\n",
    "            random_shard_path = None\n",
    "    else:\n",
    "        print_status(f\"WebDataset 分片目录缺失: {SHARDS_OUTPUT_PATH}\", success=False)\n",
    "        all_exist = False\n",
    "        random_shard_path = None\n",
    "        \n",
    "    return all_exist, random_shard_path\n",
    "\n",
    "\n",
    "def inspect_master_adata():\n",
    "    \"\"\"检查主 AnnData 文件的内部结构\"\"\"\n",
    "    print_header(\"2. 主 AnnData 文件完整性检查\")\n",
    "    try:\n",
    "        adata = sc.read_h5ad(FINAL_ADATA_PATH)\n",
    "        print_status(f\"成功加载 AnnData 文件。\")\n",
    "        print_info(f\"AnnData 维度 (spots, genes): {adata.n_obs} x {adata.n_vars}\")\n",
    "        \n",
    "        # 检查关键列\n",
    "        required_obs = ['sample_id', 'image_path', 'sentence_path']\n",
    "        for col in required_obs:\n",
    "            if col not in adata.obs.columns:\n",
    "                print_status(f\"'.obs' 中缺少关键列: '{col}'\", success=False)\n",
    "                return None\n",
    "        print_status(\"'.obs' 中的关键列均存在。\")\n",
    "        \n",
    "        # 检查空间坐标\n",
    "        if 'spatial' not in adata.obsm:\n",
    "            print_status(\"'.obsm' 中缺少空间坐标 'spatial'\", success=False)\n",
    "            return None\n",
    "        print_status(\"空间坐标 '.obsm['spatial']' 存在。\")\n",
    "\n",
    "        # 检查空间邻接图 (最重要)\n",
    "        if 'spatial_connectivities' not in adata.obsp:\n",
    "            print_status(\"'.obsp' 中缺少空间邻接图 'spatial_connectivities'\", success=False)\n",
    "            return None\n",
    "        print_status(\"空间邻接图 '.obsp['spatial_connectivities']' 存在。\")\n",
    "        \n",
    "        return adata\n",
    "    except Exception as e:\n",
    "        print_status(f\"加载或检查 AnnData 文件时出错: {e}\", success=False)\n",
    "        return None\n",
    "\n",
    "\n",
    "def inspect_tar_shard(shard_path):\n",
    "    \"\"\"检查单个 .tar 分片的内部结构\"\"\"\n",
    "    print_header(\"3. WebDataset 分片结构检查\")\n",
    "    if not shard_path:\n",
    "        print_status(\"没有可供检查的分片文件。\", success=False)\n",
    "        return False\n",
    "        \n",
    "    try:\n",
    "        print_info(f\"正在抽样检查分片: {os.path.basename(shard_path)}\")\n",
    "        dataset = wds.WebDataset(shard_path)\n",
    "        \n",
    "        sample_count = 0\n",
    "        for i, sample in enumerate(dataset):\n",
    "            if i >= 3: break  # 只检查前3个样本\n",
    "            \n",
    "            # 检查关键键\n",
    "            expected_keys = {'__key__', 'png', 'txt'}\n",
    "            if not expected_keys.issubset(sample.keys()):\n",
    "                print_status(f\"样本 {sample['__key__']} 缺少关键键。期望: {expected_keys}, 实际: {sample.keys()}\", success=False)\n",
    "                return False\n",
    "            \n",
    "            # 尝试解码\n",
    "            Image.open(io.BytesIO(sample['png'])).convert(\"RGB\")\n",
    "            sample['txt'].decode('utf-8')\n",
    "            sample_count += 1\n",
    "\n",
    "        if sample_count > 0:\n",
    "            print_status(f\"成功检查了 {sample_count} 个样本，结构正确。\")\n",
    "            return True\n",
    "        else:\n",
    "            print_status(\"分片文件为空或无法读取样本。\", success=False)\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print_status(f\"检查 .tar 分片时出错: {e}\", success=False)\n",
    "        return False\n",
    "\n",
    "\n",
    "def perform_end_to_end_check(adata, tar_urls):\n",
    "    \"\"\"对随机样本进行端到端一致性检查\"\"\"\n",
    "    print_header(\"4. 端到端一致性检查\")\n",
    "    \n",
    "    try:\n",
    "        spot_ids_to_check = random.sample(adata.obs_names.tolist(), k=NUM_SAMPLES_TO_CHECK)\n",
    "        print_info(f\"将随机抽样检查以下 {NUM_SAMPLES_TO_CHECK} 个 spots: {spot_ids_to_check}\")\n",
    "\n",
    "        # 使用WebDataset查找并加载这些样本\n",
    "        dataset = wds.WebDataset(tar_urls).select(lambda x: x['__key__'] in spot_ids_to_check)\n",
    "        \n",
    "        found_count = 0\n",
    "        for sample in tqdm(dataset, total=len(spot_ids_to_check), desc=\"端到端检查\"):\n",
    "            spot_id = sample['__key__']\n",
    "            print(f\"\\n--- 正在检查 Spot: {spot_id} ---\")\n",
    "            \n",
    "            # 1. 验证 AnnData 中的邻居信息\n",
    "            try:\n",
    "                idx = adata.obs_names.get_loc(spot_id)\n",
    "                neighbors_indices = adata.obsp['spatial_connectivities'][idx].indices\n",
    "                neighbor_ids = adata.obs_names[neighbors_indices].tolist()\n",
    "                print_status(f\"AnnData: 找到 {len(neighbor_ids)} 个邻居。示例: {neighbor_ids[:3]}\")\n",
    "            except Exception as e:\n",
    "                print_status(f\"AnnData: 查找邻居失败: {e}\", success=False)\n",
    "                return False\n",
    "\n",
    "            # 2. 验证 WebDataset 中的数据\n",
    "            try:\n",
    "                img = Image.open(io.BytesIO(sample['png']))\n",
    "                sentence = sample['txt'].decode('utf-8')\n",
    "                print_status(\"WebDataset: 图像和文本数据成功解码。\")\n",
    "                print_info(f\"图像尺寸: {img.size}, 句子预览: '{sentence[:50]}...'\")\n",
    "            except Exception as e:\n",
    "                print_status(f\"WebDataset: 解码数据失败: {e}\", success=False)\n",
    "                return False\n",
    "            \n",
    "            found_count += 1\n",
    "        \n",
    "        if found_count == len(spot_ids_to_check):\n",
    "            print_status(f\"\\n所有 {found_count} 个抽样检查的 spot 均通过了端到端一致性验证！\")\n",
    "            return True\n",
    "        else:\n",
    "            print_status(f\"只找到了 {found_count}/{len(spot_ids_to_check)} 个抽样样本。数据可能不完整或key不匹配。\", success=False)\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print_status(f\"端到端检查过程中发生严重错误: {e}\", success=False)\n",
    "        return False\n",
    "\n",
    "\n",
    "# --- 主执行流程 ---\n",
    "def main():\n",
    "    \"\"\"主验证函数\"\"\"\n",
    "    files_ok, random_shard = check_file_existence()\n",
    "    if not files_ok:\n",
    "        print(\"\\n❌ 基础文件缺失，验证终止。请先成功运行预处理脚本。\")\n",
    "        return\n",
    "\n",
    "    adata = inspect_master_adata()\n",
    "    if adata is None:\n",
    "        print(\"\\n❌ 主 AnnData 文件存在问题，验证终止。\")\n",
    "        return\n",
    "        \n",
    "    tar_ok = inspect_tar_shard(random_shard)\n",
    "    if not tar_ok:\n",
    "        print(\"\\n❌ WebDataset 分片文件存在问题，验证终止。\")\n",
    "        return\n",
    "    \n",
    "    # 获取所有分片的URL列表\n",
    "    all_shards = [os.path.join(SHARDS_OUTPUT_PATH, f) for f in os.listdir(SHARDS_OUTPUT_PATH) if f.endswith('.tar')]\n",
    "    \n",
    "    e2e_ok = perform_end_to_end_check(adata, all_shards)\n",
    "    if not e2e_ok:\n",
    "        print(\"\\n❌ 端到端一致性检查失败，请检查数据生成逻辑。\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\\n\" + \"*\"*25 + \"  数据集验证通过  \" + \"*\"*25)\n",
    "    print(\"✅ 您的数据集已准备就绪，可以用于SpaGLaM模型训练！\")\n",
    "    print(\"*\"*80)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a088550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gigapath)",
   "language": "python",
   "name": "gigapath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
