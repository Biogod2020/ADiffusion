

<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Project Plan: SpaGLaM (Spatial Graph-Language Model)</title>
<style>
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap');


:root {
        --bg-dark: #0d1117;
        --bg-light: #161b22;
        --border-color: #30363d;
        --text-primary: #e5e7eb;
        --text-secondary: #8b949e;
        --accent-purple: #a78bfa;
        --accent-blue: #60a5fa;
        --accent-pink: #f472b6;
        --accent-gradient: linear-gradient(135deg, var(--accent-blue), var(--accent-purple), var(--accent-pink));
        --shadow-light: rgba(139, 92, 246, 0.1);
        --shadow-dark: rgba(0, 0, 0, 0.4);
    }

    /* --- Base Styles --- */
    html { scroll-behavior: smooth; }
    body {
        font-family: 'Inter', sans-serif;
        background-color: var(--bg-dark);
        color: var(--text-primary);
        margin: 0;
        padding: 3rem 1.5rem;
        line-height: 1.8;
        font-size: 16px;
    }

    .container {
        max-width: 1200px;
        margin: 0 auto;
    }

    /* --- Header --- */
    .header {
        text-align: center;
        margin-bottom: 5rem;
    }
    .header h1 {
        font-size: 3.5rem;
        font-weight: 800;
        letter-spacing: -0.04em;
        background: var(--accent-gradient);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        margin-bottom: 1rem;
    }
    .header p {
        font-size: 1.25rem;
        color: var(--text-secondary);
        max-width: 850px;
        margin: 0 auto;
    }

    /* --- Section Styling --- */
    .section {
        background: var(--bg-light);
        border: 1px solid var(--border-color);
        border-radius: 1.25rem;
        padding: 2.5rem;
        margin-bottom: 2.5rem;
        box-shadow: 0 8px 30px rgba(0,0,0,0.2);
    }

    .section h2 {
        font-size: 2.25rem;
        font-weight: 700;
        margin: 0 0 1.5rem;
        padding-bottom: 1rem;
        border-bottom: 1px solid var(--border-color);
    }

    .section h3 {
        font-size: 1.5rem;
        font-weight: 600;
        color: var(--accent-purple);
        margin: 2rem 0 1.25rem;
    }

    p, ul, ol { color: var(--text-secondary); font-size: 1rem; }
    ul, ol { padding-left: 0; }
    ul { list-style: none; }
    ol { padding-left: 1.75rem; }
    ul li, ol li {
        position: relative;
        padding-left: 1.75rem;
        margin-bottom: 0.75rem;
    }
    ul li::before {
        content: '»';
        color: var(--accent-blue);
        font-weight: bold;
        position: absolute;
        left: 0;
        top: -2px;
        font-size: 1.2rem;
    }
    ol li { padding-left: 0.5rem; }
    
    blockquote {
        margin: 1.5rem 0;
        padding: 1rem 1.5rem;
        border-left: 4px solid var(--accent-purple);
        background: rgba(0,0,0,0.2);
        border-radius: 0 8px 8px 0;
        font-style: italic;
        color: var(--text-primary);
    }
    
    code, .code-inline {
        background-color: #2d333b;
        padding: 0.2rem 0.45rem;
        border-radius: 6px;
        font-family: 'SF Mono', 'Fira Code', 'Menlo', monospace;
        font-size: 0.9rem;
        color: #c9d1d9;
    }

    /* --- Pseudocode Block --- */
    .code-block {
        background: #0d1117;
        padding: 1.5rem;
        border-radius: 12px;
        border: 1px solid var(--border-color);
        overflow-x: auto;
    }
    .code-block pre {
        margin: 0;
        font-family: 'SF Mono', 'Fira Code', 'monospace';
        font-size: 0.9rem;
        line-height: 1.7;
    }
    .code-block .comment { color: #8b949e; }
    .code-block .keyword { color: #f472b6; }
    .code-block .function { color: #60a5fa; }
    .code-block .variable { color: #e2e8f0; }
    .code-block .parameter { color: #a78bfa; }
    
    /* --- Timeline Table --- */
    .timeline-table {
        width: 100%;
        border-collapse: separate;
        border-spacing: 0 0.5rem;
        margin-top: 1.5rem;
    }
    .timeline-table th, .timeline-table td {
        padding: 1rem 1.5rem;
        text-align: left;
        border-bottom: 1px solid var(--border-color);
    }
    .timeline-table th { font-weight: 600; color: var(--text-primary); }
    .timeline-table tr td:first-child { font-weight: 600; }
    .timeline-table td { vertical-align: top; }

    @media (max-width: 768px) {
        body { padding: 1.5rem 1rem; }
        .header h1 { font-size: 2.75rem; }
        .section { padding: 1.5rem; }
    }
</style>

</head>
<body>


<div class="container">
    <header class="header">
        <h1>Project Plan: SpaGLaM</h1>
        <p>A Hierarchical Spatial Graph-Language Model for Multimodal Spatial Biology</p>
    </header>

    <section class="section">
        <h2>1. Novelty Statement and Central Hypothesis</h2>
        <p>The field of computational spatial biology currently faces a dichotomy: models excel at either capturing the semantic meaning of gene expression within a single spot or modeling the spatial arrangement of spots based on their quantitative features. Our proposed model, <strong>SpaGLaM (Spatial Graph-Language Model)</strong>, introduces a paradigm shift by creating a hierarchical representation that does both.</p>
        <blockquote>
            <strong>Central Hypothesis:</strong> By first encoding the high-level semantic identity of each spot in a neighborhood using a rank-based gene language model, and then aggregating these semantic units with a graph neural network, SpaGLaM will produce a more robust, context-aware, and biologically insightful representation of tissue microenvironments than any existing single-modality or direct-fusion approach.
        </blockquote>
        <p>This approach is novel because it learns the <strong>spatial grammar</strong> of <strong>semantic concepts</strong> (the functional identity of spots, as defined by their gene expression), a higher-order task that moves beyond simple numerical aggregation.</p>
    </section>

    <section class="section">
        <h2>2. Datasets and Preprocessing</h2>
        <h3>2.1. Dataset Curation</h3>
        <ul>
            <li><strong>Primary Benchmarking Dataset:</strong> The 10x Visium Human DLPFC dataset (from `Dann et al.` and `Jiang et al.`). Its well-defined cortical layers provide an excellent ground truth for evaluating spatial domain identification.</li>
            <li><strong>Complex Disease Case Study:</strong> A multi-patient breast cancer (HER2+ from `He et al.`) or pancreatic cancer (PDAC from `Maan et al.`) dataset. This will demonstrate the model's ability to handle inter-patient heterogeneity and complex tumor microenvironments (TMEs).</li>
            <li><strong>High-Resolution Validation:</strong> A dataset with single-cell resolution (e.g., Xenium from the `Thor` paper) to serve as an ultimate ground truth for validating cell-level predictions and fine-grained patterns.</li>
        </ul>
        <h3>2.2. Preprocessing Pipeline</h3>
        <ul>
            <li><strong>AnnData Input:</strong> All data will be managed in `AnnData` objects for reproducibility and compatibility with the Scanpy ecosystem.</li>
            <li><strong>Transcriptomics:</strong>
                <ul>
                    <li>Raw counts are normalized by library size (target sum: `10,000`).</li>
                    <li>Data is log-transformed using `log1p`.</li>
                    <li>For each spot, a <strong>gene sentence</strong> is created by taking the gene symbols of the <strong>top 50</strong> most highly expressed genes, concatenated into a single string (as in **OmiCLIP**). This rank-based approach provides robustness to batch effects.</li>
                </ul>
            </li>
            <li><strong>Histology Images:</strong>
                <ul>
                    <li>High-resolution H&E images are tiled into `224x224` pixel patches centered on each ST spot's coordinates.</li>
                    <li>Image features will be extracted using a frozen, pre-trained Vision Transformer, such as the one from the **UNI** model (`ViT-L/16`), yielding a `1024-d` vector per patch (as in **DeepSpot**).</li>
                </ul>
            </li>
            <li><strong>Spatial Graph:</strong>
                <ul>
                    <li>A spatial graph `G = (V, E)` is constructed where `V` are the spots.</li>
                    <li>Edges `E` are defined between each spot and its **k=6** nearest neighbors based on physical coordinates (a robust parameter choice from **STAIG** and **Hist2ST**).</li>
                </ul>
            </li>
        </ul>
    </section>

    <section class="section">
        <h2>3. SpaGLaM: Model Architecture & Technical Implementation</h2>
        <h3>3.1. Module 1: Neighborhood Semantic Encoding</h3>
        <p>This module generates high-level semantic embeddings for every spot within a local neighborhood, leveraging the methodology established by **OmiCLIP**.</p>
        <ul>
            <li><strong>Input:</strong> For a target spot `i`, its neighborhood `N(i)`, including itself.</li>
            <li><strong>Process:</strong> For each neighbor `j ∈ N(i)`:
                <ul>
                    <li>Gene Sentence Embedding: <code class="code-inline">E_gene(j) = OmiCLIP_Text_Encoder(gene_sentence_j)</code>. This produces a `d=768` dimensional semantic vector.</li>
                    <li>Image Patch Embedding: <code class="code-inline">E_image(j) = UNI_Image_Encoder(H&E_patch_j)</code>. This produces a `d=1024` dimensional morphology vector.</li>
                </ul>
            </li>
        </ul>

        <h3>3.2. Module 2: Graph Attention Network for Contextual Aggregation</h3>
        <p>This module learns to intelligently aggregate the semantic embeddings from the neighborhood to form a context-aware representation for the central spot.</p>
        <blockquote>
            A <strong>Graph Attention Network (GAT)</strong> is chosen over simpler GNNs because it can learn to weigh the importance of different neighbors. For instance, in a TME, a tumor cell might pay more attention to an adjacent fibroblast or immune cell than to another distant tumor cell. This aligns with the approach in **STAGATE**.
        </blockquote>
        <p>The hidden representation for a node `i` at layer `(l+1)` is updated using the formula:</p>
        <p style="text-align:center; font-style:italic;">
            <code>h_i^{(l+1)} = σ ( Σ_{j ∈ N(i)} α_{ij}^{(l)} W^{(l)} h_j^{(l)} )</code>
        </p>
        <p>Where `h_i^{(l)}` is the embedding of node `i` at layer `l`, `W` is a learnable weight matrix, `σ` is a non-linearity (e.g., ELU), and `α_ij` is the attention coefficient computed as:</p>
        <p style="text-align:center; font-style:italic;">
            <code>α_{ij} = softmax_j ( LeakyReLU ( a^T [W h_i || W h_j] ) )</code>
        </p>
        <p>This process is applied separately to the gene embeddings and image embeddings to produce final context vectors <code>Z_gene(i)</code> and <code>Z_image(i)</code>.</p>

        <h3>3.3. Module 3: Training with Multimodal Contrastive Loss</h3>
        <p>The model is trained end-to-end to align the context-aware gene embedding with the context-aware image embedding. This forces the model to learn spatial relationships that are consistent across both modalities.</p>
        <p>We use a symmetric contrastive loss, similar to **CLIP** and **BLEEP**:</p>
        <p style="text-align:center; font-style:italic;">
            <code>L = -1/N * Σ [ log(exp(sim(Z_gene_i, Z_image_i)/τ) / Σ_k exp(sim(Z_gene_i, Z_image_k)/τ)) + log(exp(sim(Z_gene_i, Z_image_i)/τ) / Σ_k exp(sim(Z_gene_k, Z_image_i)/τ)) ]</code>
        </p>
        <p>where `sim(a, b)` is the cosine similarity between vectors `a` and `b`, and `τ` is a learnable temperature parameter initialized at `0.07`.</p>

        <h3>3.4. Pseudocode for a Training Step</h3>
        <div class="code-block">
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
IGNORE_WHEN_COPYING_END
<pre>
<span class="keyword">def</span> <span class="function">train_step</span>(<span class="parameter">batch_spots</span>, <span class="parameter">model</span>, <span class="parameter">foundation_encoders</span>):
<span class="comment"># batch_spots contains ST data, H&E patches, and spatial graphs</span>

<span class="comment"># 1. Pre-compute semantic embeddings for all spots in the batch neighborhood</span>
<span class="variable">all_gene_embeds</span> = <span class="variable">foundation_encoders</span>.<span class="function">text_encoder</span>(<span class="variable">batch_spots</span>.gene_sentences)
<span class="variable">all_image_embeds</span> = <span class="variable">foundation_encoders</span>.<span class="function">image_encoder</span>(<span class="variable">batch_spots</span>.patches)

<span class="comment"># 2. GAT-based contextual aggregation for the central spots in the batch</span>
<span class="comment"># The GNN takes the entire neighborhood graph and pre-computed embeddings</span>
<span class="variable">Z_gene</span> = <span class="variable">model</span>.<span class="function">gene_gnn</span>(<span class="variable">batch_spots</span>.graph, <span class="variable">all_gene_embeds</span>)
<span class="variable">Z_image</span> = <span class="variable">model</span>.<span class="function">image_gnn</span>(<span class="variable">batch_spots</span>.graph, <span class="variable">all_image_embeds</span>)

<span class="comment"># 3. Compute contrastive loss for alignment</span>
<span class="variable">loss</span> = <span class="function">symmetric_contrastive_loss</span>(<span class="variable">Z_gene</span>, <span class="variable">Z_image</span>, temperature=<span class="parameter">τ</span>)

<span class="variable">loss</span>.<span class="function">backward</span>()
<span class="variable">optimizer</span>.<span class="function">step</span>()

<span class="keyword">return</span> <span class="variable">loss</span>.item()
</pre>

Generated code
</div>
    </section>

    <section class="section">
        <h2>4. Rigorous Benchmarking & Case Study Plan</h2>
        <h3>4.1. Quantitative Baselines and Ablations</h3>
        <ul>
            <li><strong>Baselines:</strong> We will compare SpaGLaM against state-of-the-art models that represent different core philosophies:
                <ul>
                    <li><strong>OmiCLIP:</strong> To demonstrate the critical value of adding spatial context via the GAT. This is the "no-GNN" baseline.</li>
                    <li><strong>DeepSpot:</strong> To show the superiority of semantic gene-sentence encoding over direct aggregation of numerical image features.</li>
                    <li><strong>STAIG:</strong> To highlight the benefits of our hierarchical semantic aggregation versus a GNN applied directly to numerical gene expression.</li>
                </ul>
            </li>
            <li><strong>Ablation Study:</strong> We will evaluate a version of SpaGLaM where the GAT is replaced with a simple `mean pooling` operation. This will rigorously quantify the performance gain attributable to the learned attention mechanism.</li>
        </ul>
        <h3>4.2. Biological Case Study: Dissecting the Tumor Immune Microenvironment (TIME)</h3>
        <p>The ultimate test of a new computational method is its ability to generate novel biological insights. We will apply SpaGLaM to a multi-patient PDAC or breast cancer dataset.</p>
        <ul>
            <li><strong>Goal:</strong> Unbiasedly identify and characterize distinct spatial niches within the TME based on the learned multimodal embeddings.</li>
            <li><strong>Analysis Workflow:</strong>
                <ol>
                    <li>Perform Leiden clustering on the final `Z_gene` embeddings to group spots into data-driven clusters.</li>
                    <li>Visualize these clusters spatially on the H&E slides to identify coherent anatomical niches (e.g., "immune-infiltrated," "immune-desert," "stromal-activated").</li>
                    <li>Perform differential gene expression and pathway analysis (e.g., GSEA) between these data-driven niches to uncover their unique molecular signatures and validate them with known biology.</li>
                    <li>Consult with a pathologist to validate the anatomical relevance of the identified niches and interpret novel findings.</li>
                </ol>
            </li>
        </ul>
    </section>
    
    <section class="section">
        <h2>5. Project Timeline and Publication Strategy</h2>
        <h3>High-Level Timeline</h3>
        <table class="timeline-table">
            <thead>
                <tr>
                    <th>Phase</th>
                    <th>Timeline</th>
                    <th>Key Deliverables</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>1. Foundational Work</strong></td>
                    <td>Months 1-4</td>
                    <td>- Finalized data preprocessing pipeline for all datasets.<br>- Implemented SpaGLaM v1.0 in PyTorch.<br>- Successful training and convergence on the DLPFC benchmark dataset.</td>
                </tr>
                <tr>
                    <td><strong>2. Rigorous Evaluation</strong></td>
                    <td>Months 5-7</td>
                    <td>- Completed benchmarking against all baselines (OmiCLIP, DeepSpot, STAIG).<br>- Finished ablation studies to quantify GAT impact.<br>- Finalized hyperparameters and produced performance metrics.</td>
                </tr>
                <tr>
                    <td><strong>3. Biological Discovery</strong></td>
                    <td>Months 8-10</td>
                    <td>- Completed TME case study analysis on the cancer dataset.<br>- Generated all key figures showing novel biological insights (niche maps, pathway plots).<br>- Validated key findings with pathologist consultation and literature review.</td>
                </tr>
                <tr>
                    <td><strong>4. Dissemination</strong></td>
                    <td>Months 11-12</td>
                    <td>- Manuscript written and submitted to a high-impact journal (*Nature Methods* or similar).<br>- Open-source code with tutorials and documentation published on GitHub.<br>- Preprint posted on *bioRxiv* to ensure rapid dissemination.</td>
                </tr>
            </tbody>
        </table>
        <h3>Dissemination Plan</h3>
        <p>To maximize impact and meet the standards of top-tier journals, we will ensure full reproducibility and accessibility:</p>
        <ul>
            <li><strong>Open-Source Code:</strong> The entire SpaGLaM framework, including preprocessing scripts and analysis notebooks for reproducing figures, will be released on GitHub under a permissive license (e.g., MIT).</li>
            <li><strong>User-Friendly Package:</strong> The model will be packaged for easy installation via `pip`, with detailed documentation on ReadtheDocs, tutorials, and example data.</li>
            <li><strong>Preprint:</strong> A preprint will be posted on *bioRxiv* upon journal submission to ensure the findings are rapidly available to the scientific community.</li>
        </ul>
    </section>

</div>
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
IGNORE_WHEN_COPYING_END
</body>
</html>
